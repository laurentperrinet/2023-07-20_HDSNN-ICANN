%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Laurent Perrinet at 2023-04-07 16:39:52 +0200 


%% Saved with string encoding Unicode (UTF-8) 


@article{kass_statistical_2005,
	title = {Statistical issues in the analysis of neuronal data},
	volume = {94},
	doi = {10.1152/jn.00648.2004},
	number = {1},
	journal = {Journal of neurophysiology},
	author = {Kass, Robert E. and Ventura, Valérie and Brown, Emery N.},
	year = {2005},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {8--25},
}

@misc{kohn_utah_2016,
	title = {Utah array extracellular recordings of spontaneous and visually evoked                 activity from anesthetized macaque primary visual cortex ({V1}).},
	url = {http://crcns.org/data-sets/vc/pvc-11},
	language = {en},
	urldate = {2022-12-19},
	publisher = {CRCNS.org},
	author = {Kohn, A. and Smith, M.A.},
	year = {2016},
	doi = {10.6080/K0NC5Z4X},
	keywords = {Macaque, Neuroscience, Primary visual cortex, ⛔ No INSPIRE recid found},
}

@misc{stringer_mouselandrastermap_2020,
	title = {{MouseLand}/rastermap: {A} multi-dimensional embedding algorithm},
	url = {https://github.com/MouseLand/rastermap},
	urldate = {2022-12-19},
	author = {Stringer, Carsen},
	year = {2020},
	keywords = {⛔ No INSPIRE recid found},
}

@article{simoncelli_characterization_2003,
	title = {Characterization of {Neural} {Responses} with {Stochastic} {Stimuli}},
	url = {http://pillowlab.princeton.edu/pubs/simoncelli03c-preprint.pdf},
	language = {en},
	author = {Simoncelli, Eero P and Paninski, Liam and Pillow, Jonathan and Schwartz, Odelia},
	year = {2003},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@inproceedings{deneve_bayesian_2004,
	title = {Bayesian inference in spiking neurons},
	volume = {17},
	url = {https://papers.nips.cc/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html},
	abstract = {We propose a new interpretation of spiking neurons as Bayesian integra-          tors accumulating evidence over time about events in the external world          or the body, and communicating to other neurons their certainties about          these events. In this model, spikes signal the occurrence of new infor-          mation, i.e. what cannot be predicted from the past activity. As a result,          firing statistics are close to Poisson, albeit providing a deterministic rep-          resentation of probabilities. We proceed to develop a theory of Bayesian          inference in spiking neural networks, recurrent interactions implement-          ing a variant of belief propagation.},
	urldate = {2022-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Deneve, Sophie},
	year = {2004},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@article{ben-yishai_traveling_1997,
	title = {Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs} in a {Cortical} {Network} {Module}},
	volume = {77},
	author = {Ben-yishai, Rani and Hansel, David},
	year = {1997},
	note = {00000},
	keywords = {\#nosource, orientation selectivity, population vector, primary visual cortex, ⛔ No INSPIRE recid found},
	pages = {57--77},
}

@misc{nadafian_bio-plausible_2020,
	title = {Bio-plausible {Unsupervised} {Delay} {Learning} for {Extracting} {Temporal} {Features} in {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2011.09380},
	abstract = {The plasticity of the conduction delay between neurons plays a fundamental role in learning. However, the exact underlying mechanisms in the brain for this modulation is still an open problem. Understanding the precise adjustment of synaptic delays could help us in developing effective brain-inspired computational models in providing aligned insights with the experimental evidence. In this paper, we propose an unsupervised biologically plausible learning rule for adjusting the synaptic delays in spiking neural networks. Then, we provided some mathematical proofs to show that our learning rule gives a neuron the ability to learn repeating spatio-temporal patterns. Furthermore, the experimental results of applying an STDP-based spiking neural network equipped with our proposed delay learning rule on Random Dot Kinematogram indicate the efficacy of the proposed delay learning rule in extracting temporal features.},
	publisher = {arXiv},
	author = {Nadafian, Alireza and Ganjtabesh, Mohammad},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, ⛔ No INSPIRE recid found},
}

@article{perrinet_sparse_2004,
	series = {New {Aspects} in {Neurocomputing}: 10th {European} {Symposium} on {Artificial} {Neural} {Networks} 2002},
	title = {Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit},
	volume = {57},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231204000670},
	doi = {10.1016/j.neucom.2004.01.010},
	abstract = {In order to account for the rapidity of visual processing, we explore visual coding strategies using a one-pass feed-forward spiking neural network. We based our model on the work of Van Rullen and Thorpe Neural Comput. 13 (6) (2001) 1255, which constructs a retinal representation using an orthogonal wavelet transform. This strategy provides a spike code, thanks to a rank order coding scheme which offers an alternative to the classical spike frequency coding scheme. We extended this model to efficient representations in arbitrary linear generative models by implementing lateral interactions on top of this feed-forward model. This method uses a matching pursuit scheme—recursively detecting in the image the best match with the elements of a dictionary and then subtracting it—and which may similarly define a visual spike code. In particular, this transform could be used with large and arbitrary dictionaries, so that we may define an over-complete representation which may define an efficient sparse spike coding scheme in arbitrary multi-layered architectures. We show here extensions of this method of computing with spike events, introducing an adaptive scheme leading to the emergence of V1-like receptive fields and then a model of bottom-up saliency pursuit.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Neurocomputing},
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	month = mar,
	year = {2004},
	keywords = {Natural images statistics, Parallel asynchronous processing, Sparse coding, Ultra-rapid categorization, Vision, Wavelet Hansform, ⛔ No INSPIRE recid found},
	pages = {125--134},
}

@article{perrinet_coding_2004,
	title = {Coding static natural images using spiking event times: do neurons cooperate?},
	volume = {15},
	doi = {10.1109/TNN.2004.833303},
	number = {5},
	journal = {IEEE Transactions on neural networks},
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	year = {2004},
	note = {Publisher: IEEE},
	keywords = {Biomembranes, Central nervous system, Filters, Fires, Image coding, Image reconstruction, Neurons, Retina, Statistics, Wavelet transforms, ⛔ No INSPIRE recid found},
	pages = {1164--1175},
}

@article{konishi_coding_2003,
	title = {Coding of auditory space},
	volume = {26},
	issn = {0147-006X},
	doi = {10.1146/annurev.neuro.26.041002.131123},
	abstract = {Behavioral, anatomical, and physiological approaches can be integrated in the study of sound localization in barn owls. Space representation in owls provides a useful example for discussion of place and ensemble coding. Selectivity for space is broad and ambiguous in low-order neurons. Parallel pathways for binaural cues and for different frequency bands converge on high-order space-specific neurons, which encode space more precisely. An ensemble of broadly tuned place-coding neurons may converge on a single high-order neuron to create an improved labeled line. Thus, the two coding schemes are not alternate methods. Owls can localize sounds by using either the isomorphic map of auditory space in the midbrain or forebrain neural networks in which space is not mapped.},
	language = {eng},
	journal = {Annual Review of Neuroscience},
	author = {Konishi, Masakazu},
	year = {2003},
	pmid = {14527266},
	keywords = {Animals, Auditory Pathways, Auditory Perception, Behavior, Animal, Brain, Brain Mapping, Forms and Records Control, Models, Neurological, Nerve Net, Neural Inhibition, Sound Localization, Space Perception, Strigiformes, ⛔ No INSPIRE recid found},
	pages = {31--55},
}

@article{carr_circuit_1990,
	title = {A circuit for detection of interaural time differences in the brain stem of the barn owl},
	volume = {10},
	copyright = {© 1990 by Society for Neuroscience},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/10/10/3227},
	doi = {10.1523/JNEUROSCI.10-10-03227.1990},
	abstract = {Detection of interaural time differences underlies azimuthal sound localization in the barn owl Tyto alba. Axons of the cochlear nucleus magnocellularis, and their targets in the binaural nucleus laminaris, form the circuit responsible for encoding these interaural time differences. The nucleus laminaris receives bilateral inputs from the cochlear nucleus magnocellularis such that axons from the ipsilateral cochlear nucleus enter the nucleus laminaris dorsally, while contralateral axons enter from the ventral side. This interdigitating projection to the nucleus laminaris is tonotopic, and the afferents are both sharply tuned and matched in frequency to the neighboring afferents. Recordings of phase-locked spikes in the afferents show an orderly change in the arrival time of the spikes as a function of distance from the point of their entry into the nucleus laminaris. The same range of conduction time (160 mu sec) was found over the 700-mu m depth of the nucleus laminaris for all frequencies examined (4-7.5 kHz) and corresponds to the range of interaural time differences available to the barn owl. The estimated conduction velocity in the axons is low (3-5 m/sec) and may be regulated by short internodal distances (60 mu m) within the nucleus laminaris. Neurons of the nucleus laminaris have large somata and very short dendrites. These cells are frequency selective and phase-lock to both monaural and binaural stimuli. The arrival time of phase-locked spikes in many of these neurons differs between the ipsilateral and contralateral inputs. When this disparity is nullified by imposition of an appropriate interaural time difference, the neurons respond maximally. The number of spikes elicited in response to a favorable interaural time difference is roughly double that elicited by a monaural stimulus. Spike counts for unfavorable interaural time differences fall well below monaural response levels. These findings indicate that the magnocellular afferents work as delay lines, and the laminaris neurons work as co- incidence detectors. The orderly distribution of conduction times, the predictability of favorable interaural time differences from monaural phase responses, and the pattern of the anatomical projection from the nucleus laminaris to the central nucleus of the inferior colliculus suggest that interaural time differences and their phase equivalents are mapped in each frequency band along the dorsoventral axis of the nucleus laminaris.},
	language = {en},
	number = {10},
	urldate = {2022-12-16},
	journal = {Journal of Neuroscience},
	author = {Carr, C. E. and Konishi, M.},
	month = oct,
	year = {1990},
	pmid = {2213141},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {⛔ No INSPIRE recid found},
	pages = {3227--3246},
}

@article{bienenstock_model_1995,
	title = {A model of neocortex},
	volume = {6},
	issn = {0954-898X},
	url = {https://doi.org/10.1088/0954-898X_6_2_004},
	doi = {10.1088/0954-898X_6_2_004},
	abstract = {Prompted by considerations about (i) the compositionality of cognitive functions, (ii) the physiology of individual cortical neurons, (iii) the role of accurately timed spike patterns in cortex, and (iv) the regulation of global cortical activity, we suggest that the dynamics of cortex on the 1-ms time scale may be described as the activation of circuits of the synfire-chain type (Abeles 1982, 1991). We suggest that the fundamental computational unit in cortex may be a wave-like spatio-temporal pattern of synfire type, and that the binding mechanism underlying compositionality in cognition may be the accurate synchronization of synfire waves that propagate simultaneously on distinct, weakly coupled, synfire chains. We propose that Hebbian synaptic plasticity may result in a superposition of synfire chains in cortical connectivity, whereby a given neuron participates in many distinct chains. We investigate the behaviour of a much-simplified model of cortical dynamics devised along these principles. Calculations and numerical experiments are performed based on an assumption of randomness of stored chains, in the style of statistical physics. It is demonstrated that: (i) there exists a critical value for the total length of stored chains; (ii) this storage capacity is linear in the network's size; (iii) the behaviour of the network around the critical point is characterized by the self-regulation of the number of synfire waves coactive in the network at any given time.},
	number = {2},
	urldate = {2022-12-16},
	journal = {Network: Computation in Neural Systems},
	author = {Bienenstock, Elie},
	month = jan,
	year = {1995},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1088/0954-898X\_6\_2\_004},
	keywords = {⛔ No INSPIRE recid found},
	pages = {179--224},
}

@article{lazar_time_2004,
	title = {Time encoding with an integrate-and-fire neuron with a refractory period},
	volume = {58-60},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231204000177},
	doi = {10.1016/j.neucom.2004.01.022},
	abstract = {Time encoding is a formal method of mapping amplitude information into a time sequence. We show that under simple conditions, bandlimited stimuli encoded with an integrate-and-ÿre neuron with an absolute refractory period can be recovered loss-free from the neural spike train at its output. We provide an algorithm for perfect recovery and derive conditions for its convergence. c 2003 Elsevier B.V. All rights reserved.},
	language = {en},
	urldate = {2022-12-19},
	journal = {Neurocomputing},
	author = {Lazar, Aurel A.},
	month = jun,
	year = {2004},
	keywords = {⛔ No INSPIRE recid found},
	pages = {53--58},
}

@article{golding_dendritic_2002,
	title = {Dendritic spikes as a mechanism for cooperative long-term potentiation},
	volume = {418},
	copyright = {2002 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature00854},
	doi = {10.1038/nature00854},
	abstract = {Strengthening of synaptic connections following coincident pre- and postsynaptic activity was proposed by Hebb as a cellular mechanism for learning1. Contemporary models assume that multiple synapses must act cooperatively to induce the postsynaptic activity required for hebbian synaptic plasticity2,3,4,5. One mechanism for the implementation of this cooperation is action potential firing, which begins in the axon, but which can influence synaptic potentiation following active backpropagation into dendrites6. Backpropagation is limited, however, and action potentials often fail to invade the most distal dendrites7,8,9,10. Here we show that long-term potentiation of synapses on the distal dendrites of hippocampal CA1 pyramidal neurons does require cooperative synaptic inputs, but does not require axonal action potential firing and backpropagation. Rather, locally generated and spatially restricted regenerative potentials (dendritic spikes) contribute to the postsynaptic depolarization and calcium entry necessary to trigger potentiation of distal synapses. We find that this mechanism can also function at proximal synapses, suggesting that dendritic spikes participate generally in a form of synaptic potentiation that does not require postsynaptic action potential firing in the axon.},
	language = {en},
	number = {6895},
	urldate = {2022-12-16},
	journal = {Nature},
	author = {Golding, Nace L. and Staff, Nathan P. and Spruston, Nelson},
	month = jul,
	year = {2002},
	note = {Number: 6895
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {326--331},
}

@article{mel_synaptic_2017,
	title = {Synaptic plasticity in dendrites: complications and coping strategies},
	volume = {43},
	issn = {1873-6882},
	shorttitle = {Synaptic plasticity in dendrites},
	doi = {10.1016/j.conb.2017.03.012},
	abstract = {The elaborate morphology, nonlinear membrane mechanisms and spatiotemporally varying synaptic activation patterns of dendrites complicate the expression, compartmentalization and modulation of synaptic plasticity. To grapple with this complexity, we start with the observation that neurons in different brain areas face markedly different learning problems, and dendrites of different neuron types contribute to the cell's input-output function in markedly different ways. By committing to specific assumptions regarding a neuron's learning problem and its input-output function, specific inferences can be drawn regarding the synaptic plasticity mechanisms and outcomes that we 'ought' to expect for that neuron. Exploiting this assumption-driven approach can help both in interpreting existing experimental data and designing future experiments aimed at understanding the brain's myriad learning processes.},
	language = {eng},
	journal = {Current Opinion in Neurobiology},
	author = {Mel, Bartlett W. and Schiller, Jackie and Poirazi, Panayiota},
	month = apr,
	year = {2017},
	pmid = {28453975},
	keywords = {Dendrites, Humans, Learning, Models, Neurological, Neuronal Plasticity, Synapses, ⛔ No INSPIRE recid found},
	pages = {177--186},
}

@article{jeffress_place_1948,
	title = {A place theory of sound localization},
	volume = {41},
	issn = {0021-9940},
	doi = {10.1037/h0061495},
	abstract = {The author presents a place theory of sound localization based upon the time difference of stimulation of the 2 ears. The hypothesis depends upon the known slow rate of conduction in small nerve fibers and the phenomenon of spatial summation. It assumes that some secondary fibers of the auditory tract divide, sending branches homolaterally and contralaterally. There is a further assumption that these neurons make synaptic connection with other fibers on each side, the latter neurones synapsing with both contralateral and homolateral neurones. Then, if the sound is in a median plane, the summation effect would be maximal in a central group of the tertiary fibers on each side. If the sound source is shifted, the summation effect would result in a shifting of the transmission through synapses in the tertiary zone. This provides a spatial change in the pattern of nerve discharge as a consequence of a temporal change in the binaural stimulation. The anatomical location of such a center is suggested either in the inferior colliculus or the medial geniculate body. Possible experimental procedures are suggested. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Journal of Comparative and Physiological Psychology},
	author = {Jeffress, Lloyd A.},
	year = {1948},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Auditory Localization, Ear (Anatomy), Neurons, Temporal Frequency, ⛔ No INSPIRE recid found},
	pages = {35--39},
}

@incollection{middlebrooks_chapter_2015,
	series = {The {Human} {Auditory} {System}},
	title = {Chapter 6 - {Sound} localization},
	volume = {129},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444626301000068},
	abstract = {The auditory system derives locations of sound sources from spatial cues provided by the interaction of sound with the head and external ears. Those cues are analyzed in specific brainstem pathways and then integrated as cortical representation of locations. The principal cues for horizontal localization are interaural time differences (ITDs) and interaural differences in sound level (ILDs). Vertical and front/back localization rely on spectral-shape cues derived from direction-dependent filtering properties of the external ears. The likely first sites of analysis of these cues are the medial superior olive (MSO) for ITDs, lateral superior olive (LSO) for ILDs, and dorsal cochlear nucleus (DCN) for spectral-shape cues. Localization in distance is much less accurate than that in horizontal and vertical dimensions, and interpretation of the basic cues is influenced by additional factors, including acoustics of the surroundings and familiarity of source spectra and levels. Listeners are quite sensitive to sound motion, but it remains unclear whether that reflects specific motion detection mechanisms or simply detection of changes in static location. Intact auditory cortex is essential for normal sound localization. Cortical representation of sound locations is highly distributed, with no evidence for point-to-point topography. Spatial representation is strictly contralateral in laboratory animals that have been studied, whereas humans show a prominent right-hemisphere dominance.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	author = {Middlebrooks, John C.},
	editor = {Aminoff, Michael J. and Boller, François and Swaab, Dick F.},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-444-62630-1.00006-8},
	keywords = {HRTF, Spatial hearing, auditory cortex, auditory motion, distance perception, interaural level difference, interaural time difference, precedence effect, superior colliculus, superior olivary complex, ⛔ No INSPIRE recid found},
	pages = {99--116},
}

@article{rinberg_speed-accuracy_2006,
	title = {Speed-{Accuracy} {Tradeoff} in {Olfaction}},
	volume = {51},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627306005538},
	doi = {10.1016/j.neuron.2006.07.013},
	abstract = {The basic psychophysical principle of speed-accuracy tradeoff (SAT) has been used to understand key aspects of neuronal information processing in vision and audition, but the principle of SAT is still debated in olfaction. In this study we present the direct observation of SAT in olfaction. We developed a behavioral paradigm for mice in which both the duration of odorant sampling and the difficulty of the odor discrimination task were controlled by the experimenter. We observed that the accuracy of odor discrimination increases with the duration of imposed odorant sampling, and that the rate of this increase is slower for harder tasks. We also present a unifying picture of two previous, seemingly disparate experiments on timing of odorant sampling in odor discrimination tasks. The presence of SAT in olfaction provides strong evidence for temporal integration in olfaction and puts a constraint on models of olfactory processing.},
	language = {en},
	number = {3},
	urldate = {2022-12-16},
	journal = {Neuron},
	author = {Rinberg, Dmitry and Koulakov, Alexei and Gelperin, Alan},
	month = aug,
	year = {2006},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	pages = {351--358},
}

@incollection{cleland_construction_2014,
	title = {Construction of {Odor} {Representations} by {Olfactory} {Bulb} {Microcircuits}},
	volume = {208},
	isbn = {978-0-444-63350-7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444633507000073},
	abstract = {Like other sensory systems, the olfactory system transduces specific features of the external environment and must construct an organized sensory representation from these highly fragmented inputs. As with these other systems, this representation is not accurate per se, but is constructed for utility, and emphasizes certain, presumably useful, features over others. I here describe the cellular and circuit mechanisms of the peripheral olfactory system that underlie this process of sensory construction, emphasizing the distinct architectures and properties of the two prominent computational layers in the olfactory bulb. Notably, while the olfactory system solves essentially similar conceptual problems to other sensory systems, such as contrast enhancement, activity normalization, and extending dynamic range, its peculiarities often require qualitatively different computational algorithms than are deployed in other sensory modalities. In particular, the olfactory modality is intrinsically high dimensional, and lacks a simple, externally defined basis analogous to wavelength or pitch on which elemental odor stimuli can be quantitatively compared. Accordingly, the quantitative similarities of the receptive fields of different odorant receptors (ORs) vary according to the statistics of the odor environment. To resolve these unusual challenges, the olfactory bulb appears to utilize unique nontopographical computations and intrinsic learning mechanisms to perform the necessary high-dimensional, similarity-dependent computations. In sum, the early olfactory system implements a coordinated set of early sensory transformations directly analogous to those in other sensory systems, but accomplishes these with unique circuit architectures adapted to the properties of the olfactory modality.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Cleland, Thomas A.},
	year = {2014},
	doi = {10.1016/B978-0-444-63350-7.00007-3},
	keywords = {⛔ No INSPIRE recid found},
	pages = {177--203},
}

@article{kashiwadani_synchronized_1999,
	title = {Synchronized {Oscillatory} {Discharges} of {Mitral}/{Tufted} {Cells} {With} {Different} {Molecular} {Receptive} {Ranges} in the {Rabbit} {Olfactory} {Bulb}},
	volume = {82},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.1999.82.4.1786},
	doi = {10.1152/jn.1999.82.4.1786},
	abstract = {Individual glomeruli in the mammalian olfactory bulb represent a single or a few type(s) of odorant receptors. Signals from different types of receptors are thus sorted out into different glomeruli. How does the neuronal circuit in the olfactory bulb contribute to the combination and integration of signals received by different glomeruli? Here we examined electrophysiologically whether there were functional interactions between mitral/tufted cells associated with different glomeruli in the rabbit olfactory bulb. First, we made simultaneous recordings of extracellular single-unit spike responses of mitral/tufted cells and oscillatory local field potentials in the dorsomedial fatty acid–responsive region of the olfactory bulb in urethan-anesthetized rabbits. Using periodic artificial inhalation, the olfactory epithelium was stimulated with a homologous series ofn-fatty acids or n-aliphatic aldehydes. The odor-evoked spike discharges of mitral/tufted cells tended to phase-lock to the oscillatory local field potential, suggesting that spike discharges of many cells occur synchronously during odor stimulation. We then made simultaneous recordings of spike discharges from pairs of mitral/tufted cells located 300–500 μm apart and performed a cross-correlation analysis of their spike responses to odor stimulation. In ∼27\% of cell pairs examined, two cells with distinct molecular receptive ranges showed synchronized oscillatory discharges when olfactory epithelium was stimulated with one or a mixture of odorant(s) effective in activating both. The results suggest that the neuronal circuit in the olfactory bulb causes synchronized spike discharges of specific pairs of mitral/tufted cells associated with different glomeruli and the synchronization of odor-evoked spike discharges may contribute to the temporal binding of signals derived from different types of odorant receptor.},
	number = {4},
	urldate = {2022-12-16},
	journal = {Journal of Neurophysiology},
	author = {Kashiwadani, Hideki and Sasaki, Yasnory F. and Uchida, Naoshige and Mori, Kensaku},
	month = oct,
	year = {1999},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1786--1792},
}

@article{vidyasagar_multiple_1996,
	title = {Multiple mechanisms underlying the orientation selectivity of visual cortical neurones},
	volume = {19},
	issn = {0166-2236},
	url = {https://www.sciencedirect.com/science/article/pii/S016622369620027X},
	doi = {10.1016/S0166-2236(96)20027-X},
	abstract = {For over three decades, the mechanism of orientation selectivity of visual cortical neurones has been hotly debated. While intracortical inhibition has been implicated as playing a vital role, it has been difficult to observe it clearly. On the basis of recent findings, we propose a model in which the visual cortex brings together a number of different mechanisms for generating orientation-selective responses. Orientation biases in the thalamo-cortical input fibres provide an initial weak selectivity either directly in the excitatory input or by acting via cortical interneurones. This weak selectivity of postsynaptic potentials is then amplified by voltage-sensitive conductances of the cell membrane and excitatory and inhibitory intracortical circuitry, resulting in the sharp tuning seen in the spike discharges of visual cortical cells.},
	language = {en},
	number = {7},
	urldate = {2022-12-16},
	journal = {Trends in Neurosciences},
	author = {Vidyasagar, T. R. and Pei, X. and Volgushev, M.},
	month = jul,
	year = {1996},
	keywords = {⛔ No INSPIRE recid found},
	pages = {272--277},
}

@article{ben-yishai_traveling_1996,
	title = {Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs} in a {Cortical} {Network} {Module}},
	abstract = {Recent studies have shown that local cortical feedback can have an important effect on the response of neurons in primary visual cortex to the orientation of visual stimuli. In this work, we study the role of the cortical feedback in shaping the spatiotemporal patterns of activity in cortex. Two questions are addressed: one, what are the limitations on the ability of cortical neurons to lock their activity to rotating oriented stimuli within a single receptive ﬁeld? Two, can the local architecture of visual cortex lead to the generation of spontaneous traveling pulses of activity? We study these issues analytically by a population-dynamic model of a hypercolumn in visual cortex. The order parameter that describes the macroscopic behavior of the network is the time-dependent population vector of the network. We ﬁrst study the network dynamics under the inﬂuence of a weakly tuned input that slowly rotates within the receptive ﬁeld. We show that if the cortical interactions have strong spatial modulation, the network generates a sharply tuned activity proﬁle that propagates across the hypercolumn in a path that is completely locked to the stimulus rotation. The resultant rotating population vector maintains a constant angular lag relative to the stimulus, the magnitude of which grows with the stimulus rotation frequency. Beyond a critical frequency the population vector does not lock to the stimulus but executes a quasi-periodic motion with an average frequency that is smaller than that of the stimulus. In the second part we consider the stable intrinsic state of the cortex under the inﬂuence of isotropic stimulation. We show that if the local inhibitory feedback is sufﬁciently strong, the network does not settle into a stationary state but develops spontaneous traveling pulses of activity. Unlike recent models of wave propagation in cortical networks, the connectivity pattern in our model is spatially symmetric, hence the direction of propagation of these waves is arbitrary. The interaction of these waves with an external-oriented stimulus is studied. It is shown that the system can lock to a weakly tuned rotating stimulus if the stimulus frequency is close to the frequency of the intrinsic wave.},
	language = {en},
	journal = {Journal of Computational Neuroscience},
	author = {Ben-Yishai, Rani and Hansel, David and Sompolinsky, Haim},
	year = {1996},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@article{chagnac-amitai_horizontal_1989,
	title = {Horizontal spread of synchronized activity in neocortex and its control by {GABA}-mediated inhibition},
	volume = {61},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/abs/10.1152/jn.1989.61.4.747},
	doi = {10.1152/jn.1989.61.4.747},
	abstract = {1. Suppression of GABAA receptor-mediated inhibition disrupts the neural activity of neocortex and can lead to synchronized discharges that mimic those of partial epilepsy. We have studied the role of GABAA-mediated inhibition in controlling the synchronization and horizontal (tangential) spread of cortical activity. 2. Slices of rat SmI were maintained in vitro and focally stimulated in layer VI while recording with a horizontal array of extracellular electrodes. Inhibition was slightly suppressed by adding low concentrations of the GABAA antagonists bicuculline or bicuculline methiodide to the bathing medium. Under control conditions neural activity was narrowly confined to a vertical strip of cortex. The horizontal spread of activity expanded about twofold in the presence of antagonist concentrations (less than or equal to 0.5 microM) that were expected to suppress GABAA function by no more than 10-20\%. 3. At antagonist concentrations between 0.4 and 1.0 microM, evoked epileptiform activity appeared. These threshold-dose epileptiform events showed wide variations in size and duration (even at the same recording site), very variable distances of horizontal propagation, specific sites of propagation failure, reversals of propagation direction, and directional asymmetries in their probability of propagation. This contrasts with activity observed previously (Ref. 9) in high bicuculline concentrations (greater than or equal to 10 microM): large, stereotyped events that propagate reliably without decrement or reflection. 4. Intracellular recordings were obtained from pyramidal neurons in layers II/III in the presence of less than or equal to 1 microM bicuculline. Inhibitory postsynaptic potentials (IPSPs) were observed during both primary evoked responses and propagating epileptiform events and were often comparable in size and duration to those in untreated cortex. Epileptiform field potentials were always correlated with synaptic activity in single cells, but the pattern and type of PSPs varied with the form of the field potentials. Large amplitude epileptiform events coincided with an overwhelming inhibition of upper layer neurons. 5. We conclude that 1) the horizontal spread of normal cortical activity is strongly constrained by GABAA-mediated IPSPs, 2) a relatively small reduction in the efficacy of inhibition leads to a large increase in the spread of excitation, 3) initiation and propagation of synchronized epileptiform activity can occur even in the presence of robust cortical inhibition, and 4) the character of epileptiform activity is strongly affected by the influences of inhibition.},
	number = {4},
	urldate = {2022-12-16},
	journal = {Journal of Neurophysiology},
	author = {Chagnac-Amitai, Y. and Connors, B. W.},
	month = apr,
	year = {1989},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {747--758},
}

@article{ballard_dual_2011,
	title = {Dual {Roles} for {Spike} {Signaling} in {Cortical} {Neural} {Populations}},
	volume = {5},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2011.00022},
	abstract = {A prominent feature of signaling in cortical neurons is that of randomness in the action potential. The output of a typical pyramidal cell can be well fit with a Poisson model, and variations in the Poisson rate repeatedly have been shown to be correlated with stimuli. However while the rate provides a very useful characterization of neural spike data, it may not be the most fundamental description of the signaling code. Recent data showing γ frequency range multi-cell action potential correlations, together with spike timing dependent plasticity, are spurring a re-examination of the classical model, since precise timing codes imply that the generation of spikes is essentially deterministic. Could the observed Poisson randomness and timing determinism reflect two separate modes of communication, or do they somehow derive from a single process? We investigate in a timing-based model whether the apparent incompatibility between these probabilistic and deterministic observations may be resolved by examining how spikes could be used in the underlying neural circuits. The crucial component of this model draws on dual roles for spike signaling. In learning receptive fields from ensembles of inputs, spikes need to behave probabilistically, whereas for fast signaling of individual stimuli, the spikes need to behave deterministically. Our simulations show that this combination is possible if deterministic signals using γ latency coding are probabilistically routed through different members of a cortical cell population at different times. This model exhibits standard features characteristic of Poisson models such as orientation tuning and exponential interval histograms. In addition, it makes testable predictions that follow from the γ latency coding.},
	urldate = {2022-12-16},
	journal = {Frontiers in Computational Neuroscience},
	author = {Ballard, Dana and Jehee, Janneke},
	year = {2011},
	keywords = {⛔ No INSPIRE recid found},
}

@article{feller_dynamic_1997,
	title = {Dynamic {Processes} {Shape} {Spatiotemporal} {Properties} of {Retinal} {Waves}},
	volume = {19},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S089662730080940X},
	doi = {10.1016/S0896-6273(00)80940-X},
	abstract = {In the developing mammalian retina, spontaneous waves of action potentials are present in the ganglion cell layer weeks before vision. These waves are known to be generated by a synaptically connected network of amacrine cells and retinal ganglion cells, and exhibit complex spatiotemporal patterns, characterized by shifting domains of coactivation. Here, we present a novel dynamical model consisting of two coupled populations of cells that quantitatively reproduces the experimentally observed domain sizes, interwave intervals, and wavefront velocity profiles. Model and experiment together show that the highly correlated activity generated by retinal waves can be explained by a combination of random spontaneous activation of cells and the past history of local retinal activity.},
	language = {en},
	number = {2},
	urldate = {2022-12-16},
	journal = {Neuron},
	author = {Feller, Marla B. and Butts, Daniel A. and Aaron, Holly L. and Rokhsar, Daniel S. and Shatz, Carla J.},
	month = aug,
	year = {1997},
	keywords = {⛔ No INSPIRE recid found},
	pages = {293--306},
}

@article{javanshir_advancements_2022,
	title = {Advancements in {Algorithms} and {Neuromorphic} {Hardware} for {Spiking} {Neural} {Networks}},
	volume = {34},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01499},
	doi = {10.1162/neco_a_01499},
	abstract = {Artificial neural networks (ANNs) have experienced a rapid advancement for their success in various application domains, including autonomous driving and drone vision. Researchers have been improving the performance efficiency and computational requirement of ANNs inspired by the mechanisms of the biological brain. Spiking neural networks (SNNs) provide a power-efficient and brain-inspired computing paradigm for machine learning applications. However, evaluating large-scale SNNs on classical von Neumann architectures (central processing units/graphics processing units) demands a high amount of power and time. Therefore, hardware designers have developed neuromorphic platforms to execute SNNs in and approach that combines fast processing and low power consumption. Recently, field-programmable gate arrays (FPGAs) have been considered promising candidates for implementing neuromorphic solutions due to their varied advantages, such as higher flexibility, shorter design, and excellent stability. This review aims to describe recent advances in SNNs and the neuromorphic hardware platforms (digital, analog, hybrid, and FPGA based) suitable for their implementation. We present that biological background of SNN learning, such as neuron models and information encoding techniques, followed by a categorization of SNN training. In addition, we describe state-of-the-art SNN simulators. Furthermore, we review and present FPGA-based hardware implementation of SNNs. Finally, we discuss some future directions for research in this field.},
	number = {6},
	urldate = {2022-12-16},
	journal = {Neural Computation},
	author = {Javanshir, Amirhossein and Nguyen, Thanh Thi and Mahmud, M. A. Parvez and Kouzani, Abbas Z.},
	month = may,
	year = {2022},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1289--1328},
}

@misc{fang_incorporating_2021,
	title = {Incorporating {Learnable} {Membrane} {Time} {Constant} to {Enhance} {Learning} of {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.05785},
	doi = {10.48550/arXiv.2007.05785},
	abstract = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothee and Huang, Tiejun and Tian, Yonghong},
	month = aug,
	year = {2021},
	note = {arXiv:2007.05785 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ⛔ No INSPIRE recid found},
}

@article{vinje_sparse_2000,
	title = {Sparse {Coding} and {Decorrelation} in {Primary} {Visual} {Cortex} {During} {Natural} {Vision}},
	volume = {287},
	url = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	doi = {10.1126/science.287.5456.1273},
	abstract = {Theoretical studies suggest that primary visual cortex (area V1) uses a sparse code to efficiently represent natural scenes. This issue was investigated by recording from V1 neurons in awake behaving macaques during both free viewing of natural scenes and conditions simulating natural vision. Stimulation of the nonclassical receptive field increases the selectivity and sparseness of individual V1 neurons, increases the sparseness of the population response distribution, and strongly decorrelates the responses of neuron pairs. These effects are due to both excitatory and suppressive modulation of the classical receptive field by the nonclassical receptive field and do not depend critically on the spatiotemporal structure of the stimuli. During natural vision, the classical and nonclassical receptive fields function together to form a sparse representation of the visual world. This sparse code may be computationally efficient for both early vision and higher visual processing.},
	number = {5456},
	urldate = {2022-12-15},
	journal = {Science},
	author = {Vinje, William E. and Gallant, Jack L.},
	month = feb,
	year = {2000},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1273--1276},
}

@article{decharms_primary_1996,
	title = {Primary cortical representation of sounds by the coordination of action-potential timing},
	volume = {381},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/381610a0},
	doi = {10.1038/381610a0},
	abstract = {CORTICAL population coding could in principle rely on either the mean rate of neuronal action potentials, or the relative timing of action potentials, or both. When a single sensory stimulus drives many neurons to fire at elevated rates, the spikes of these neurons become tightly synchronized1,2, which could be involved in 'binding' together individual firing-rate feature representations into a unified object percept3. Here we demonstrate that the relative timing of cortical action potentials can signal stimulus features themselves, a function even more basic than feature grouping. Populations of neurons in the primary auditory cortex can coordinate the relative timing of their action potentials such that spikes occur closer together in time during continuous stimuli. In this way cortical neurons can signal stimuli even when their firing rates do not change. Population coding based on relative spike timing can systematically signal stimulus features, it is topographically mapped, and it follows the stimulus time course even where mean firing rate does not.},
	language = {en},
	number = {6583},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {deCharms, R. Christopher and Merzenich, Michael M.},
	month = jun,
	year = {1996},
	note = {Number: 6583
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {610--613},
}

@article{maunsell_functional_1983,
	title = {Functional properties of neurons in middle temporal visual area of the macaque monkey. {I}. {Selectivity} for stimulus direction, speed, and orientation},
	volume = {49},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.1983.49.5.1127},
	doi = {10.1152/jn.1983.49.5.1127},
	abstract = {1. Recordings were made from single units in the middle temporal visual area (MT) of anesthetized, paralyzed macaque monkeys. A computer-driven stimulator was used to make quantitative tests of selectivity for stimulus direction, speed, and orientation. The data were taken from 168 units that were histologically identified as being in MT. 2. The results confirm previous reports of a high degree of direction selectivity in MT. The response above background to stimuli moving in a unit's preferred direction was, an average, 10.9 times that to stimuli moving in the opposite direction. There was a marked tendency for nearby units to have similar preferred directions. 3. Most units were also sharply tuned for the speed of stimulus motion. For some cells the response fell to less than half-maximal at speeds only a factor of two from the optimum; on average, responses were greater than half-maximal only over a 7.7-fold range of speed. The distribution of preferred speeds for different units was unimodal, with a peak near 32 degrees/s; the total range of preferred speeds extended from 2 to 256 degrees/s. Nearby units generally responded best to similar speeds of motion. 4. Most units in MT showed selectivity for stimulus orientation when tested with stationary, flashed bars. However, stationary stimuli generally elicited only brief responses; when averaged over the duration of the stimulus, the responses were much less than those to moving stimuli. The preferred orientation was usually, but not always, perpendicular to the preferred direction of movement. 5. A comparison of the results of the present study with a previous quantitative investigation in the owl monkey shows a striking similarity in response properties in MT of the two species. 6. The presence of both direction and speed selectivity in MT of the macaque suggests that this area is more specialized for the analysis of visual motion than has been previously recognized.},
	language = {en},
	number = {5},
	urldate = {2022-12-15},
	journal = {Journal of Neurophysiology},
	author = {Maunsell, J. H. and Van Essen, D. C.},
	month = may,
	year = {1983},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1127--1147},
}

@article{montemurro_phase--firing_2008,
	title = {Phase-of-{Firing} {Coding} of {Natural} {Visual} {Stimuli} in {Primary} {Visual} {Cortex}},
	volume = {18},
	issn = {0960-9822},
	url = {http://www.cell.com/current-biology/abstract/S0960-9822(08)00168-1},
	doi = {10.1016/j.cub.2008.02.023},
	language = {English},
	number = {5},
	urldate = {2022-12-15},
	journal = {Current Biology},
	author = {Montemurro, Marcelo A. and Rasch, Malte J. and Murayama, Yusuke and Logothetis, Nikos K. and Panzeri, Stefano},
	month = mar,
	year = {2008},
	pmid = {18328702},
	note = {Publisher: Elsevier},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	pages = {375--380},
}

@article{butts_temporal_2007,
	title = {Temporal precision in the neural code and the timescales of natural vision},
	volume = {449},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature06105},
	doi = {10.1038/nature06105},
	abstract = {In mammalian visual system, spikes evoked by visual stimuli have millisecond-scale timing even though the relevant timescales of visual processing themselves are much slower. It has therefore long been debated whether spike timing itself carries some form of the neural code. Now experiments in the lateral geniculate nucleus of cats, the part of the brain that is the primary processor of visual information, show that spike timing precision is not absolute for all classes of visual stimuli. Rather, the degree of precision is relative to the timescale of the stimulus, and this relatively high level of precision is required to construct an accurate representation of the stimulus.},
	language = {en},
	number = {7158},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {Butts, Daniel A. and Weng, Chong and Jin, Jianzhong and Yeh, Chun-I. and Lesica, Nicholas A. and Alonso, Jose-Manuel and Stanley, Garrett B.},
	month = sep,
	year = {2007},
	note = {Number: 7158
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {92--95},
}

@article{gouras_graded_1960,
	title = {Graded potentials of bream retina},
	volume = {152},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363334/},
	abstract = {Images
null},
	number = {3},
	urldate = {2022-12-14},
	journal = {The Journal of Physiology},
	author = {Gouras, P.},
	month = jul,
	year = {1960},
	pmid = {13828605},
	pmcid = {PMC1363334},
	keywords = {⛔ No INSPIRE recid found},
	pages = {487--505},
}

@article{bryant_spike_1976,
	title = {Spike initiation by transmembrane current: a white-noise analysis.},
	volume = {260},
	copyright = {© 1976 The Physiological Society},
	issn = {1469-7793},
	shorttitle = {Spike initiation by transmembrane current},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1976.sp011516},
	doi = {10.1113/jphysiol.1976.sp011516},
	abstract = {1. Those features of a transmembrane current correlated with spike initiation were examined in Aplysia neurones using a Gaussian white-noise stimulus. This stimulus has the advantages that it presents numerous wave forms in random order without prejudgement as to their efficacies, and that it allows straightforward statistical calculations. 2. Stimulation with a repeating segment of Gaussian white-noise current revealed remarkable invariance in the firing times of the tested neurones and indicated a high degree of reliability of their response. 3. Frequencies (less than 5 Hz) involved in spike triggering propagated faithfully for up to several millimetres, justifying intrasomatic current injection to examine spike initiation at the trigger locus. 4. Examination of current wave forms preceding spikes indicated that a wide variety could be effective. Hence, a statistical analysis was performed, including computation of probability densities, averages, standard deviations and correlation coefficients of pairs of current values. Each statistic was displayed as a function of time before the spike. 5. The average current trajectory preceding a spike was multiphasic and depended on the presence and polarity of a d.c. bias. An early relatively small inward- or outward-going phase was followed by a large outward phase before the spike. The early phase tended to oppose the polarity of the d.c. bias. 6. The late outward phase of the average current trajectory reached a maximum 40–75 msec before triggering the action potential (AP) and returned to near zero values at the moment of triggering. The fact that the current peak occurs in advance of the AP may be partially explained by a phase delay between the transmembrane current and potential. The failure of the average current trajectory to return to control values immediately following the peak argues for a positive role of the declining phase in spike triggering. 7. Probability densities preceding spikes were Gaussian, indicating that the average was also the most probable value. Although the densities were broad, confirming that spikes were preceded by a wide variety of current wave forms, their standard deviations were reduced significantly with respect to controls, suggesting preferred status of the average current trajectory in spike triggering. 8. The matrix of correlation coefficients between current pairs suggested that spikes tended to be preceded by wave forms that in part kept close to the average current trajectory and in part preserved its shape. 9. The average first and second derivatives of spike-evoking epochs revealed that current slope and acceleration, respectively, were most crucial in the last 200 msec before spike triggering, and that these dynamic stimulus components were more important for a cell maintained under a depolarizing, rather than a hyperpolarizing bias. 10...},
	language = {en},
	number = {2},
	urldate = {2022-12-13},
	journal = {The Journal of Physiology},
	author = {Bryant, H L and Segundo, J P},
	year = {1976},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1976.sp011516},
	keywords = {⛔ No INSPIRE recid found},
	pages = {279--314},
}

@article{perrinet_feature_2004,
	series = {Decoding and interfacing the brain: from neuronal assemblies to cyborgs},
	title = {Feature detection using spikes: {The} greedy approach},
	volume = {98},
	issn = {0928-4257},
	shorttitle = {Feature detection using spikes},
	url = {https://www.sciencedirect.com/science/article/pii/S0928425705000161},
	doi = {10.1016/j.jphysparis.2005.09.012},
	abstract = {A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (VI) solving this problem in the case where the signal may be described by a robust linear generative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to ‘Matching Pursuit’ and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neurons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this transformation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.},
	language = {en},
	number = {4},
	urldate = {2022-12-13},
	journal = {Journal of Physiology-Paris},
	author = {Perrinet, Laurent},
	month = jul,
	year = {2004},
	keywords = {Distributed probabilistic representation, Inverse linear model, Matching pursuit, Neuronal representation, Over-complete dictionaries, Sparse spike coding, Spike-event computation, ⛔ No INSPIRE recid found},
	pages = {530--539},
}

@article{perrinet_sparse_2002,
	title = {Sparse {Image} {Coding} {Using} an {Asynchronous} {Spiking} {Neural} {Network}},
	abstract = {In order to explore coding strategies in the retina, we use a wavelet-like transform which output is sparse, as is observed in biological retinas [4]. This transform is defined in the context of a one-pass feed-forward spiking neural network, and the output is the list of its neurons' spikes: it is recursively constructed using a greedy matching pursuit scheme which first selects higher contrast energy values.},
	author = {Perrinet, Laurent and Samuelides, Manuel},
	month = aug,
	year = {2002},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@book{dilorenzo_spike_2013,
	title = {Spike {Timing}: {Mechanisms} and {Function}},
	isbn = {978-1-4398-3815-0},
	shorttitle = {Spike {Timing}},
	abstract = {Neuronal communication forms the basis for all behavior, from the smallest movement to our grandest thought processes. Among the many mechanisms that support these functions, spike timing is among the most powerful and—until recently—perhaps the least studied. In the last two decades, however, the study of spike timing has exploded. The heightened interest is due to several factors. These include the development of physiological tools for measuring the activity of neural ensembles and analytical tools for assessing and characterizing spike timing. These advances are coupled with a growing appreciation of spike timing’s theoretical importance for the design principles of the brain.  Spike Timing: Mechanisms and Function examines the function of spike timing in sensory, motor, and integrative processes, providing readers with a broad perspective on how spike timing is produced and used by the nervous system. It brings together the work and ideas of leaders in the field to address current thinking as well as future possibilities.   The first section of the book describes the foundation for quantitative analysis and theory. It examines the information contained in spike timing, how it can be quantified, and how neural systems can extract it. The second section explores how input-output relationships are reflected in spike timing across a range of sensory systems.  Drawing together multiple perspectives, including theoretical and computational studies as well as experimental studies in a range of model systems, the book provides a firm background for investigators to consider spike timing as it applies to their own work. It also offers a glimpse of future advances related to mechanisms of spike timing and its role in neural function, such as the development of novel computational technologies.},
	language = {en},
	publisher = {CRC Press},
	author = {DiLorenzo, Patricia M. and Victor, Jonathan D.},
	month = may,
	year = {2013},
	note = {Google-Books-ID: KTHUIMUpQCUC},
	keywords = {Computers / Software Development \& Engineering / Systems Analysis \& Design, Medical / Biotechnology, Science / Life Sciences / Biophysics, Science / Life Sciences / Neuroscience, Science / Physics / General, Technology \& Engineering / Biomedical, ⛔ No INSPIRE recid found},
}

@inproceedings{arnold_conduction_2021,
	title = {Conduction delay plasticity can robustly learn spatiotemporal patterns embedded in noise},
	doi = {10.1109/IJCNN52387.2021.9533934},
	abstract = {Noise and temporal dynamics are ubiquitous in neural systems yet the computational consequences of these two phenomena interacting are not well studied. Temporal dynamics in spiking networks are often considered only implicitly as part of membrane time constants or synaptic transfer functions. We explicitly model temporal structure using plastic conduction delays between neuron's and characterise the influence of different kinds of noise on learning including temporal jitter, dropout, pattern size, and pattern presentation frequency. We simplify the conduction delay plasticity (CDP) rule called synaptic delay variance learning (SDVL) and demonstrate it is robust to several kinds of noise including; internal pattern jitter, number of pattern afferents, pattern presentation rate, and pattern spike dropout. In particular, after unsupervised training the simplified version of SDVL can achieve an accuracy of up to 99.7 percent averaged over 100 trials. These results demonstrate that learning algorithms based on explicitly modelling temporal structure in inputs can be functional and robust for unsupervised learning of spatiotemporal patterns across a range of noise conditions.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Arnold, Joshua and Stratton, Peter and Wiles, Janet},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Biological systems, Conduction delay, Delays, Jitter, Noise measurement, Spatiotemporal phenomena, Training, Transfer functions, delay learning, noise robust, plasticity, spiking neural network, ⛔ No INSPIRE recid found},
	pages = {1--10},
}

@article{rubinsky_spatio-temporal_2008,
	title = {Spatio-temporal motifs ‘remembered’ in neuronal networks following profound hypothermia},
	volume = {21},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608008001202},
	doi = {10.1016/j.neunet.2008.06.008},
	abstract = {Surgical procedures using hypothermic temperatures have been linked to complications such as seizures, impaired mental development and impaired memory. Although there is some evidence that the profound hypothermia ({\textless}12 ∘C) used in these procedures may be contributing to these neurological impairments, skepticism remains because of lack of evidence from experimental studies isolating the effects of hypothermia on neuronal networks. In order to attain a better understanding of profound hypothermia effects on neurons during surgical procedures, we applied cold to a cultured in-vitro neuronal network. The typical pattern of activity of such cultures is in the form of synchronized bursts, in which most of the recorded neurons fire action potentials in a short time period. In most cases, the bursting activity shows one or more repeating precise spatio-temporal patterns (motifs) that are sustained over long periods of time. In this experimental study, neuronal networks grown on microelectrode arrays (MEA) are subjected to profound hypothermia for an hour and the collective dynamics of the network as a whole are assessed. We show, by using a similarity analysis that compares changes in the time delays between neuronal activation at different burst motifs, that neuronal networks survive total inhibition by profound hypothermia and retain their intrinsic synchronized burst motifs even with substantial generalized neuronal degeneration. By applying multiple sessions of cold, we also show a marked monotonic reduction in the rate of burst firing and in the number of spikes of each neuron after each session.},
	language = {en},
	number = {9},
	urldate = {2022-11-19},
	journal = {Neural Networks},
	author = {Rubinsky, Liel and Raichman, Nadav and Lavee, Jacob and Frenk, Hanan and Ben-Jacob, Eshel},
	month = nov,
	year = {2008},
	keywords = {Low temperature, Microelectrode arrays, Neuronal cultures, Synchronized bursting events, ⛔ No INSPIRE recid found},
	pages = {1232--1237},
}

@article{coull_distinction_2022,
	title = {The distinction between temporal order and duration processing, and implications for schizophrenia},
	volume = {1},
	doi = {10.1038/s44159-022-00038-y},
	number = {5},
	journal = {Nature Reviews Psychology},
	author = {Coull, Jennifer T. and Giersch, Anne},
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {⛔ No INSPIRE recid found},
	pages = {257--271},
}

@article{furber_overview_2013,
	title = {Overview of the {SpiNNaker} {System} {Architecture}},
	volume = {62},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/6226357/},
	doi = {10.1109/TC.2012.142},
	number = {12},
	urldate = {2022-11-13},
	journal = {IEEE Transactions on Computers},
	author = {Furber, Steve B. and Lester, David R. and Plana, Luis A. and Garside, Jim D. and Painkras, Eustace and Temple, Steve and Brown, Andrew D.},
	month = dec,
	year = {2013},
	keywords = {⛔ No INSPIRE recid found},
	pages = {2454--2467},
}

@article{merolla_million_2014,
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1254642},
	doi = {10.1126/science.1254642},
	abstract = {Modeling computer chips on real brains
            
              Computers are nowhere near as versatile as our own brains. Merolla
              et al.
              applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.
            
            
              Science
              , this issue p.
              668
            
          , 
            A large-scale computer chip mimics many features of a real brain.
          , 
            Inspired by the brain’s structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	language = {en},
	number = {6197},
	urldate = {2022-11-13},
	journal = {Science},
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	month = aug,
	year = {2014},
	keywords = {⛔ No INSPIRE recid found},
	pages = {668--673},
}

@book{furber_spinnaker_2020,
	title = {{SpiNNaker}: {A} {Spiking} {Neural} {Network} {Architecture}},
	isbn = {978-1-68083-652-3 978-1-68083-653-0},
	shorttitle = {{SpiNNaker}},
	url = {https://nowpublishers.com/article/BookDetails/9781680836523},
	urldate = {2022-11-13},
	publisher = {Now Publishers},
	editor = {Furber, Steve and Bogdan, Petrut},
	year = {2020},
	doi = {10.1561/9781680836523},
	keywords = {⛔ No INSPIRE recid found},
}

@article{davies_loihi_2018,
	title = {Loihi: {A} {Neuromorphic} {Manycore} {Processor} with {On}-{Chip} {Learning}},
	volume = {38},
	issn = {0272-1732, 1937-4143},
	shorttitle = {Loihi},
	url = {https://ieeexplore.ieee.org/document/8259423/},
	doi = {10.1109/MM.2018.112130359},
	number = {1},
	urldate = {2022-11-13},
	journal = {IEEE Micro},
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	month = jan,
	year = {2018},
	keywords = {⛔ No INSPIRE recid found},
	pages = {82--99},
}

@article{fields_myelin_2020,
	title = {Myelin makes memories},
	volume = {23},
	issn = {1546-1726},
	doi = {10.1038/s41593-020-0606-x},
	language = {eng},
	number = {4},
	journal = {Nature Neuroscience},
	author = {Fields, R. Douglas and Bukalo, Olena},
	month = apr,
	year = {2020},
	pmid = {32094969},
	keywords = {Animals, Memory, Memory Consolidation, Mice, Myelin Sheath, ⛔ No INSPIRE recid found},
	pages = {469--470},
}

@article{madadi_asl_dendritic_2018,
	title = {Dendritic and {Axonal} {Propagation} {Delays} {May} {Shape} {Neuronal} {Networks} {With} {Plastic} {Synapses}},
	volume = {9},
	issn = {1664-042X},
	doi = {10.3389/fphys.2018.01849},
	abstract = {Biological neuronal networks are highly adaptive and plastic. For instance, spike-timing-dependent plasticity (STDP) is a core mechanism which adapts the synaptic strengths based on the relative timing of pre- and postsynaptic spikes. In various fields of physiology, time delays cause a plethora of biologically relevant dynamical phenomena. However, time delays increase the complexity of model systems together with the computational and theoretical analysis burden. Accordingly, in computational neuronal network studies propagation delays were often neglected. As a downside, a classic STDP rule in oscillatory neurons without propagation delays is unable to give rise to bidirectional synaptic couplings, i.e., loops or uncoupled states. This is at variance with basic experimental results. In this mini review, we focus on recent theoretical studies focusing on how things change in the presence of propagation delays. Realistic propagation delays may lead to the emergence of neuronal activity and synaptic connectivity patterns, which cannot be captured by classic STDP models. In fact, propagation delays determine the inventory of attractor states and shape their basins of attractions. The results reviewed here enable to overcome fundamental discrepancies between theory and experiments. Furthermore, these findings are relevant for the development of therapeutic brain stimulation techniques aiming at shifting the diseased brain to more favorable attractor states.},
	language = {eng},
	journal = {Frontiers in Physiology},
	author = {Madadi Asl, Mojtaba and Valizadeh, Alireza and Tass, Peter A.},
	year = {2018},
	pmid = {30618847},
	pmcid = {PMC6307091},
	keywords = {living systems, mathematical modeling, propagation delays, spike-timing-dependent plasticity, synchronization, ⛔ No INSPIRE recid found},
	pages = {1849},
}

@article{neftci_surrogate_2019,
	title = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}: {Bringing} the {Power} of {Gradient}-{Based} {Optimization} to {Spiking} {Neural} {Networks}},
	volume = {36},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8891809/},
	doi = {10.1109/MSP.2019.2931595},
	number = {6},
	urldate = {2022-11-14},
	journal = {IEEE Signal Processing Magazine},
	author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
	month = nov,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {51--63},
}

@article{fields_new_2015,
	title = {A new mechanism of nervous system plasticity: activity-dependent myelination},
	volume = {16},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	shorttitle = {A new mechanism of nervous system plasticity},
	url = {https://www.nature.com/articles/nrn4023},
	doi = {10.1038/nrn4023},
	abstract = {The precise timing of impulse transmission along axons is crucial for synaptic plasticity and brain oscillations, and is partly determined by myelin thickness. In this Opinion article, R. Douglas Fields discusses how electrical activity influences myelin thickness and thus conduction velocity and circuit properties.},
	language = {en},
	number = {12},
	urldate = {2021-01-07},
	journal = {Nature Reviews Neuroscience},
	author = {Fields, R. Douglas},
	month = dec,
	year = {2015},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {biology, delay-learning, myelination, ⛔ No INSPIRE recid found},
	pages = {756--767},
}

@article{steadman_disruption_2020,
	title = {Disruption of {Oligodendrogenesis} {Impairs} {Memory} {Consolidation} in {Adult} {Mice}},
	volume = {105},
	issn = {0896-6273},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7579726/},
	doi = {10.1016/j.neuron.2019.10.013},
	abstract = {The generation of myelin-forming oligodendrocytes persists throughout life and is regulated by neural activity. Here we tested whether experience-driven changes in oligodendrogenesis are important for memory consolidation. We found that water maze learning promotes oligodendrogenesis and de
novo myelination in the cortex and associated white matter tracts. Preventing these learning-induced increases in oligodendrogenesis without affecting existing oligodendrocytes impaired memory consolidation of water maze, as well as contextual fear, memories. These results suggest that de
novo myelination tunes activated circuits, promoting coordinated activity that is important for memory consolidation. Consistent with this, contextual fear learning increased the coupling of hippocampal sharp wave ripples and cortical spindles, and these learning-induced increases in ripple-spindle coupling were blocked when oligodendrogenesis was suppressed. Our results identify a non-neuronal form of plasticity that remodels hippocampal-cortical networks following learning and is required for memory consolidation., 
          
        , Experience-dependent de
novo myelination may fine-tune activated circuits by promoting brain synchrony, important for memory consolidation. Steadman et al. find that blocking this form of adaptive myelination prevents learning-induced increases in coordinated activity and impairs memory consolidation.},
	number = {1},
	urldate = {2022-11-14},
	journal = {Neuron},
	author = {Steadman, Patrick E. and Xia, Frances and Ahmed, Moriam and Mocle, Andrew J. and Penning, Amber R.A. and Geraghty, Anna C. and Steenland, Hendrik W. and Monje, Michelle and Josselyn, Sheena A. and Frankland, Paul W.},
	month = jan,
	year = {2020},
	pmid = {31753579},
	pmcid = {PMC7579726},
	keywords = {⛔ No INSPIRE recid found},
	pages = {150--164.e6},
}

@article{pan_preservation_2020,
	title = {Preservation of a remote fear memory requires new myelin formation},
	volume = {23},
	issn = {1546-1726},
	doi = {10.1038/s41593-019-0582-1},
	abstract = {Experience-dependent myelination is hypothesized to shape neural circuit function and subsequent behavioral output. Using a contextual fear memory task in mice, we demonstrate that fear learning induces oligodendrocyte precursor cells to proliferate and differentiate into myelinating oligodendrocytes in the medial prefrontal cortex. Transgenic animals that cannot form new myelin exhibit deficient remote, but not recent, fear memory recall. Recording population calcium dynamics by fiber photometry, we observe that the neuronal response to conditioned context cues evolves over time in the medial prefrontal cortex, but not in animals that cannot form new myelin. Finally, we demonstrate that pharmacological induction of new myelin formation with clemastine fumarate improves remote memory recall and promotes fear generalization. Thus, bidirectional manipulation of myelin plasticity functionally affects behavior and neurophysiology, which suggests that neural activity during fear learning instructs the formation of new myelin, which in turn supports the consolidation and/or retrieval of remote fear memories.},
	language = {eng},
	number = {4},
	journal = {Nature Neuroscience},
	author = {Pan, Simon and Mayoral, Sonia R. and Choi, Hye Sun and Chan, Jonah R. and Kheirbek, Mazen A.},
	month = apr,
	year = {2020},
	pmid = {32042175},
	pmcid = {PMC7213814},
	keywords = {Animals, Cell Proliferation, Conditioning, Classical, Fear, Memory, Long-Term, Mice, Mice, Transgenic, Myelin Sheath, Oligodendrocyte Precursor Cells, Oligodendrocyte Transcription Factor 2, Prefrontal Cortex, ⛔ No INSPIRE recid found},
	pages = {487--499},
}

@article{wan_impaired_2020,
	title = {Impaired {Postnatal} {Myelination} in a {Conditional} {Knockout} {Mouse} for the {Ferritin} {Heavy} {Chain} in {Oligodendroglial} {Cells}},
	volume = {40},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7531557/},
	doi = {10.1523/JNEUROSCI.1281-20.2020},
	abstract = {To define the importance of iron storage in oligodendrocyte development and function, the ferritin heavy subunit (Fth) was specifically deleted in oligodendroglial cells. Blocking Fth synthesis in Sox10 or NG2-positive oligodendrocytes during the first or the third postnatal week significantly reduces oligodendrocyte iron storage and maturation. The brain of Fth KO animals presented an important decrease in the expression of myelin proteins and a substantial reduction in the percentage of myelinated axons. This hypomyelination was accompanied by a decline in the number of myelinating oligodendrocytes and with a reduction in proliferating oligodendrocyte progenitor cells (OPCs). Importantly, deleting Fth in Sox10-positive oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte quantities. We also tested the capacity of Fth-deficient OPCs to remyelinate the adult brain in the cuprizone model of myelin injury and repair. Fth deletion in NG2-positive OPCs significantly reduces the number of mature oligodendrocytes and myelin production throughout the remyelination process. Furthermore, the corpus callosum of Fth KO animals presented a significant decrease in the percentage of remyelinated axons and a substantial reduction in the average myelin thickness. These results indicate that Fth synthesis during the first three postnatal weeks is important for an appropriate oligodendrocyte development, and suggest that Fth iron storage in adult OPCs is also essential for an effective remyelination of the mouse brain., SIGNIFICANCE STATEMENT To define the importance of iron storage in oligodendrocyte function, we have deleted the ferritin heavy chain (Fth) specifically in the oligodendrocyte lineage. Fth ablation in oligodendroglial cells throughout early postnatal development significantly reduces oligodendrocyte maturation and myelination. In contrast, deletion of Fth in oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte numbers. We have also tested the consequences of disrupting Fth iron storage in oligodendrocyte progenitor cells (OPCs) after demyelination. We have found that Fth deletion in NG2-positive OPCs significantly delays the remyelination process in the adult brain. Therefore, Fth iron storage is essential for early oligodendrocyte development as well as for OPC maturation in the demyelinated adult brain.},
	number = {40},
	urldate = {2022-11-14},
	journal = {The Journal of Neuroscience},
	author = {Wan, Rensheng and Cheli, Veronica T. and Santiago-González, Diara A. and Rosenblum, Shaina L. and Wan, Qiuchen and Paez, Pablo M.},
	month = sep,
	year = {2020},
	pmid = {32868463},
	pmcid = {PMC7531557},
	keywords = {⛔ No INSPIRE recid found},
	pages = {7609--7624},
}

@article{xue_demyelination_2021,
	title = {Demyelination of the {Optic} {Nerve}: {An} {Underlying} {Factor} in {Glaucoma}?},
	volume = {13},
	issn = {1663-4365},
	shorttitle = {Demyelination of the {Optic} {Nerve}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8593209/},
	doi = {10.3389/fnagi.2021.701322},
	abstract = {Neurodegenerative disorders are characterized by typical neuronal degeneration and axonal loss in the central nervous system (CNS). Demyelination occurs when myelin or oligodendrocytes experience damage. Pathological changes in demyelination contribute to neurodegenerative diseases and worsen clinical symptoms during disease progression. Glaucoma is a neurodegenerative disease characterized by progressive degeneration of retinal ganglion cells (RGCs) and the optic nerve. Since it is not yet well understood, we hypothesized that demyelination could play a significant role in glaucoma. Therefore, this study started with the morphological and functional manifestations of demyelination in the CNS. Then, we discussed the main mechanisms of demyelination in terms of oxidative stress, mitochondrial damage, and immuno-inflammatory responses. Finally, we summarized the existing research on the relationship between optic nerve demyelination and glaucoma, aiming to inspire effective treatment plans for glaucoma in the future.},
	urldate = {2022-11-13},
	journal = {Frontiers in Aging Neuroscience},
	author = {Xue, Jingfei and Zhu, Yingting and Liu, Zhe and Lin, Jicheng and Li, Yangjiani and Li, Yiqing and Zhuo, Yehong},
	month = nov,
	year = {2021},
	pmid = {34795572},
	pmcid = {PMC8593209},
	keywords = {⛔ No INSPIRE recid found},
	pages = {701322},
}

@article{nave_axonal_2006,
	title = {Axonal regulation of myelination by neuregulin 1},
	volume = {16},
	issn = {0959-4388},
	doi = {10.1016/j.conb.2006.08.008},
	abstract = {Neuregulins comprise a family of epidermal growth factor-like ligands that interact with ErbB receptor tyrosine kinases to control many aspects of neural development. One of the most dramatic effects of neuregulin-1 is on glial cell differentiation. The membrane-bound neuregulin-1 type III isoform is an axonal ligand for glial ErbB receptors that regulates the early Schwann cell lineage, including the generation of precursors. Recent studies have shown that the amount of neuregulin-1 type III expressed on axons also dictates the glial phenotype, with a threshold level triggering Schwann cell myelination. Remarkably, neuregulin-1 type III also regulates Schwann cell membrane growth to adjust myelin sheath thickness to match axon caliber precisely. Whether this signaling system operates in central nervous system myelination remains an open question of major importance for human demyelinating diseases.},
	language = {eng},
	number = {5},
	journal = {Current Opinion in Neurobiology},
	author = {Nave, Klaus-Armin and Salzer, James L.},
	month = oct,
	year = {2006},
	pmid = {16962312},
	keywords = {Animals, Axons, Cell Differentiation, Cell Lineage, Humans, Myelin Sheath, Neuregulin-1, Neuroglia, Protein Isoforms, Receptor, ErbB-2, Receptor, ErbB-3, Signal Transduction, Stem Cells, ⛔ No INSPIRE recid found},
	pages = {492--500},
}

@article{baraban_ca2_2018,
	title = {Ca2+ activity signatures of myelin sheath formation and growth in vivo},
	volume = {21},
	issn = {1097-6256},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5742537/},
	doi = {10.1038/s41593-017-0040-x},
	abstract = {During myelination, individual oligodendrocytes initially over-produce short myelin sheaths that are either retracted or stabilised. By live imaging oligodendrocyte Ca2+ activity in vivo, we find that high-amplitude long-duration Ca2+ transients in sheaths prefigure retractions, mediated by calpain. Following stabilisation, myelin sheaths grow along axons, and we find that higher frequency Ca2+ transient activity in sheaths precedes faster elongation. Our data implicate local Ca2+ signalling in regulating distinct stages of myelination.},
	number = {1},
	urldate = {2022-11-13},
	journal = {Nature neuroscience},
	author = {Baraban, Marion and Koudelka, Sigrid and Lyons, David A},
	month = jan,
	year = {2018},
	pmid = {29230058},
	pmcid = {PMC5742537},
	keywords = {⛔ No INSPIRE recid found},
	pages = {19--23},
}

@article{kuhn_oligodendrocytes_2019,
	title = {Oligodendrocytes in {Development}, {Myelin} {Generation} and {Beyond}},
	volume = {8},
	issn = {2073-4409},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6912544/},
	doi = {10.3390/cells8111424},
	abstract = {Oligodendrocytes are the myelinating cells of the central nervous system (CNS) that are generated from oligodendrocyte progenitor cells (OPC). OPC are distributed throughout the CNS and represent a pool of migratory and proliferative adult progenitor cells that can differentiate into oligodendrocytes. The central function of oligodendrocytes is to generate myelin, which is an extended membrane from the cell that wraps tightly around axons. Due to this energy consuming process and the associated high metabolic turnover oligodendrocytes are vulnerable to cytotoxic and excitotoxic factors. Oligodendrocyte pathology is therefore evident in a range of disorders including multiple sclerosis, schizophrenia and Alzheimer’s disease. Deceased oligodendrocytes can be replenished from the adult OPC pool and lost myelin can be regenerated during remyelination, which can prevent axonal degeneration and can restore function. Cell population studies have recently identified novel immunomodulatory functions of oligodendrocytes, the implications of which, e.g., for diseases with primary oligodendrocyte pathology, are not yet clear. Here, we review the journey of oligodendrocytes from the embryonic stage to their role in homeostasis and their fate in disease. We will also discuss the most common models used to study oligodendrocytes and describe newly discovered functions of oligodendrocytes.},
	number = {11},
	urldate = {2022-11-13},
	journal = {Cells},
	author = {Kuhn, Sarah and Gritti, Laura and Crooks, Daniel and Dombrowski, Yvonne},
	month = nov,
	year = {2019},
	pmid = {31726662},
	pmcid = {PMC6912544},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1424},
}

@article{cullen_periaxonal_2021,
	title = {Periaxonal and nodal plasticities modulate action potential conduction in the adult mouse brain},
	volume = {34},
	issn = {2211-1247},
	url = {https://www.cell.com/cell-reports/abstract/S2211-1247(20)31630-2},
	doi = {10.1016/j.celrep.2020.108641},
	language = {English},
	number = {3},
	urldate = {2022-11-13},
	journal = {Cell Reports},
	author = {Cullen, Carlie L. and Pepper, Renee E. and Clutterbuck, Mackenzie T. and Pitman, Kimberley A. and Oorschot, Viola and Auderset, Loic and Tang, Alexander D. and Ramm, Georg and Emery, Ben and Rodger, Jennifer and Jolivet, Renaud B. and Young, Kaylene M.},
	month = jan,
	year = {2021},
	pmid = {33472075},
	note = {Publisher: Elsevier},
	keywords = {action potential, computational modeling, conduction velocity, myelin, node of Ranvier, oligodendrocyte, periaxonal space, plasticity, spatial learning, transcranial magnetic stimulation, ⛔ No INSPIRE recid found},
}

@article{gibson_neuronal_2014,
	title = {Neuronal {Activity} {Promotes} {Oligodendrogenesis} and {Adaptive} {Myelination} in the {Mammalian} {Brain}},
	volume = {344},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096908/},
	doi = {10.1126/science.1252304},
	abstract = {Myelination of the central nervous system requires the generation of functionally mature oligodendrocytes from oligodendrocyte precursor cells (OPCs). Electrically active neurons may influence OPC function and selectively instruct myelination of an active neural circuit. In this work, we use optogenetic stimulation of the premotor cortex in awake, behaving mice to demonstrate that neuronal activity elicits a mitogenic response of neural progenitor cells and OPCs, promotes oligodendrogenesis, and increases myelination within the deep layers of the premotor cortex and subcortical white matter. We further show that this neuronal activity–regulated oligodendrogenesis and myelination is associated with improved motor function of the corresponding limb. Oligodendrogenesis and myelination appear necessary for the observed functional improvement, as epigenetic blockade of oligodendrocyte differentiation and myelin changes prevents the activity-regulated behavioral improvement.},
	number = {6183},
	urldate = {2022-11-13},
	journal = {Science (New York, N.Y.)},
	author = {Gibson, Erin M. and Purger, David and Mount, Christopher W. and Goldstein, Andrea K. and Lin, Grant L. and Wood, Lauren S. and Inema, Ingrid and Miller, Sarah E. and Bieri, Gregor and Zuchero, J. Bradley and Barres, Ben A. and Woo, Pamelyn J. and Vogel, Hannes and Monje, Michelle},
	month = may,
	year = {2014},
	pmid = {24727982},
	pmcid = {PMC4096908},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1252304},
}

@article{spencer_compensation_2018,
	title = {Compensation for {Traveling} {Wave} {Delay} {Through} {Selection} of {Dendritic} {Delays} {Using} {Spike}-{Timing}-{Dependent} {Plasticity} in a {Model} of the {Auditory} {Brainstem}},
	volume = {12},
	issn = {1662-5188},
	doi = {10.3389/fncom.2018.00036},
	abstract = {Asynchrony among synaptic inputs may prevent a neuron from responding to behaviorally relevant sensory stimuli. For example, "octopus cells" are monaural neurons in the auditory brainstem of mammals that receive input from auditory nerve fibers (ANFs) representing a broad band of sound frequencies. Octopus cells are known to respond with finely timed action potentials at the onset of sounds despite the fact that due to the traveling wave delay in the cochlea, synaptic input from the auditory nerve is temporally diffuse. This paper provides a proof of principle that the octopus cells' dendritic delay may provide compensation for this input asynchrony, and that synaptic weights may be adjusted by a spike-timing dependent plasticity (STDP) learning rule. This paper used a leaky integrate and fire model of an octopus cell modified to include a "rate threshold," a property that is known to create the appropriate onset response in octopus cells. Repeated audio click stimuli were passed to a realistic auditory nerve model which provided the synaptic input to the octopus cell model. A genetic algorithm was used to find the parameters of the STDP learning rule that reproduced the microscopically observed synaptic connectivity. With these selected parameter values it was shown that the STDP learning rule was capable of adjusting the values of a large number of input synaptic weights, creating a configuration that compensated the traveling wave delay of the cochlea.},
	language = {eng},
	journal = {Frontiers in Computational Neuroscience},
	author = {Spencer, Martin J. and Meffin, Hamish and Burkitt, Anthony N. and Grayden, David B.},
	year = {2018},
	pmid = {29922141},
	pmcid = {PMC5996126},
	keywords = {auditory brainstem, cochlear nucleus, dendritic delay, octopus cells, spike-timing dependent plasticity, ⛔ No INSPIRE recid found},
	pages = {36},
}

@book{mead_analog_1989,
	title = {Analog {VLSI} {Implementation} of {Neural} {Systems}},
	isbn = {978-0-7923-9040-4},
	abstract = {This volume contains the proceedings of a workshop on Analog Integrated Neural Systems held May 8, 1989, in connection with the International Symposium on Circuits and Systems. The presentations were chosen to encompass the entire range of topics currently under study in this exciting new discipline. Stringent acceptance requirements were placed on contributions: (1) each description was required to include detailed characterization of a working chip, and (2) each design was not to have been published previously. In several cases, the status of the project was not known until a few weeks before the meeting date. As a result, some of the most recent innovative work in the field was presented. Because this discipline is evolving rapidly, each project is very much a work in progress. Authors were asked to devote considerable attention to the shortcomings of their designs, as well as to the notable successes they achieved. In this way, other workers can now avoid stumbling into the same traps, and evolution can proceed more rapidly (and less painfully). The chapters in this volume are presented in the same order as the corresponding presentations at the workshop. The first two chapters are concerned with fmding solutions to complex optimization problems under a predefmed set of constraints. The first chapter reports what is, to the best of our knowledge, the first neural-chip design. In each case, the physics of the underlying electronic medium is used to represent a cost function in a natural way, using only nearest-neighbor connectivity.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Mead, Carver and Ismail, Mohammed},
	month = aug,
	year = {1989},
	note = {Google-Books-ID: 9e29dOiXeiMC},
	keywords = {Computers / CAD-CAM, Computers / Computer Vision \& Pattern Recognition, Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / Circuits / General, Technology \& Engineering / Electronics / General, Technology \& Engineering / Imaging Systems, ⛔ No INSPIRE recid found},
}

@article{benjamin_neurogrid_2014,
	title = {Neurogrid: {A} {Mixed}-{Analog}-{Digital} {Multichip} {System} for {Large}-{Scale} {Neural} {Simulations}},
	volume = {102},
	issn = {1558-2256},
	shorttitle = {Neurogrid},
	doi = {10.1109/JPROC.2014.2313565},
	abstract = {In this paper, we describe the design of Neurogrid, a neuromorphic system for simulating large-scale neural models in real time. Neuromorphic systems realize the function of biological neural systems by emulating their structure. Designers of such systems face three major design choices: 1) whether to emulate the four neural elements-axonal arbor, synapse, dendritic tree, and soma-with dedicated or shared electronic circuits; 2) whether to implement these electronic circuits in an analog or digital manner; and 3) whether to interconnect arrays of these silicon neurons with a mesh or a tree network. The choices we made were: 1) we emulated all neural elements except the soma with shared electronic circuits; this choice maximized the number of synaptic connections; 2) we realized all electronic circuits except those for axonal arbors in an analog manner; this choice maximized energy efficiency; and 3) we interconnected neural arrays in a tree network; this choice maximized throughput. These three choices made it possible to simulate a million neurons with billions of synaptic connections in real time-for the first time-using 16 Neurocores integrated on a board that consumes three watts.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R. and Bussat, Jean-Marie and Alvarez-Icaza, Rodrigo and Arthur, John V. and Merolla, Paul A. and Boahen, Kwabena},
	month = may,
	year = {2014},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Analog circuits, Computer architecture, Electronic circuits, Integrated circuit modeling, Nerve fibers, Neural networks, Neuroscience, Random access memory, Synchronous digital hierarchy, application specific integrated circuits, asynchronous circuits, brain modeling, computational neuroscience, interconnection networks, mixed analog-digital integrated circuits, neural network hardware, neuromorphic electronic systems, ⛔ No INSPIRE recid found},
	pages = {699--716},
}

@article{duncan_neuron-oligodendrocyte_2021,
	title = {Neuron-{Oligodendrocyte} {Interactions} in the {Structure} and {Integrity} of {Axons}},
	volume = {9},
	issn = {2296-634X},
	url = {https://www.frontiersin.org/articles/10.3389/fcell.2021.653101},
	doi = {10.3389/fcell.2021.653101},
	abstract = {The myelination of axons by oligodendrocytes is a highly complex cell-to-cell interaction. Oligodendrocytes and axons have a reciprocal signaling relationship in which oligodendrocytes receive cues from axons that direct their myelination, and oligodendrocytes subsequently shape axonal structure and conduction. Oligodendrocytes are necessary for the maturation of excitatory domains on the axon including nodes of Ranvier, help buffer potassium, and support neuronal energy metabolism. Disruption of the oligodendrocyte-axon unit in traumatic injuries, Alzheimer’s disease and demyelinating diseases such as multiple sclerosis results in axonal dysfunction and can culminate in neurodegeneration. In this review, we discuss the mechanisms by which demyelination and loss of oligodendrocytes compromise axons. We highlight the intra-axonal cascades initiated by demyelination that can result in irreversible axonal damage. Both the restoration of oligodendrocyte myelination or neuroprotective therapies targeting these intra-axonal cascades are likely to have therapeutic potential in disorders in which oligodendrocyte support of axons is disrupted.},
	urldate = {2022-11-13},
	journal = {Frontiers in Cell and Developmental Biology},
	author = {Duncan, Greg J. and Simkins, Tyrell J. and Emery, Ben},
	year = {2021},
	keywords = {⛔ No INSPIRE recid found},
}

@article{stimberg_brian_2019,
	title = {Brian 2, an intuitive and efficient neural simulator},
	volume = {8},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/47314},
	doi = {10.7554/eLife.47314},
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	language = {en},
	urldate = {2022-11-14},
	journal = {eLife},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
	month = aug,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {e47314},
}

@article{hazan_bindsnet_2018,
	title = {{BindsNET}: {A} {Machine} {Learning}-{Oriented} {Spiking} {Neural} {Networks} {Library} in {Python}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {{BindsNET}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00089},
	doi = {10.3389/fninf.2018.00089},
	abstract = {The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared toward machine learning and reinforcement learning. Our software, called BindsNET1, enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on the PyTorch deep neural networks library, facilitating the implementation of spiking neural networks on fast CPU and GPU computational platforms. Moreover, the BindsNET framework can be adjusted to utilize other existing computing and hardware backends; e.g., TensorFlow and SpiNNaker. We provide an interface with the OpenAI gym library, allowing for training and evaluation of spiking networks on reinforcement learning environments. We argue that this package facilitates the use of spiking networks for large-scale machine learning problems and show some simple examples by using BindsNET in practice.},
	urldate = {2022-11-14},
	journal = {Frontiers in Neuroinformatics},
	author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
	year = {2018},
	keywords = {⛔ No INSPIRE recid found},
}

@inproceedings{farquhar_field_2006,
	address = {Island of Kos, Greece},
	title = {A {Field} {Programmable} {Neural} {Array}},
	isbn = {978-0-7803-9389-9},
	url = {http://ieeexplore.ieee.org/document/1693534/},
	doi = {10.1109/ISCAS.2006.1693534},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2006 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	publisher = {IEEE},
	author = {Farquhar, E. and Gordon, C. and Hasler, P.},
	year = {2006},
	keywords = {⛔ No INSPIRE recid found},
	pages = {4114--4117},
}

@article{markram_introducing_2011,
	series = {Proceedings of the 2nd {European} {Future} {Technologies} {Conference} and {Exhibition} 2011 ({FET} 11)},
	title = {Introducing the {Human} {Brain} {Project}},
	volume = {7},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050911006806},
	doi = {10.1016/j.procs.2011.12.015},
	abstract = {The Human Brain Project (HBP) is a candidate project in the European Union's FET Flagship Program, funded by the ICT Program in the Seventh Framework Program. The project will develop a new integrated strategy for understanding the human brain and a novel research platform that will integrate all the data and knowledge we can acquire about the structure and function of the brain and use it to build unifying models that can be validated by simulations running on supercomputers. The project will drive the development of supercomputing for the life sciences, generate new neuroscientific data as a benchmark for modeling, develop radically new tools for informatics, modeling and simulation, and build virtual laboratories for collaborative basic and clinical studies, drug simulation and virtual prototyping of neuroprosthetic, neuromorphic, and robotic devices.},
	language = {en},
	urldate = {2022-11-14},
	journal = {Procedia Computer Science},
	author = {Markram, Henry and Meier, Karlheinz and Lippert, Thomas and Grillner, Sten and Frackowiak, Richard and Dehaene, Stanislas and Knoll, Alois and Sompolinsky, Haim and Verstreken, Kris and DeFelipe, Javier and Grant, Seth and Changeux, Jean-Pierre and Saria, Alois},
	month = jan,
	year = {2011},
	keywords = {HPC, Human brain, medicine, modeling, neuroinformatics, neuromorphics, neuroprosthetics, neurorobotics, neuroscience, simulation, supercomputing, ⛔ No INSPIRE recid found},
	pages = {39--42},
}

@inproceedings{schemmel_wafer-scale_2010,
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	doi = {10.1109/ISCAS.2010.5536970},
	abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project "FACETS" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a unified neural system description language, called PyNN, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within FACETS is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Schemmel, Johannes and Brüderle, Daniel and Grübl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	month = may,
	year = {2010},
	note = {ISSN: 2158-1525},
	keywords = {Biological neural networks, Biological system modeling, Biological tissues, Biology computing, Hardware, Large-scale systems, Neuromorphics, Numerical simulation, Semiconductor device modeling, Usability, ⛔ No INSPIRE recid found},
	pages = {1947--1950},
}

@inproceedings{diehl_efficient_2014,
	address = {Beijing, China},
	title = {Efficient implementation of {STDP} rules on {SpiNNaker} neuromorphic hardware},
	isbn = {978-1-4799-1484-5 978-1-4799-6627-1},
	url = {https://ieeexplore.ieee.org/document/6889876},
	doi = {10.1109/IJCNN.2014.6889876},
	abstract = {Recent development of neuromorphic hardware offers great potential to speed up simulations of neural networks. SpiNNaker is a neuromorphic hardware and software system designed to be scalable and ﬂexible enough to implement a variety of different types of simulations of neural systems, including spiking simulations with plasticity and learning. Spiketiming dependent plasticity (STDP) rules are the most common form of learning used in spiking networks. However, to date very few such rules have been implemented on SpiNNaker, in part because implementations must be designed to ﬁt the specialized nature of the hardware. Here we explain how general STDP rules can be efﬁciently implemented in the SpiNNaker system. We give two examples of applications of the implemented rule: learning of a temporal sequence, and balancing inhibition and excitation of a neural network. Comparing the results from the SpiNNaker system to a conventional double-precision simulation, we ﬁnd that the network behavior is comparable, and the ﬁnal weights differ by less than 3\% between the two simulations, while the SpiNNaker simulation runs much faster, since it runs in real time, independent of network size.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Diehl, Peter U. and Cook, Matthew},
	month = jul,
	year = {2014},
	keywords = {⛔ No INSPIRE recid found},
	pages = {4288--4295},
}

@article{neckar_braindrop_2019,
	title = {Braindrop: {A} mixed-signal neuromorphic architecture with a dynamical systems-based programming model},
	volume = {107},
	doi = {10.1109/JPROC.2018.2881432},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Neckar, Alexander and Fok, Sam and Benjamin, Ben V. and Stewart, Terrence C. and Oza, Nick N. and Voelker, Aaron R. and Eliasmith, Chris and Manohar, Rajit and Boahen, Kwabena},
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {144--164},
}

@inproceedings{wang_fpga_2014,
	title = {An {FPGA} design framework for large-scale spiking neural networks},
	doi = {10.1109/ISCAS.2014.6865169},
	abstract = {We present an FPGA design framework for large-scale spiking neural networks, particularly the ones with a high-density of connections or all-to-all connections. The proposed FPGA design framework is based on a reconfigurable neural layer, which is implemented using a time-multiplexing approach to achieve up to 200,000 virtual neurons with one physical neuron using only a fraction of the hardware resources in commercial-off-the-shelf FPGAs (even entry level ones). Rather than using a mathematical computational model, the physical neuron was efficiently implemented with a conductance-based model, of which the parameters were randomised between neurons to emulate the variance in biological neurons. Besides these building blocks, the proposed time-multiplexed reconfigurable neural layer has an address buffer, which will generate a fixed random weight for each connection on the fly for incoming spikes. This structure effectively reduces the usage of memory. After presenting the architecture of the proposed neural layer, we present a network with 23 proposed neural layers, each containing 64k neurons, yielding 1.5 M neurons and 92 G synapses with a total spike throughput of 1.2T spikes/s, while running in real-time on a Virtex 6 FPGA.},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Wang, Runchun and Hamilton, Tara Julia and Tapson, Jonathan and van Schaik, André},
	month = jun,
	year = {2014},
	note = {ISSN: 2158-1525},
	keywords = {Arrays, Biological neural networks, Biological system modeling, Digital signal processing, Field programmable gate arrays, Generators, Neurons, ⛔ No INSPIRE recid found},
	pages = {457--460},
}

@incollection{cheng_fpaa_2009,
	address = {Berlin, Heidelberg},
	title = {{FPAA} {Based} on {Integration} of {CMOS} and {Nanojunction} {Devices} for {Neuromorphic} {Applications}},
	volume = {3},
	isbn = {978-3-642-02426-9 978-3-642-02427-6},
	url = {http://link.springer.com/10.1007/978-3-642-02427-6_9},
	abstract = {In this paper, a novel field programmable analog arrays (FPAA) architecture, namely, NueroFPAA, is introduced to utilize nanodevices to build a programmable neuromorphic system. By using nanodevices as programmable components, the proposed FPAA can achieve high-density and low-power operations for neuromorphic applications. The routing and function blocks of the FPAA are specifically designed so that this proposed architecture can support large-scale neuromorphic design as well as various analog circuitries.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {Nano-{Net}},
	publisher = {Springer Berlin Heidelberg},
	author = {Liu, Ming and Yu, Hua and Wang, Wei},
	editor = {Cheng, Maggie},
	year = {2009},
	keywords = {⛔ No INSPIRE recid found},
	pages = {44--48},
}

@article{diesmann_nest_2003,
	title = {{NEST}: {An} {Environment} for {Neural} {Systems} {Simulations}},
	abstract = {NEST is a framework for simulating large, structured neuronal systems. It is designed to investigate the functional behavior of neuronal systems in the context of their anatomical, morphological, and electrophysiological properties. NEST aims at large networks, while maintaining an appropriate degree of biological detail. This is achieved by combining a broad range of abstraction levels in a single network simulation. Great biological detail is then maintained only at the points of interest, while the rest of the system can be modeled by more abstract components. Here, we describe the conception of NEST and illustrate its key features. We demonstrate that software design and organizational aspects were of equal importance for the success of the project.},
	language = {en},
	journal = {GWDG-Bericht Nr. 58 Theo Plesser, Volker Macho (Hrsg.)},
	author = {Diesmann, Markus and Gewaltig, Marc-Oliver},
	year = {2003},
	keywords = {⛔ No INSPIRE recid found},
	pages = {29},
}

@article{camon_timing_2019,
	title = {The {Timing} of {Sensory}-{Guided} {Behavioral} {Response} is {Represented} in the {Mouse} {Primary} {Somatosensory} {Cortex}},
	volume = {29},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/bhy169},
	doi = {10.1093/cercor/bhy169},
	abstract = {Whisker-guided decision making in mice is thought to critically depend on
information processing occurring in the primary somatosensory cortex. However,
it is not clear if neuronal activity in this “early”
sensory region contains information about the timing and speed of motor
response. To address this question we designed a new task in which freely moving
mice learned to associate a whisker stimulus to reward delivery. The task was
tailored in such a way that a wide range of delays between whisker stimulation
and reward collection were observed due to differences of motivation and
perception. After training, mice were anesthetized and neuronal responses evoked
by stimulating trained and untrained whiskers were recorded across several
cortical columns of barrel cortex. We found a strong correlation between the
delay of the mouse behavioral response and the timing of multiunit activity
evoked by the trained whisker, outside its principal cortical column, in layers
4 and 5A but not in layer 2/3. Circuit mapping ex vivo revealed this effect was
associated with a weakening of layer 4 to layer 2/3 projection. We conclude that
the processes controlling the propagation of key sensory inputs to naive
cortical columns and the timing of sensory-guided action are linked.},
	number = {7},
	urldate = {2022-11-16},
	journal = {Cerebral Cortex},
	author = {Camon, Jérémy and Hugues, Sandrine and Erlandson, Melissa A and Robbe, David and Lagoun, Sabria and Marouane, Emna and Bureau, Ingrid},
	month = jul,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {3034--3047},
}

@article{boerlin_spike-based_2011,
	title = {Spike-{Based} {Population} {Coding} and {Working} {Memory}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	doi = {10.1371/journal.pcbi.1001080},
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	language = {en},
	number = {2},
	urldate = {2022-11-14},
	journal = {PLOS Computational Biology},
	author = {Boerlin, Martin and Denève, Sophie},
	month = feb,
	year = {2011},
	keywords = {Action potentials, Memory, Neural networks, Neuronal tuning, Neurons, Sensory cues, Sensory perception, Working memory, ⛔ No INSPIRE recid found},
	pages = {e1001080},
}

@article{young_functioning_1938,
	title = {The {Functioning} of the {Giant} {Nerve} {Fibres} of the {Squid}},
	volume = {15},
	issn = {0022-0949},
	url = {https://doi.org/10.1242/jeb.15.2.170},
	doi = {10.1242/jeb.15.2.170},
	abstract = {1. Stimulation of single giant nerve fibres in the stellar nerves of the squid (Loligo pealii) shows them to be motor axons which produce contraction of the circular fibres of the mantle muscles.2. When a stellar nerve is stimulated with condenser discharges a maximal response is obtained at threshold voltage. No increase of response is obtained by further increase in the strength of stimulation except for an occasional slight increase at about ten times threshold voltage probably due to repetitive firing. It therefore appears that the stimulus produces a single impulse in the giant fibre, and that this is capable of exciting contraction in all the muscle fibres which it reaches. This confirms the conclusion reached on histological grounds that in spite of their syncytial nature each of the giant nerve fibres is a single functional unit.3. Since there are about ten giant fibres on each side the mantle is divided into 20 neuromotor units, each nerve fibre innervating an enormous number of muscle fibres. The existence of these units can also very readily be demonstrated by the fact that threshold electrical stimulation at any point within the territory innervated by each single giant fibre sets up a contraction of the muscle fibres of all parts of the territory with which the stimulated area is in connexion through the nerve.4. Stimulation of the smaller fibres in a stellar nerve after destruction of the giant fibre also causes contraction of the circular muscles of the mantle. The amount of this contraction increases progressively with increased voltage, presumably on account of the stimulation of more and more nerve fibres. The maximum tension developed in this way is always very much less than that produced by stimulation of the giant fibres.5. The mantle is therefore provided with a double mechanism of expiratory contraction, maximal contractions being produced by single impulses in the giant fibres and graded contractions by those in the smaller fibres of the nerve. Presumably the former contractions are those involved in rapid movement, the latter in respiration.6. There are also radial muscles, running through the thickness of the mantle, whose contractions effect the inspiration by making the cavity larger.},
	number = {2},
	urldate = {2022-11-13},
	journal = {Journal of Experimental Biology},
	author = {Young, J. Z.},
	month = apr,
	year = {1938},
	keywords = {⛔ No INSPIRE recid found},
	pages = {170--185},
}

@article{linden_movement_2022,
	title = {Movement is governed by rotational neural dynamics in spinal motor networks},
	volume = {610},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05293-w},
	doi = {10.1038/s41586-022-05293-w},
	abstract = {Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. As flexor and extensor muscle activities alternate during rhythmic movements such as walking, it is often assumed that the responsible neural circuitry is similarly exhibiting alternating activity1. Here we present ensemble recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional ‘rotation’ in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behaviour. The radius of rotation correlates with the intended muscle force, and a perturbation of the low-dimensional trajectory can modify the motor behaviour. As existing models of spinal motor control do not offer an adequate explanation of rotation1,2, we propose a theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control and multifunctionalism, are readily explained.},
	language = {en},
	number = {7932},
	urldate = {2022-11-16},
	journal = {Nature},
	author = {Lindén, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	month = oct,
	year = {2022},
	keywords = {Central pattern generators, Network models, ⛔ No INSPIRE recid found},
	pages = {526--531},
}

@article{linden_movement_2021,
	title = {Movement is governed by rotational population dynamics in spinal motor networks},
	url = {https://doi.org/gqg6rb},
	abstract = {{\textless}jats:title{\textgreater}ABSTRACT{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. Since flexor- and extensor-muscles alternate during rhythmic movements like walking, it is often assumed that the responsible neural circuitry is similarly displaying alternating activity. Here, we present ensemble-recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional “rotation” in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behavior. The radius of rotation correlates with the intended muscle force and a perturbation of the low-dimensional trajectory can modify the motor behavior. Since existing models of spinal motor control offer an inadequate explanation of rotation, we propose a new theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control, and multi-functionalism, are readily explained.{\textless}/jats:p{\textgreater}},
	author = {Lindén, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	month = sep,
	year = {2021},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {08936080},
	shorttitle = {Networks of spiking neurons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. © 1997 Elsevier Science Ltd. All rights reserved.},
	language = {en-US},
	number = {9},
	urldate = {2020-07-06},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	year = {1997},
	pages = {1659--1671},
}

@article{renner_sparse_2022,
	title = {Sparse {Vector} {Binding} on {Spiking} {Neuromorphic} {Hardware} {Using} {Synaptic} {Delays}},
	copyright = {info:eu-repo/semantics/closedAccess},
	url = {https://www.zora.uzh.ch/id/eprint/219676/},
	doi = {10.1145/3546790.3546820},
	abstract = {Vector Symbolic Architectures (VSA) were first proposed as connectionist models for symbolic reasoning, leveraging parallel and in-memory computing in brains and neuromorphic hardware that enable low-power, low-latency applications.
Symbols are defined in VSAs as points/vectors in a high-dimensional neural state-space.
For spiking neuromorphic hardware (and brains), particularly sparse representations are of interest, as they minimize the number of costly spikes. Furthermore, sparse representations can be efficiently stored in simple Hebbian auto-associative memories, which provide error correction in VSAs. 
However, the binding of spatially sparse representations is computationally expensive because it is not local to corresponding pairs of neurons as in VSAs with dense vectors.
Here, we present the first implementation of a sparse VSA on spiking neuromorphic hardware, specifically Intel's neuromorphic research chip Loihi.
To reduce the cost of binding, a delay line and coincidence detection are used, trading off space with time.
We show as proof of principle that our network on Loihi can perform the binding operation of a classical analogical reasoning task and discuss the cost of different sparse binding operations.
The proposed binding mechanism can be used as a building block for VSA-based architectures on neuromorphic hardware.},
	language = {eng},
	urldate = {2022-11-10},
	journal = {Proceedings of the International Conference on Neuromorphic Systems},
	author = {Renner, Alpha and Sandamirskaya, Yulia and Sommer, Friedrich T. and Frady, E. Paxon},
	month = jul,
	year = {2022},
	note = {Conference Name: ICONS 2022: International Conference on Neuromorphic Systems 2022
Meeting Name: ICONS 2022: International Conference on Neuromorphic Systems 2022
Place: Knoxville, TN, USA
Publisher: ACM Digital library},
	keywords = {Binding, Coincidence detection, Hyperdimensional Computing, Neuromorphic Hardware, Sparse distributed code, Spiking Neural Networks, Vector Symbolic Architecture (VSA)},
}

@article{schuman_survey_2017,
	title = {A {Survey} of {Neuromorphic} {Computing} and {Neural} {Networks} in {Hardware}},
	url = {http://arxiv.org/abs/1705.06963},
	abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
	urldate = {2021-03-25},
	journal = {arXiv:1705.06963 [cs]},
	author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06963},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{foss_multistability_2000,
	title = {Multistability in {Recurrent} {Neural} {Loops} {Arising} {From} {Delay}},
	volume = {84},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.2000.84.2.975},
	doi = {10/gmvbsh},
	abstract = {The dynamics of a recurrent inhibitory neural loop composed of a periodically spiking Aplysia motoneuron reciprocally connected to a computer are investigated as a function of the time delay, τ, for propagation around the loop. It is shown that for certain choices of τ, multiple qualitatively different neural spike trains co-exist. A mathematical model is constructed for the dynamics of this pulsed-coupled recurrent loop in which all parameters are readily measured experimentally: the phase resetting curve of the neuron for a given simulated postsynaptic current and τ. For choices of the parameters for which multiple spiking patterns co-exist in the experimental paradigm, the model exhibits multistability. Numerical simulations suggest that qualitatively similar results will occur if the motoneuron is replaced by several other types of neurons and that once τ becomes sufficiently long, multistability will be the dominant form of dynamical behavior. These observations suggest that great care must be taken in determining the etiology of qualitative changes in neural spiking patterns, particularly when propagation times around polysynaptic loops are long.},
	language = {en},
	number = {2},
	urldate = {2021-09-16},
	journal = {Journal of Neurophysiology},
	author = {Foss, Jennifer and Milton, John},
	month = aug,
	year = {2000},
	pages = {975--985},
}

@article{bernert_attention-based_2018,
	title = {An {Attention}-{Based} {Spiking} {Neural} {Network} for {Unsupervised} {Spike}-{Sorting}},
	volume = {29},
	issn = {0129-0657},
	url = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	doi = {10.1142/s0129065718500594},
	abstract = {Bio-inspired computing using artificial spiking neural networks promises performances outperforming currently available computational approaches. Yet, the number of applications of such networks remains limited due to the absence of generic training procedures for complex pattern recognition, which require the design of dedicated architectures for each situation. We developed a spike-timing-dependent plasticity (STDP) spiking neural network (SSN) to address spike-sorting, a central pattern recognition problem in neuroscience. This network is designed to process an extracellular neural signal in an online and unsupervised fashion. The signal stream is continuously fed to the network and processed through several layers to output spike trains matching the truth after a short learning period requiring only few data. The network features an attention mechanism to handle the scarcity of action potential occurrences in the signal, and a threshold adaptation mechanism to handle patterns with different sizes. This method outperforms two existing spike-sorting algorithms at low signal-to-noise ratio (SNR) and can be adapted to process several channels simultaneously in the case of tetrode recordings. Such attention-based STDP network applied to spike-sorting opens perspectives to embed neuromorphic processing of neural data in future brain implants.},
	number = {08},
	urldate = {2021-01-26},
	journal = {International Journal of Neural Systems},
	author = {Bernert, Marie and Yvert, Blaise},
	month = dec,
	year = {2018},
	pages = {1850059},
}

@article{bernert_fully_2017,
	title = {Fully unsupervised online spike sorting based on an artificial spiking neural network},
	url = {https://www.biorxiv.org/content/10.1101/236224v1},
	doi = {10.1101/236224},
	abstract = {Spike sorting is a crucial step of neural data processing widely used in neuroscience and neuroprosthetics. However, current methods remain not fully automatic and require heavy computations making them not embeddable in implantable devices. To overcome these limitations, we propose a novel method based on an artificial spiking neural network designed to process neural data online and completely automatically. An input layer continuously encodes the data stream into artificial spike trains, which are then processed by two further layers to output artificial trains of spikes reproducing the real spiking activity present in the input signal. The proposed method can be adapted to process several channels simultaneously in the case of tetrode recordings. It outperforms two existing algorithms at low SNR and has the advantage to be compatible with neuromorphic computing and the perspective of being embedded in very low-power analog systems for future implantable devices serving neurorehabilitation applications.},
	language = {en},
	urldate = {2022-04-08},
	author = {Bernert, Marie and Yvert, Blaise},
	month = dec,
	year = {2017},
	pages = {236224},
}

@article{carandini_normalization_2012,
	title = {Normalization as a canonical neural computation},
	volume = {13},
	doi = {10.1038/nrn3136},
	number = {1},
	journal = {Nature Reviews Neuroscience},
	author = {Carandini, Matteo and Heeger, David J},
	year = {2012},
	keywords = {Adaptation, Afferent Pathways, Anim, Physiological, contrast\_response, divisive\_normalization, normalization},
	pages = {51--62},
}

@article{pasturel_humans_2020,
	title = {Humans adapt their anticipatory eye movements to the volatility of visual motion properties},
	copyright = {All rights reserved},
	url = {https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020},
	doi = {10.1371/journal.pcbi.1007438},
	abstract = {Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate "how much they are confident that the target will move to the right or left in the next trial" and to adjust the cursor's position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile).},
	journal = {PLoS Computational Biology},
	author = {Pasturel, Chloé and Montagnini, Anna and Perrinet, Laurent U},
	month = jan,
	year = {2020},
	keywords = {motion anticipation},
}

@article{izhikevich_polychronous_2009,
	title = {Polychronous {Wavefront} {Computations}},
	volume = {19},
	issn = {0218-1274, 1793-6551},
	url = {https://www.izhikevich.org/publications/polychronous_wavefront_computations.htm},
	doi = {10/db98d7},
	abstract = {There is great interest in methods for computing that do not involve digital machines. Many computational paradigms were inspired by brain research, such as Boolean neuronal logic [McCulloch \& Pitts, 1943], the perceptron [Rosenblatt, 1958], attractor neural networks [Hopfield, 1982] and cellular neural nets [Chua \& Yang, 1988]. All these paradigms abstract biological circuits to artificial neural networks, i.e. interconnected units (neurons) that perform computations based on the connections between the units (synapses). Here we present a novel computational framework based on polychronous wavefront dynamics. It is entirely different from an artificial neural network paradigm, rather it is based on temporal and spatial patterns of activity in pulse-propagating media and their interaction with transponders, which create pulses in response to receiving appropriate inputs, e.g. two coincident input pulses. A pulse propagates as a circular wave from its source to other transponders. Computations result from interactions between transponders, and they are encoded by the exact physical locations of transponders and by precise timings of pulses. We illustrate temporal pattern recognition, reverberating memory, temporal signal analysis and basic logical operations using polychronous wavefront computations. This work reveals novel principles for designing nanoscale computational devices.},
	language = {en},
	number = {05},
	urldate = {2019-09-10},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Izhikevich, Eugene M. and Hoppensteadt, Frank C.},
	month = may,
	year = {2009},
	keywords = {polychronization},
	pages = {1733--1739},
}

@article{huning_synaptic_1998,
	title = {Synaptic {Delay} {Learning} in {Pulse}-{Coupled} {Neurons}},
	volume = {10},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976698300017665},
	doi = {10/cthfps},
	abstract = {We present rules for the unsupervised learning of coincidence between excitatory postsynaptic potentials (EPSPs) by the adjustment of post-synaptic delays between the transmitter binding and the opening of ion channels. Starting from a gradient descent scheme, we develop a robust and more biological threshold rule by which EPSPs from different synapses can be gradually pulled into coincidence. The synaptic delay changes are determined from the summed potential—at the site where the coincidence is to be established—and from postulated synaptic learning functions that accompany the individual EPSPs. According to our scheme, templates for the detection of spatiotemporal patterns of synaptic activation can be learned, which is demonstrated by computer simulation. Finally, we discuss possible relations to biological mechanisms.},
	number = {3},
	urldate = {2021-09-16},
	journal = {Neural Computation},
	author = {Hüning, Harald and Glünder, Helmut and Palm, Günther},
	month = apr,
	year = {1998},
	pages = {555--565},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {1469-7793},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1968.sp008455},
	doi = {10.1113/jphysiol.1968.sp008455},
	language = {english},
	number = {1},
	journal = {The Journal of Physiology},
	author = {Hubel, David H and Wiesel, Torsten N},
	year = {1968},
	keywords = {area-v1, bicv-motion, bicv-sparse},
	pages = {215--243},
}

@article{haessig_event-based_2020,
	title = {Event-{Based} {Computation} for {Touch} {Localization} {Based} on {Precise} {Spike} {Timing}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00420},
	doi = {10/ghvnxj},
	abstract = {Precise spike timing and temporal coding are used extensively within the nervous system of insects and in the sensory periphery of higher order animals. However, conventional Artificial Neural Networks (ANNs) and machine learning algorithms cannot take advantage of this coding strategy, due to their rate-based representation of signals. Even in the case of artificial Spiking Neural Networks (SNNs), identifying applications where temporal coding outperforms the rate coding strategies of ANNs is still an open challenge. Neuromorphic sensory-processing systems provide an ideal context for exploring the potential advantages of temporal coding, as they are able to efficiently extract the information required to cluster or classify spatio-temporal activity patterns from relative spike timing. Here we propose a neuromorphic model inspired by the sand scorpion to explore the benefits of temporal coding, and validate it in an event-based sensory-processing task. The task consists in localizing a target using only the relative spike timing of eight spatially-separated vibration sensors. We propose two different approaches in which the SNNs learns to cluster spatio-temporal patterns in an unsupervised manner and we demonstrate how the task can be solved both analytically and through numerical simulation of multiple SNN models. We argue that the models presented are optimal for spatio-temporal pattern classification using precise spike timing in a task that could be used as a standard benchmark for evaluating event-based sensory processing models based on temporal coding.},
	urldate = {2021-10-20},
	journal = {Frontiers in Neuroscience},
	author = {Haessig, Germain and Milde, Moritz B. and Aceituno, Pau Vilimelis and Oubari, Omar and Knight, James C. and van Schaik, André and Benosman, Ryad B. and Indiveri, Giacomo},
	year = {2020},
	pages = {420},
}

@article{dard_rapid_2022,
	title = {The rapid developmental rise of somatic inhibition disengages hippocampal dynamics from self-motion},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.78116},
	doi = {10.7554/eLife.78116},
	abstract = {Early electrophysiological brain oscillations recorded in preterm babies and newborn rodents are initially mostly driven by bottom-up sensorimotor activity and only later can detach from external inputs. This is a hallmark of most developing brain areas, including the hippocampus, which, in the adult brain, functions in integrating external inputs onto internal dynamics. Such developmental disengagement from external inputs is likely a fundamental step for the proper development of cognitive internal models. Despite its importance, the developmental timeline and circuit basis for this disengagement remain unknown. To address this issue, we have investigated the daily evolution of CA1 dynamics and underlying circuits during the first two postnatal weeks of mouse development using two-photon calcium imaging in non-anesthetized pups. We show that the first postnatal week ends with an abrupt shift in the representation of self-motion in CA1. Indeed, most CA1 pyramidal cells switch from activated to inhibited by self-generated movements at the end of the first postnatal week, whereas the majority of GABAergic neurons remain positively modulated throughout this period. This rapid switch occurs within 2 days and follows the rapid anatomical and functional surge of local somatic GABAergic innervation. The observed change in dynamics is consistent with a two-population model undergoing a strengthening of inhibition. We propose that this abrupt developmental transition inaugurates the emergence of internal hippocampal dynamics.},
	urldate = {2022-10-05},
	journal = {eLife},
	author = {Dard, Robin F and Leprince, Erwan and Denis, Julien and Rao Balappa, Shrisha and Suchkov, Dmitrii and Boyce, Richard and Lopez, Catherine and Giorgi-Kurz, Marie and Szwagier, Tom and Dumont, Théo and Rouault, Hervé and Minlebaev, Marat and Baude, Agnès and Cossart, Rosa and Picardo, Michel A},
	editor = {Peyrache, Adrien and Colgin, Laura L and Butt, Simon JB},
	month = jul,
	year = {2022},
	pages = {e78116},
}

@article{grossberger_unsupervised_2018,
	title = {Unsupervised clustering of temporal patterns in high-dimensional neuronal ensembles using a novel dissimilarity measure},
	volume = {14},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006283},
	doi = {10/gdvbsx},
	abstract = {Temporally ordered multi-neuron patterns likely encode information in the brain. We introduce an unsupervised method, SPOTDisClust (Spike Pattern Optimal Transport Dissimilarity Clustering), for their detection from high-dimensional neural ensembles. SPOTDisClust measures similarity between two ensemble spike patterns by determining the minimum transport cost of transforming their corresponding normalized cross-correlation matrices into each other (SPOTDis). Then, it performs density-based clustering based on the resulting inter-pattern dissimilarity matrix. SPOTDisClust does not require binning and can detect complex patterns (beyond sequential activation) even when high levels of out-of-pattern “noise” spiking are present. Our method handles efficiently the additional information from increasingly large neuronal ensembles and can detect a number of patterns that far exceeds the number of recorded neurons. In an application to neural ensemble data from macaque monkey V1 cortex, SPOTDisClust can identify different moving stimulus directions on the sole basis of temporal spiking patterns.},
	language = {en},
	number = {7},
	urldate = {2021-11-30},
	journal = {PLOS Computational Biology},
	author = {Grossberger, Lukas and Battaglia, Francesco P. and Vinck, Martin},
	year = {2018},
	pages = {e1006283},
}

@article{ikegaya_synfire_2004,
	title = {Synfire {Chains} and {Cortical} {Songs}: {Temporal} {Modules} of {Cortical} {Activity}},
	volume = {304},
	shorttitle = {Synfire {Chains} and {Cortical} {Songs}},
	url = {http://www.science.org/doi/10.1126/science.1093173},
	doi = {10/djckcn},
	number = {5670},
	urldate = {2021-11-29},
	journal = {Science},
	author = {Ikegaya, Yuji and Aaron, Gloster and Cossart, Rosa and Aronov, Dmitriy and Lampl, Ilan and Ferster, David and Yuste, Rafael},
	month = apr,
	year = {2004},
	keywords = {polychronization},
	pages = {559--564},
}

@inproceedings{thanasoulis_delay-based_2021,
	title = {Delay-{Based} {Neural} {Computation}: {Pulse} {Routing} {Architecture} and {Benchmark} {Application} in {FPGA}},
	shorttitle = {Delay-{Based} {Neural} {Computation}},
	doi = {10.1109/ICECS53924.2021.9665468},
	abstract = {Neuromorphic engineering implements large-scale systems that provide a high integration density of power efficient synapse-and-neuron blocks. This represents a promising alternative to the numerical simulations for studying the dynamics of spiking neural networks. A key aspect of these systems is the implementation of communication and routing of pulse events produced by the neural network. In this paper we present a measurement methodology and results of a neural benchmark that tests the configurable delays, multicasting and connectivity implemented by a routing logic for neuromorphic hardware. Pulses are handled according to their timestamp and transmitted with configurable delays and routing to different post-synaptic neurons. The results show the suitability of communication and routing logic for delay-based neural computation and point out effects of time discretization in resolution of pulse timestamps.},
	booktitle = {2021 28th {IEEE} {International} {Conference} on {Electronics}, {Circuits}, and {Systems} ({ICECS})},
	author = {Thanasoulis, Vasilis and Vogginger, Bernhard and Partzsch, Johannes and Mayr, Christian},
	month = nov,
	year = {2021},
	keywords = {Benchmark testing, Hardware, Multicast communication, Neuromorphic engineering, Neurons, Numerical simulation, Routing},
	pages = {1--5},
}

@article{simons_oligodendrocytes_2016,
	title = {Oligodendrocytes: {Myelination} and {Axonal} {Support}},
	volume = {8},
	issn = {1943-0264},
	shorttitle = {Oligodendrocytes},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691794/},
	doi = {10.1101/cshperspect.a020479},
	abstract = {Myelinated nerve fibers have evolved to enable fast and efficient transduction of electrical signals in the nervous system. To act as an electric insulator, the myelin sheath is formed as a multilamellar membrane structure by the spiral wrapping and subsequent compaction of the oligodendroglial plasma membrane around central nervous system (CNS) axons. Current evidence indicates that the myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes are metabolically active and functionally connected to the subjacent axon via cytoplasmic-rich myelinic channels for movement of macromolecules to and from the internodal periaxonal space under the myelin sheath. This review summarizes our current understanding of how myelin is generated and also the role of oligodendrocytes in supporting the long-term integrity of myelinated axons., The myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes in the sheath are connected to the subjacent axon via cytoplasmic-rich myelinic channels, and they actively support the integrity of the neuron.},
	number = {1},
	urldate = {2022-11-10},
	journal = {Cold Spring Harbor Perspectives in Biology},
	author = {Simons, Mikael and Nave, Klaus-Armin},
	month = jan,
	year = {2016},
	pmid = {26101081},
	pmcid = {PMC4691794},
	pages = {a020479},
}

@inproceedings{pfeil_neuromorphic_2013,
	title = {Neuromorphic learning towards nano second precision},
	doi = {10.1109/IJCNN.2013.6706828},
	abstract = {Temporal coding is one approach to representing information in spiking neural networks. An example of its application is the location of sounds by barn owls that requires especially precise temporal coding. Dependent upon the azimuthal angle, the arrival times of sound signals are shifted between both ears. In order to determine these interaural time differences, the phase difference of the signals is measured. We implemented this biologically inspired network on a neuromorphic hardware system and demonstrate spike-timing dependent plasticity on an analog, highly accelerated hardware substrate. Our neuromorphic implementation enables the resolution of time differences of less than 50 ns. On-chip Hebbian learning mechanisms select inputs from a pool of neurons which code for the same sound frequency. Hence, noise caused by different synaptic delays across these inputs is reduced. Furthermore, learning compensates for variations on neuronal and synaptic parameters caused by device mismatch intrinsic to the neuromorphic substrate.},
	booktitle = {The 2013 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Pfeil, Thomas and Scherzer, Anne-Christine and Schemmel, Johannes and Meier, Karlheinz},
	month = aug,
	year = {2013},
	note = {ISSN: 2161-4407},
	keywords = {Delays, Emulation, Hardware, Neuromorphics, Neurons, System-on-chip, Vectors},
	pages = {1--5},
}

@article{bartolozzi_synaptic_2007,
	title = {Synaptic {Dynamics} in {Analog} {VLSI}},
	volume = {19},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/19/10/2581-2603/7219},
	doi = {10.1162/neco.2007.19.10.2581},
	abstract = {Synapses are crucial elements for computation and information transfer in both real and artificial neural systems. Recent experimental findings and theoretical models of pulse-based neural networks suggest that synaptic dynamics can play a crucial role for learning neural codes and encoding spatio-temporal spike patterns. Within the context of hardware implementations of pulse based neural networks, several analog VLSI circuits modeling synaptic functionality have been proposed. We present an overview of previously proposed circuits and describe a novel analog VLSI synaptic circuit suitable for integration in large VLSI spike-based neural systems. The circuit proposed is based on a computational model that fits the real post-synaptic currents with exponentials. We present experimental data showing how the circuit exhibits realistic dynamics and show how it can be connected to additional modules for implementing a wide range of synaptic properties.},
	language = {en},
	number = {10},
	urldate = {2022-11-10},
	journal = {Neural Computation},
	author = {Bartolozzi, Chiara and Indiveri, Giacomo},
	month = oct,
	year = {2007},
	pages = {2581--2603},
}

@article{chan_aer_2007,
	title = {{AER} {EAR}: {A} {Matched} {Silicon} {Cochlea} {Pair} {With} {Address} {Event} {Representation} {Interface}},
	volume = {54},
	issn = {1558-0806},
	shorttitle = {{AER} {EAR}},
	doi = {10.1109/TCSI.2006.887979},
	abstract = {In this paper, we present an analog integrated circuit containing a matched pair of silicon cochleae and an address event interface. Each section of the cochlea, modeled by a second-order low-pass filter, is followed by a simplified inner hair cell circuit and a spiking neuron circuit. When the neuron spikes, an address event is generated on the asynchronous data bus. We present the results of the chip characterization and the results of an interaural time difference based sound localization experiment using the address event representation (AER) EAR. The chip was fabricated in a 3-metal 2-poly 0.5-mum CMOS process},
	number = {1},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Chan, Vincent and Liu, Shih-Chii and van Schaik, Andr},
	month = jan,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {Analog integrated circuits, Biological system modeling, Circuits, Ear, Low pass filters, Neuromorphics, Neurons, Protocols, Silicon, Timing, Transmitters, neuromorphic engineering, silicon cochlea, sound localization},
	pages = {48--59},
}

@inproceedings{tschechne_bio-inspired_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Bio-{Inspired} {Optic} {Flow} from {Event}-{Based} {Neuromorphic} {Sensor} {Input}},
	isbn = {978-3-319-11656-3},
	doi = {10.1007/978-3-319-11656-3_16},
	abstract = {Computational models of visual processing often use frame-based image acquisition techniques to process a temporally changing stimulus. This approach is unlike biological mechanisms that are spike-based and independent of individual frames. The neuromorphic Dynamic Vision Sensor (DVS) [Lichtsteiner et al., 2008] provides a stream of independent visual events that indicate local illumination changes, resembling spiking neurons at a retinal level. We introduce a new approach for the modelling of cortical mechanisms of motion detection along the dorsal pathway using this type of representation. Our model combines filters with spatio-temporal tunings also found in visual cortex to yield spatio-temporal and direction specificity. We probe our model with recordings of test stimuli, articulated motion and ego-motion. We show how our approach robustly estimates optic flow and also demonstrate how this output can be used for classification purposes.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Tschechne, Stephan and Sailer, Roman and Neumann, Heiko},
	editor = {El Gayar, Neamat and Schwenker, Friedhelm and Suen, Cheng},
	year = {2014},
	keywords = {Classification, Event-Vision, Neural Model, Optic Flow},
	pages = {171--182},
}

@article{kaiser_emulating_2022,
	series = {Dendritic contributions to biological and artificial computations},
	title = {Emulating {Dendritic} {Computing} {Paradigms} on {Analog} {Neuromorphic} {Hardware}},
	volume = {489},
	issn = {0306-4522},
	url = {https://www.sciencedirect.com/science/article/pii/S0306452221004218},
	doi = {10.1016/j.neuroscience.2021.08.013},
	abstract = {BrainScaleS-2 is an accelerated and highly configurable neuromorphic system with physical models of neurons and synapses. Beyond networks of spiking point neurons, it allows for the implementation of user-defined neuron morphologies. Both passive propagation of electric signals between compartments as well as dendritic spikes and plateau potentials can be emulated. In this paper, three multi-compartment neuron morphologies are chosen to demonstrate passive propagation of postsynaptic potentials, spatio-temporal coincidence detection of synaptic inputs in a dendritic branch, and the replication of the BAC burst firing mechanism found in layer 5 pyramidal neurons of the neocortex.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Neuroscience},
	author = {Kaiser, Jakob and Billaudelle, Sebastian and Müller, Eric and Tetzlaff, Christian and Schemmel, Johannes and Schmitt, Sebastian},
	month = may,
	year = {2022},
	keywords = {AdEx neuron model, accelerated technology, mixed-signal neuromorphic, multi-compartmental models, physical model},
	pages = {290--300},
}

@article{rajendran_low-power_2019,
	title = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}: {A} {Review} of {Architectural} and {System}-{Level} {Design} {Approaches}},
	volume = {36},
	issn = {1558-0792},
	shorttitle = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}},
	doi = {10.1109/MSP.2019.2933719},
	abstract = {Machine learning has emerged as the dominant tool for implementing complex cognitive tasks that require supervised, unsupervised, and reinforcement learning. While the resulting machines have demonstrated in some cases even superhuman performance, their energy consumption has often proved to be prohibitive in the absence of costly supercomputers. Most state-of-the-art machine-learning solutions are based on memoryless models of neurons. This is unlike the neurons in the human brain that encode and process information using temporal information in spike events. The different computing principles underlying biological neurons and how they combine together to efficiently process information is believed to be a key factor behind their superior efficiency compared to current machine-learning systems.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Rajendran, Bipin and Sebastian, Abu and Schmuker, Michael and Srinivasa, Narayan and Eleftheriou, Evangelos},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Encoding, Hardware, Mathematical model, Neuromorphics, Neurons, Signal processing algorithms, Synapses},
	pages = {97--110},
}

@inproceedings{hill_spike-timing_2017,
	title = {A {Spike}-{Timing} {Neuromorphic} {Architecture}},
	doi = {10.1109/ICRC.2017.8123631},
	abstract = {Unlike general purpose computer architectures that are comprised of complex processor cores and sequential computation, the brain is innately parallel and contains highly complex connections between computational units (neurons). Key to the architecture of the brain is a functionality enabled by the combined effect of spiking communication and sparse connectivity with unique variable efficacies and temporal latencies. Utilizing these neuroscience principles, we have developed the Spiking Temporal Processing Unit (STPU) architecture which is well-suited for areas such as pattern recognition and natural language processing. In this paper, we formally describe the STPU, implement the STPU on a field programmable gate array, and show measured performance data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Rebooting} {Computing} ({ICRC})},
	author = {Hill, Aaron J. and Donaldson, Jonathon W. and Rothganger, Fredrick H. and Vineyard, Craig M. and Follett, David R. and Follett, Pamela L. and Smith, Michael R. and Verzi, Stephen J. and Severa, William and Wang, Felix and Aimone, James B. and Naegle, John H. and James, Conrad D.},
	month = nov,
	year = {2017},
	keywords = {Biological system modeling, Computer architecture, Delays, Neuromorphics, Neurons},
	pages = {1--8},
}

@article{schuman_opportunities_2022,
	title = {Opportunities for neuromorphic computing algorithms and applications},
	volume = {2},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {http://www.nature.com/articles/s43588-021-00184-y},
	doi = {10.1038/s43588-021-00184-y},
	abstract = {Neuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic computing has focused on hardware development. Here, we review recent results in neuromorphic computing algorithms and applications. We highlight characteristics of neuromorphic computing technologies that make them attractive for the future of computing and we discuss opportunities for future development of algorithms and applications on these systems.},
	language = {en},
	number = {1},
	urldate = {2022-11-10},
	journal = {Nature Computational Science},
	author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science},
	pages = {10--19},
}

@article{nieters_neuromorphic_2017,
	title = {Neuromorphic computation in multi-delay coupled models},
	volume = {61},
	issn = {0018-8646},
	doi = {10.1147/JRD.2017.2664698},
	abstract = {Neuromorphic computing provides a promising platform for processing high-dimensional noisy signals on dedicated hardware. Using design elements inspired by neurobiological findings and advances in machine learning methodology, delay-coupled systems have recently been developed in the field of neuromorphic computing. Delayed feedback connections enable such systems to generate a complex representation of injected input in the internal state of single nodes, which in our context refer to hardware components with nonlinear behavior and without any memory. In contrast to classical combinatorial circuits or feed-forward networks, this state is not distributed in space but in time. Hardware implementations with low hardware component counts are therefore particularly easy to design for delay-coupled systems. In this paper, we present an argument for using delay-coupled reservoirs using multiple feedback terms with different delays. We present a theoretical analysis of the resulting system, discuss surprising effects pertaining to the precise choice of delays, and provide a guideline for the optimal design of such systems.},
	number = {2/3},
	journal = {IBM Journal of Research and Development},
	author = {Nieters, P. and Leugering, J. and Pipa, G.},
	month = mar,
	year = {2017},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Biological neural networks, Computational modeling, Computer architecture, Learning systems, Neuromorphics, Noise measurement},
	pages = {8:7--8:9},
}

@article{liang_neuromorphic_2022,
	title = {A {Neuromorphic} {Model} {With} {Delay}-{Based} {Reservoir} for {Continuous} {Ventricular} {Heartbeat} {Detection}},
	volume = {69},
	issn = {1558-2531},
	doi = {10.1109/TBME.2021.3129306},
	abstract = {There is a growing interest in neuromorphic hardware since it offers a more intuitive way to achieve bio-inspired algorithms. This paper presents a neuromorphic model for intelligently processing continuous electrocardiogram (ECG) signal. This model aims to develop a hardware-based signal processing model and avoid employing digitally intensive operations, such as signal segmentation and feature extraction, which are not desired in an analogue neuromorphic system. We apply delay-based reservoir computing as the information processing core, along with a novel training and labelling method. Different from the conventional ECG classification techniques, this computation model is a end-to-end dynamic system that mimics the real-time signal flow in neuromorphic hardware. The input is the raw ECG stream, while the amplitude of the output represents the risk factor of a ventricular ectopic heartbeat. The intrinsic memristive property of the reservoir empowers the system to retain the historical ECG information for high-dimensional mapping. This model was evaluated with the MIT-BIH database under the inter-patient paradigm and yields 81\% sensitivity and 98\% accuracy. Under this architecture, the minimum size of memory required in the inference process can be as low as 3.1 MegaByte(MB) because the majority of the computation takes place in the analogue domain. Such computational modelling boosts memory efficiency by simplifying the computing procedure and minimizing the required memory for future wearable devices.},
	number = {6},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Liang, Xiangpeng and Li, Haobo and Vuckovic, Aleksandra and Mercer, John and Heidari, Hadi},
	month = jun,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	keywords = {Computational modeling, Continuous ventricular heartbeat detection, Databases, Electrocardiography, Heart beat, Memory efficient analogue computing, Neuromorphics, Neurons, Physical neural network, Reservoirs, delay-based reservoir computing},
	pages = {1837--1849},
}

@inproceedings{hussain_deltron_2012,
	title = {{DELTRON}: {Neuromorphic} architectures for delay based learning},
	shorttitle = {{DELTRON}},
	doi = {10.1109/APCCAS.2012.6419032},
	abstract = {We present a neuromorphic spiking neural network, the DELTRON, that can remember and store patterns by changing the delays of every connection as opposed to modifying the weights. The advantage of this architecture over traditional weight based ones is simpler hardware implementation without multipliers or digital-analog converters (DACs). The name is derived due to similarity in the learning rule with an earlier architecture called Tempotron. We present simulations of memory capacity of the DELTRON for different random spatio-temporal spike patterns and also present SPICE simulation results of the core circuits involved in a reconfigurable mixed signal implementation of this architecture.},
	booktitle = {2012 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems}},
	author = {Hussain, Shaista and Basu, Arindam and Wang, Mark and Hamilton, Tara Julia},
	month = dec,
	year = {2012},
	keywords = {Computer architecture, Delay, Nerve fibers, Neuromorphics, Registers, Training},
	pages = {304--307},
}

@article{sandamirskaya_neuromorphic_2022,
	title = {Neuromorphic computing hardware and neural architectures for robotics},
	volume = {7},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.abl8419},
	doi = {10.1126/scirobotics.abl8419},
	abstract = {Neuromorphic hardware enables fast and power-efficient neural network–based artificial intelligence that is well suited to solving robotic tasks. Neuromorphic algorithms can be further developed following neural computing principles and neural network architectures inspired by biological neural systems. In this Viewpoint, we provide an overview of recent insights from neuroscience that could enhance signal processing in artificial neural networks on chip and unlock innovative applications in robotics and autonomous intelligent systems. These insights uncover computing principles, primitives, and algorithms on different levels of abstraction and call for more research into the basis of neural computation and neuronally inspired computing hardware.},
	number = {67},
	urldate = {2022-11-10},
	journal = {Science Robotics},
	author = {Sandamirskaya, Yulia and Kaboli, Mohsen and Conradt, Jorg and Celikel, Tansu},
	month = jun,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabl8419},
}

@article{markovic_physics_2020,
	title = {Physics for neuromorphic computing},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5820},
	url = {http://www.nature.com/articles/s42254-020-0208-2},
	doi = {10.1038/s42254-020-0208-2},
	abstract = {Neuromorphic computing takes inspiration from the brain to create energy-efficient hardware for information processing, capable of highly sophisticated tasks. Systems built with standard electronics achieve gains in speed and energy by mimicking the distributed topology of the brain. Scaling-up such systems and improving their energy usage, speed and performance by several orders of magnitude requires a revolution in hardware. We discuss how including more physics in the algorithms and nanoscale materials used for data processing could have a major impact in the field of neuromorphic computing. We review striking results that leverage physics to enhance the computing capabilities of artificial neural networks, using resistive switching materials, photonics, spintronics and other technologies. We discuss the paths that could lead these approaches to maturity, towards low-power, miniaturized chips that could infer and learn in real time.},
	language = {en},
	number = {9},
	urldate = {2022-11-10},
	journal = {Nature Reviews Physics},
	author = {Marković, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
	month = sep,
	year = {2020},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Electronics, Nanoscale devices, photonics and device physics},
	pages = {499--510},
}

@inproceedings{kim_real-time_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {3D} {Reconstruction} and 6-{DoF} {Tracking} with an {Event} {Camera}},
	isbn = {978-3-319-46466-4},
	doi = {10.1007/978-3-319-46466-4_21},
	abstract = {We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Kim, Hanme and Leutenegger, Stefan and Davison, Andrew J.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {3D reconstruction, 6-DoF tracking, Event-based camera, Intensity reconstruction, SLAM, Visual odometry},
	pages = {349--364},
}

@inproceedings{stoffregen_event-based_2019,
	title = {Event-{Based} {Motion} {Segmentation} by {Motion} {Compensation}},
	doi = {10.1109/ICCV.2019.00734},
	abstract = {In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called "events"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10\%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90\% accuracy at 4 pixels relative displacement.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Stoffregen, Timo and Gallego, Guillermo and Drummond, Tom and Kleeman, Lindsay and Scaramuzza, Davide},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Cameras, Computer vision, Image segmentation, Motion compensation, Motion segmentation, Robot vision systems, Tracking},
	pages = {7243--7252},
}

@article{hidalgo-carrio_learning_2020,
	title = {Learning {Monocular} {Dense} {Depth} from {Events}},
	url = {http://arxiv.org/abs/2010.08350},
	abstract = {Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous events instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50\% improvement in average depth error with respect to previous event-based methods.},
	urldate = {2021-01-22},
	journal = {arXiv:2010.08350 [cs]},
	author = {Hidalgo-Carrió, Javier and Gehrig, Daniel and Scaramuzza, Davide},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{clady_asynchronous_2014,
	title = {Asynchronous visual event-based time-to-contact},
	volume = {8},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2014.00009/full},
	doi = {10.3389/fnins.2014.00009},
	abstract = {A reliable and fast sensing of the environment is a fundamental necessity of mobile platforms. Unfortunately conventional cameras due to the current frame-based acquisition paradigm output low temporal dynamics and redundant data flow leading to high computational costs. It is obviously incompatible with the necessities of mobile platforms where energy consumption and computational load are a major issue. The restrictions of the frame-based paradigm are contradictory with applications requiring high speed sensor-based reactive control. This paper introduces a fast obstacle avoidance using the output of an asynchronous event-based time encoded imaging sensor. The approach is event-based in the sense that every incoming event adds to the computation process thus allowing fast avoidance responses. It introduces an event-based time-to-contact approach relying on the computation of visual event-based motion flows. Experiments on a mobile robot are presented in an indoor environment. Time to contact results are compared with those provided by a laser range finder showing that event-based sensing offers new perspectives for mobile robotics sensing.},
	language = {English},
	urldate = {2020-11-04},
	journal = {Frontiers in Neuroscience},
	author = {Clady, Xavier and Clercq, Charles and Ieng, Sio-Hoi and Houseini, Fouzhan and Randazzo, Marco and Natale, Lorenzo and Bartolozzi, Chiara and Benosman, Ryad Benjamin},
	year = {2014},
	note = {00000 
Publisher: Frontiers},
	keywords = {Computer Vision, Event-based Computation, Neuromorphic vision, Robotics, time-to-contact},
}

@article{perez-cerda_pio_2015,
	title = {Pío del {Río} {Hortega} and the discovery of the oligodendrocytes},
	volume = {9},
	issn = {1662-5129},
	doi = {10.3389/fnana.2015.00092},
	abstract = {Pío del Río Hortega (1882-1945) discovered microglia and oligodendrocytes (OLGs), and after Ramón y Cajal, was the most prominent figure of the Spanish school of neurology. He began his scientific career with Nicolás Achúcarro from whom he learned the use of metallic impregnation techniques suitable to study non-neuronal cells. Later on, he joined Cajal's laboratory. and Subsequently, he created his own group, where he continued to develop other innovative modifications of silver staining methods that revolutionized the study of glial cells a century ago. He was also interested in neuropathology and became a leading authority on Central Nervous System (CNS) tumors. In parallel to this clinical activity, del Río Hortega rendered the first systematic description of a major polymorphism present in a subtype of macroglial cells that he named as oligodendroglia and later OLGs. He established their ectodermal origin and suggested that they built the myelin sheath of CNS axons, just as Schwann cells did in the periphery. Notably, he also suggested the trophic role of OLGs for neuronal functionality, an idea that has been substantiated in the last few years. Del Río Hortega became internationally recognized and established an important neurohistological school with outstanding pupils from Spain and abroad, which nearly disappeared after his exile due to the Spanish civil war. Yet, the difficulty of metal impregnation methods and their variability in results, delayed for some decades the confirmation of his great insights into oligodendrocyte biology until the development of electron microscopy and immunohistochemistry. This review aims at summarizing the pioneer and essential contributions of del Río Hortega to the current knowledge of oligodendrocyte structure and function, and to provide a hint of the scientific personality of this extraordinary and insufficiently recognized man.},
	language = {eng},
	journal = {Frontiers in Neuroanatomy},
	author = {Pérez-Cerdá, Fernando and Sánchez-Gómez, María Victoria and Matute, Carlos},
	year = {2015},
	pmid = {26217196},
	pmcid = {PMC4493393},
	keywords = {Del Río Hortega, Ramón y Cajal, myelin sheath, oligodendrocyte precursor cell (OPC), oligodendroglia},
	pages = {92},
}

@article{stetson_effects_1992,
	title = {Effects of age, sex, and anthropometric factors on nerve conduction measures},
	volume = {15},
	issn = {1097-4598},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mus.880151007},
	doi = {10.1002/mus.880151007},
	abstract = {Associations among measures of median, ulnar, and sural nerve conduction and age, skin temperature, sex, and anthropometric factors were evaluated in a population of 105 healthy, asymptomatic adults without occupational exposure to highly repetitive or forceful hand exertions. Height was negatively associated with sensory amplitude in all nerves tested (P {\textless} 0.001), and positively associated with median and ulnar sensory distal latencies (P {\textless} 0.01) and sural latency (P {\textless} 0.001). Index finger circumference was negatively associated with median and ulnar sensory amplitudes (P {\textless} 0.05). Sex, in isolation from highly correlated anthropometric factors such as height, was not found to be a significant predictor of median or ulnar nerve conduction measures. Equations using age, height, and finger circumference for prediction of normal values are presented. Failure to adjust normal nerve conduction values for these factors decreases the diagnostic specificity and sensitivity of the described measures, and may result in misclassification of individuals. © 1992 John Wiley \& Sons, Inc.},
	language = {en},
	number = {10},
	urldate = {2022-11-10},
	journal = {Muscle \& Nerve},
	author = {Stetson, Diana S. and Albers, James W. and Silverstein, Barbara A. and Wolfe, Robert A.},
	year = {1992},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mus.880151007},
	keywords = {age, anthropometry, height, nerve conduction studies, normal values},
	pages = {1095--1104},
}

@article{von_helmholz_messungen_1850,
	title = {Messungen über den zeitlichen {Verlauf} der {Zuckung} animalischer {Muskeln} und die {Fortpflanzungsgeschwindigkeit} der {Reizung} in den {Nerven}},
	volume = {17},
	url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2346292},
	language = {deu},
	urldate = {2022-11-09},
	journal = {Archiv für Anatomie, Physiologie und wissenschaftliche Medicin},
	author = {Von Helmholz, H.},
	year = {1850},
	pages = {176--364},
}

@article{peyrard_how_2020,
	title = {How is information transmitted in a nerve?},
	volume = {46},
	issn = {0092-0606},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7719126/},
	doi = {10.1007/s10867-020-09557-2},
	abstract = {In the last 15 years, a debate has emerged about the validity of the famous Hodgkin-Huxley model for nerve impulse. Mechanical models have been proposed. This note reviews the experimental properties of the nerve impulse and discusses the proposed alternatives. The experimental data, which rule out some of the alternative suggestions, show that while the Hodgkin-Huxley model may not be complete, it nevertheless includes essential features that should not be overlooked in the attempts made to improve, or supersede, it.},
	number = {4},
	urldate = {2022-11-08},
	journal = {Journal of Biological Physics},
	author = {Peyrard, Michel},
	month = dec,
	year = {2020},
	pmid = {33037976},
	pmcid = {PMC7719126},
	pages = {327--341},
}

@article{jazayeri_optimal_2006,
	title = {Optimal representation of sensory information by neural populations},
	volume = {9},
	doi = {10.1038/nn1691},
	number = {5},
	journal = {Nature neuroscience},
	author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
	year = {2006},
	note = {Publisher: Nature Publishing Group},
	pages = {690--696},
}

@article{lichtsteiner_128x128_2008,
	title = {A 128x128, 120 {dB} 15 ms {Latency} {Asynchronous} {Temporal} {Contrast} {Vision} {Sensor}},
	volume = {2},
	issn = {0018-9200, 1558-173X},
	url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000004444573},
	doi = {10.1109/JSSC.2007.914337},
	abstract = {This paper describes a 128 times 128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1\% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is \&amp;gt; 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at \&amp;gt; 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum\&lt;sup\&gt;2\&lt;/sup\&gt; pixels with 9.4\% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.},
	language = {English},
	number = {43},
	urldate = {2022-11-08},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Lichtsteiner, P. and Posch, C. and Delbruck, T.},
	year = {2008},
	pages = {566--576},
}

@book{flourens_recherches_1842,
	title = {Recherches expérimentales sur les propriétés et les fonctions du système nerveux, dans les animaux vertébrés},
	language = {fr},
	publisher = {J.-B. Balliere},
	author = {Flourens, Marie Jean Pierre},
	year = {1842},
}

@article{schmitt_ultrastructure_1939,
	title = {The {Ultrastructure} of the {Nerve} {Axon} {Sheath}},
	volume = {14},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-185X.1939.tb00922.x},
	doi = {10.1111/j.1469-185X.1939.tb00922.x},
	abstract = {1. In avoiding certain inherent indeterminacies in classical morphological methods and in obtaining further details regarding the microscopic and ultra-microscopic structure of nerve axon sheaths, the methods of polarization optics and X-ray diffraction are of great value. In the case of the myelin sheaths of vertebrate nerve fibres, for example, the optical and diffraction studies indicate the structure of the living fibre's sheath to be of smectic mixed fluid-crystalline nature. The structure is, therefore, readily altered by chemical treatment to form the artifacts commonly observed in histological preparations. 2. A number of considerations suggest that the specific configuration of the lipoid and protein components of the myelin sheath is as follows. The proteins occur as thin sheets wrapped concentrically about the axon, with two bimolecular layers of lipoids interspersed between adjacent protein layers. While this means that in a radial direction within the cylindrical sheath there are alternate predominantly aqueous and predominantly hyirocarbon phases, the latter cannot be described as being entirely “non-aqueous” 3. Polarization optical studies show that, contrary to the general view, invertebrate nerve fibres quite widely possess, aside from connective tissue investments, thin sheaths which are essentially similar in ultrastructure to the well-defined myelin sheaths of vertebrate fibres. The demonstration of this fact involved a reinterpretation of the meaning of Gothlin's metatropic reaction, in which immersion of the fibre in media of high refractive index permits the (intrinsic) birefringence of lipoids present in the normal sheath in an oriented condition to become apparent by the reduction of the masking (form) double refraction of protein. Associated with the invertebrate metatropic axon sheaths are cells similar to the Schwann cells of vertebrate fibres. 4. Quantitative birefringence studies have disclosed that the axon sheaths of a wide variety of fibre types differ chiefly with respect to the relative amounts of oriented protein and lipoid present. This difference is observed not only between typical invertebrate and vertebrate fibres, but also when the fibres of a single vertebrate nerve are compared. For example, the curve obtained when sheath birefringence of frog sciatic fibres is plotted against fibre diameter shows wide variations in the magnitude of double refraction, changing continuously from birefringence due preponderantly to lipoids, in the case of the larger fibres, to that which, in the smallest fibres, results primarily from proteins. The transition from lipoid to protein predominance occurs at a fibre diameter of about 2μ., agreeing well with the division between “medullated” and “non-medullated” fibres arrived at by histologists. It has been suggested that the low concentration of lipoid in the sheaths of small fibres is related to physical factors opposing the introduction of the lipoids into cylindrical structures of high curvature. 5. Examination of available information with respect to the relation of the velocity of impulse propagation to certain fibre characteristics, such as diameter and sheath ultrastructure, indicates that in a wide variety of fibres conduction velocity is a function of both of these factors. Thus, if fibres from invertebrate and vertebrate sources are classified according to sheath composition and ultrastructure, it is found that, within a group having similar sheaths, fast conduction is favoured by large diameter, while between groups with different sheaths, heavy myelination results in faster propagation. Comparison of fibre velocities with diameter alone, without regard to degree of myelination, is apt to be confusing, a fact which should be borne in mind in attempting to relate conduction velocity to diameter in a nerve, such as the frog sciatic, which contains fibres with very different sheaths. 6. Several types of invertebrate and vertebrate unipolar ganglion cells have been observed to possess investments similar to the axon sheaths and continuous with the latter. The entire surface of these neurons, therefore, is provided with a characteristic lipoid-protein covering, except possibly at the nodes of the myelin sheaths of the vertebrate sensory axons. The limiting envelopes of certain other cells and nuclei have been shown to possess an ultrastructure similar in type to that of the axon sheath. Permeability studies on cells have indicated the importance of lipoids and proteins in determining the properties of the plasma membrane, but it cannot be concluded that the visible envelopes are identical with the membrane which determines the physiological properties, since electrical and chemical studies favour the view that this membrane is extremely thin. The parallelisms observed between nerve sheath ultrastructure and physiological function, however, suggest some relation of these to membrane phenomena, and it is particularly difficult to understand how a multilayered structure, such as the vertebrate axon's myelin sheath, could fail to influence the chemical and electrical accessibility of the axon.},
	language = {en},
	number = {1},
	urldate = {2022-11-08},
	journal = {Biological Reviews},
	author = {Schmitt, Francis O. and Bear, Richard S.},
	year = {1939},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-185X.1939.tb00922.x},
	pages = {27--50},
}

@article{reynolds_study_1928,
	title = {A {Study} of the {Structure} and {Function} of the {Interstitial} {Tissue} of the {Central} {Nervous} {System}},
	volume = {35},
	issn = {0367-1038},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5297025/},
	number = {2},
	urldate = {2022-11-08},
	journal = {Edinburgh Medical Journal},
	author = {Reynolds, F. E. and Slater, James K.},
	month = feb,
	year = {1928},
	pmid = {null},
	pmcid = {PMC5297025},
	pages = {49--57},
}

@article{brill_conduction_1977,
	title = {Conduction velocity and spike configuration in myelinated fibres: computed dependence on internode distance.},
	volume = {40},
	issn = {0022-3050},
	shorttitle = {Conduction velocity and spike configuration in myelinated fibres},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC492833/},
	abstract = {It has been argued theoretically and confirmed experimentally that conduction velocity (theta) should be proportional to nerve fibre diameter for myelinated fibre tracts, such as normal peripheral nerve, exhibiting 'structural' similarity'. In some axons, however, the nodes of Ranvier are more closely spaced than in normal peripheral nerve. Analytic arguments have suggested that when internodal distance (L) alone is changed, the plot of theta versus L should have a relatively flat maximum. This was confirmed by several previous computer simulations of myelinated axons, but internode lengths of less than half the normal case were not examined. In order to gain insight into impulse propagation in myelinated and remyelinated fibres with short internodal lengths, the present study examines the conduction velocity and spike configuration for a wide range of internodal lengths. As L becomes large, theta falls and finally propagation is blocked; as L becomes small, theta decreases more and more steeply. From this, it is predicted that for fibres with very short internodal lengths, small local changes in L should affect substantially the conduction velocity.},
	number = {8},
	urldate = {2022-11-07},
	journal = {Journal of Neurology, Neurosurgery, and Psychiatry},
	author = {Brill, M H and Waxman, S G and Moore, J W and Joyner, R W},
	month = aug,
	year = {1977},
	pmid = {925697},
	pmcid = {PMC492833},
	pages = {769--774},
}

@article{gasser_axon_1939,
	title = {{AXON} {DIAMETERS} {IN} {RELATION} {TO} {THE} {SPIKE} {DIMENSIONS} {AND} {THE} {CONDUCTION} {VELOCITY} {IN} {MAMMALIAN} {A} {FIBERS}},
	copyright = {Copyright © 1939 by American Physiological Society},
	url = {https://journals.physiology.org/doi/10.1152/ajplegacy.1939.127.2.393},
	doi = {10.1152/ajplegacy.1939.127.2.393},
	language = {en},
	urldate = {2022-11-07},
	journal = {American Journal of Physiology-Legacy Content},
	author = {Gasser, H. S. and Grundfest, H.},
	month = aug,
	year = {1939},
	note = {Publisher: American Physiological Society},
}

@article{benosman_event-based_2014,
	title = {Event-{Based} {Visual} {Flow}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	url = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	doi = {10.1109/tnnls.2013.2273537},
	abstract = {This paper introduces a new methodology to compute dense visual ﬂow using the precise timings of spikes from an asynchronous event-based retina. Biological retinas, and their artiﬁcial counterparts, are totally asynchronous and data-driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework to estimate visual ﬂow from the local properties of events’ spatiotemporal space. We will show that precise visual ﬂow orientation and amplitude can be estimated using a local differential approach on the surface deﬁned by coactive events. Experimental results are presented; they show the method adequacy with high data sparseness and temporal resolution of event-based acquisition that allows the computation of motion ﬂow with microsecond accuracy and at very low computational cost.},
	language = {en},
	number = {2},
	urldate = {2022-02-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Benosman, Ryad and Clercq, Charles and Lagorce, Xavier and {Sio-Hoi Ieng} and Bartolozzi, Chiara},
	month = feb,
	year = {2014},
	pages = {407--417},
}

@article{bonilla_analyzing_2022,
	title = {Analyzing time-to-first-spike coding schemes},
	volume = {16},
	url = {https://hal.archives-ouvertes.fr/hal-03796195},
	doi = {10.3389/fnins.2022.971937},
	abstract = {Spiking neural networks (SNNs) using time-to-first-spike (TTFS) codes, in which neurons fire at most once, are appealing for rapid and low power processing. In this theoretical paper, we focus on information coding and decoding in those networks, and introduce a new unifying mathematical framework that allows the comparison of various coding schemes. In an early proposal, called rank-order coding (ROC), neurons are maximally activated when inputs arrive in the order of their synaptic weights, thanks to a shunting inhibition mechanism that progressively desensitizes the neurons as spikes arrive. In another proposal, called NoM coding, only the first N spikes of M input neurons are propagated, and these “first spike patterns” can be readout by downstream neurons with homogeneous weights and no desensitization: as a result, the exact order between the first spikes does not matter. This paper also introduces a third option—“Ranked-NoM” (R-NoM), which combines features from both ROC and NoM coding schemes: only the first N input spikes are propagated, but their order is readout by downstream neurons thanks to inhomogeneous weights and linear desensitization. The unifying mathematical framework allows the three codes to be compared in terms of discriminability, which measures to what extent a neuron responds more strongly to its preferred input spike pattern than to random patterns. This discriminability turns out to be much higher for R-NoM than for the other codes, especially in the early phase of the responses. We also argue that R-NoM is much more hardware-friendly than the original ROC proposal, although NoM remains the easiest to implement in hardware because it only requires binary synapses.},
	urldate = {2022-11-08},
	journal = {Frontiers in Neuroscience},
	author = {Bonilla, Lina and Gautrais, Jacques and Thorpe, Simon and Masquelier, Timothée},
	year = {2022},
	note = {Publisher: Frontiers},
}

@article{ermentrout_reliability_2008,
	title = {Reliability, synchrony and noise},
	volume = {31},
	issn = {0166-2236},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2574942/},
	doi = {10.1016/j.tins.2008.06.002},
	abstract = {The brain is noisy. Neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular. Much of this activity is spontaneous—uncoupled to overt stimuli or motor outputs—leading to questions about the functional impact of this noise. Although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli, recent theoretical and experimental work has shown that noise can play a constructive role—leading to increased reliability or regularity of neuronal firing in single neurons and across populations. These results raise fundamental questions about how noise can influence neural function and computation.},
	number = {8},
	urldate = {2022-11-08},
	journal = {Trends in neurosciences},
	author = {Ermentrout, G. Bard and Galán, Roberto F. and Urban, Nathaniel N.},
	month = aug,
	year = {2008},
	pmid = {18603311},
	pmcid = {PMC2574942},
	pages = {428--434},
}

@article{vanni_coinciding_2001,
	title = {Coinciding early activation of the human primary visual cortex  and anteromedial cuneus},
	volume = {98},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC30215/},
	doi = {10.1073/pnas.041600898},
	abstract = {Proper understanding of processes underlying visual perception
 requires information on the activation order of distinct brain areas.
 We measured dynamics of cortical signals with magnetoencephalography
 while human subjects viewed stimuli at four visual quadrants. The
 signals were analyzed with minimum current estimates at the individual
 and group level. Activation emerged 55–70 ms after stimulus onset both
 in the primary posterior visual areas and in the anteromedial part of
 the cuneus. Other cortical areas were active after this initial dual
 activation. Comparison of data between species suggests that the
 anteromedial cuneus either comprises a homologue of the monkey area V6
 or is an area unique to humans. Our results show that visual stimuli
 activate two cortical areas right from the beginning of the cortical
 response. The anteromedial cuneus has the temporal position needed to
 interact with the primary visual cortex V1 and thereby to modify
 information transferred via V1 to extrastriate cortices.},
	number = {5},
	urldate = {2022-11-08},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Vanni, Simo and Tanskanen, Topi and Seppä, Mika and Uutela, Kimmo and Hari, Riitta},
	month = feb,
	year = {2001},
	pmid = {11226316},
	pmcid = {PMC30215},
	pages = {2776--2780},
}

@article{schmolesky_signal_1998,
	title = {Signal timing across the macaque visual system},
	volume = {79},
	issn = {0022-3077},
	doi = {10.1152/jn.1998.79.6.3272},
	abstract = {The onset latencies of single-unit responses evoked by flashing visual stimuli were measured in the parvocellular (P) and magnocellular (M) layers of the dorsal lateral geniculate nucleus (LGNd) and in cortical visual areas V1, V2, V3, V4, middle temporal area (MT), medial superior temporal area (MST), and in the frontal eye field (FEF) in individual anesthetized monkeys. Identical procedures were carried out to assess latencies in each area, often in the same monkey, thereby permitting direct comparisons of timing across areas. This study presents the visual flash-evoked latencies for cells in areas where such data are common (V1 and V2), and are therefore a good standard, and also in areas where such data are sparse (LGNd M and P layers, MT, V4) or entirely lacking (V3, MST, and FEF in anesthetized preparation). Visual-evoked onset latencies were, on average, 17 ms shorter in the LGNd M layers than in the LGNd P layers. Visual responses occurred in V1 before any other cortical area. The next wave of activation occurred concurrently in areas V3, MT, MST, and FEF. Visual response latencies in areas V2 and V4 were progressively later and more broadly distributed. These differences in the time course of activation across the dorsal and ventral streams provide important temporal constraints on theories of visual processing.},
	language = {eng},
	number = {6},
	journal = {Journal of Neurophysiology},
	author = {Schmolesky, M. T. and Wang, Y. and Hanes, D. P. and Thompson, K. G. and Leutgeb, S. and Schall, J. D. and Leventhal, A. G.},
	month = jun,
	year = {1998},
	pmid = {9636126},
	keywords = {Animals, Evoked Potentials, Visual, Macaca, Neurons, Photic Stimulation, Signal Transduction, Time Factors, Vision, Ocular, Visual Cortex},
	pages = {3272--3278},
}

@inproceedings{grimaldi_learning_2022,
	title = {Learning hetero-synaptic delays for motion detection in a single layer of spiking neurons},
	doi = {10.1109/ICIP46576.2022.9897394},
	abstract = {The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. However, most artificial neuronal models do not take advantage of this minute temporal dimension and here, we develop a model for the efficient detection of temporal spiking motifs based on a layer of neurons with hetero-synaptic delays. Indeed, the variety of synaptic delays on the dendritic tree allows to synchronize synaptic inputs as they reach the basal dendritic tree. We show this can be formalized as a time-invariant logistic regression which can be trained using labelled data. We apply this model to solve the specific computer vision problem of motion detection, and demonstrate its application to synthetic naturalistic videos transformed into event streams similar to the output of event-based cameras. In particular, we quantify how its accuracy can vary with the total computational load. This end-to-end event-driven computational brick could help improve the performance of future Spiking Neural Network (SNN) algorithms and their prospective use in neuromorphic chips.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Grimaldi, Antoine and Perrinet, Laurent U},
	month = oct,
	year = {2022},
	note = {ISSN: 2381-8549},
	keywords = {Biological neural networks, Cameras, Delays, Motion detection, Neuromorphics, Neurons, Synchronization, efficient coding, event-based computations, logistic regression, motion detection, spiking neural networks, time code},
	pages = {3591--3595},
}

@inproceedings{sironi_hats_2018,
	address = {Salt Lake City, UT, USA},
	title = {{HATS}: {Histograms} of {Averaged} {Time} {Surfaces} for {Robust} {Event}-{Based} {Object} {Classification}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{HATS}},
	url = {https://ieeexplore.ieee.org/document/8578284/},
	doi = {10.1109/CVPR.2018.00186},
	abstract = {Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classiﬁcation algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their framebased counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classiﬁcation and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efﬁciently leverage past temporal information and build a robust eventbased representation. Second, we release the ﬁrst large real-world event-based dataset for object classiﬁcation. We compare our method to the state-of-the-art with extensive experiments, showing better classiﬁcation performance and real-time computation.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sironi, Amos and Brambilla, Manuele and Bourdis, Nicolas and Lagorce, Xavier and Benosman, Ryad},
	month = jun,
	year = {2018},
	pages = {1731--1740},
}

@article{maro_event-based_2020,
	title = {Event-{Based} {Gesture} {Recognition} {With} {Dynamic} {Background} {Suppression} {Using} {Smartphone} {Computational} {Capabilities}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00275/full?report=reader},
	doi = {10.3389/fnins.2020.00275},
	abstract = {In this paper, we introduce a framework for dynamic gesture recognition with background suppression operating on the output of a moving event-based camera. The system is developed to operate in real-time using only the computational capabilities of a mobile phone. It introduces a new development around the concept of time-surfaces. It also presents a novel event-based methodology to dynamically remove backgrounds that uses the high temporal resolution properties of event-based cameras. To our knowledge, this is the first Android event-based framework for vision-based recognition of {\textbackslash}textit\{dynamic\} gestures running on a smartphone without off-board processing. We assess the performances by considering several scenarios in both indoors and outdoors, for static and dynamic conditions, in uncontrolled lighting conditions. We also introduce a new event-based dataset for gesture recognition with static and dynamic backgrounds (made publicly available). The set of gestures has been selected following a clinical trial to allow human-machine interaction for the visually impaired and older adults. We finally report comparisons with prior work that addressed event-based gesture recognition reporting comparable results, without the use of advanced classification techniques nor power greedy hardware.},
	language = {English},
	urldate = {2021-01-15},
	journal = {Frontiers in Neuroscience},
	author = {Maro, Jean-Matthieu and Ieng, Sio-Hoi and Benosman, Ryad},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Background suppression, Dynamic gesture recognition, Neuromorphic, dynamic vision sensor (DVS), event-based, gesture recognition, mobile device, smartphone},
}

@techreport{chintaluri_metabolically_2022,
	type = {preprint},
	title = {Metabolically driven action potentials serve neuronal energy homeostasis and protect from reactive oxygen species},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.10.16.512428},
	abstract = {So-called spontaneous neuronal activity is a central hallmark of most nervous systems. Such non-causal firing is contrary to the tenet of spikes as a means of communication, and its origin and purpose remain unclear. Here, we propose that non-input driven firing can serve as a release valve to protect neurons from the toxic conditions arising in mitochondria from lower-than-baseline energy consumption. We built a framework of models that incorporate homeostatic control of metabolic products–ATP, ADP, and reactive oxygen species, among others–by way of changes in firing. Our theory can account for key features of neuronal activity observed in many experiments in studies ranging from ion channels function all the way to resting state dynamics. We propose an integrated, crucial role for metabolic spiking that bridges the gap between metabolic homeostasis and neuronal function. Finally, we make testable predictions to validate or falsify our theory.},
	language = {en},
	urldate = {2022-11-07},
	institution = {Neuroscience},
	author = {Chintaluri, Chaitanya and Vogels, Tim P.},
	month = oct,
	year = {2022},
	doi = {10.1101/2022.10.16.512428},
}

@article{seidl_mechanisms_2010,
	title = {Mechanisms for adjusting interaural time differences to achieve binaural coincidence detection},
	volume = {30},
	issn = {1529-2401},
	doi = {10.1523/JNEUROSCI.3464-09.2010},
	abstract = {Understanding binaural perception requires detailed analyses of the neural circuitry responsible for the computation of interaural time differences (ITDs). In the avian brainstem, this circuit consists of internal axonal delay lines innervating an array of coincidence detector neurons that encode external ITDs. Nucleus magnocellularis (NM) neurons project to the dorsal dendritic field of the ipsilateral nucleus laminaris (NL) and to the ventral field of the contralateral NL. Contralateral-projecting axons form a delay line system along a band of NL neurons. Binaural acoustic signals in the form of phase-locked action potentials from NM cells arrive at NL and establish a topographic map of sound source location along the azimuth. These pathways are assumed to represent a circuit similar to the Jeffress model of sound localization, establishing a place code along an isofrequency contour of NL. Three-dimensional measurements of axon lengths reveal major discrepancies with the current model; the temporal offset based on conduction length alone makes encoding of physiological ITDs impossible. However, axon diameter and distances between Nodes of Ranvier also influence signal propagation times along an axon. Our measurements of these parameters reveal that diameter and internode distance can compensate for the temporal offset inferred from axon lengths alone. Together with other recent studies, these unexpected results should inspire new thinking on the cellular biology, evolution, and plasticity of the circuitry underlying low-frequency sound localization in both birds and mammals.},
	language = {eng},
	number = {1},
	journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
	author = {Seidl, Armin H. and Rubel, Edwin W. and Harris, David M.},
	month = jan,
	year = {2010},
	pmid = {20053889},
	pmcid = {PMC2822993},
	keywords = {Acoustic Stimulation, Animals, Animals, Newborn, Auditory Pathways, Auditory Perception, Brain Stem, Chickens, Nerve Net, Sound Localization, Time Factors, itd},
	pages = {70--80},
}

@article{goltz_fast_2020,
	title = {Fast and deep: energy-efficient neuromorphic learning with first-spike times},
	shorttitle = {Fast and deep},
	url = {http://arxiv.org/abs/1912.11443},
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems also strive for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. In the time-to-first-spike-coding framework, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of learning such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how it can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the chip's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	urldate = {2021-03-02},
	journal = {arXiv:1912.11443 [cs, q-bio, stat]},
	author = {Göltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	month = nov,
	year = {2020},
	note = {arXiv: 1912.11443},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	url = {https://ieeexplore.ieee.org/document/9138762/},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a ﬁxed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of ms), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging ﬁeld of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic ﬂow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efﬁcient, bio-inspired way for machines to perceive and interact with the world.},
	language = {en},
	number = {1},
	urldate = {2022-07-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jorg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	pages = {154--180},
}

@article{zenke_visualizing_2021,
	title = {Visualizing a joint future of neuroscience and neuromorphic engineering},
	volume = {109},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S089662732100009X},
	doi = {10.1016/j.neuron.2021.01.009},
	abstract = {Recent research resolves the challenging problem of building biophysically plausible spiking neural models that are also capable of complex information processing. This advance creates new opportunities in neuroscience and neuromorphic engineering, which we discussed at an online focus meeting.},
	language = {en},
	number = {4},
	urldate = {2021-02-24},
	journal = {Neuron},
	author = {Zenke, Friedemann and Bohté, Sander M. and Clopath, Claudia and Comşa, Iulia M. and Göltz, Julian and Maass, Wolfgang and Masquelier, Timothée and Naud, Richard and Neftci, Emre O. and Petrovici, Mihai A. and Scherr, Franz and Goodman, Dan F. M.},
	month = feb,
	year = {2021},
	pages = {571--575},
}

@article{christensen_2022_2022,
	title = {2022 roadmap on neuromorphic computing and engineering},
	issn = {2634-4386},
	url = {http://iopscience.iop.org/article/10.1088/2634-4386/ac4a83},
	doi = {10.1088/2634-4386/ac4a83},
	abstract = {Modern computation based on the von Neumann architecture is today a mature cutting-edge science. In the Von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this Roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The Roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this Roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.},
	language = {en},
	urldate = {2022-01-13},
	journal = {Neuromorphic Computing and Engineering},
	author = {Christensen, Dennis Valbjørn and Dittmann, Regina and Linares-Barranco, Bernabe and Sebastian, Abu and Le Gallo, Manuel and Redaelli, Andrea and Slesazeck, Stefan and Mikolajick, Thomas and Spiga, Sabina and Menzel, Stephan and Valov, Ilia and Milano, Gianluca and Ricciardi, Carlo and Liang, Shi-Jun and Miao, Feng and Lanza, Mario and Quill, Tyler J. and Keene, Scott Tom and Salleo, Alberto and Grollier, Julie and Markovic, Danijela and Mizrahi, Alice and Yao, Peng and Yang, J. Joshua and Indiveri, Giacomo and Strachan, John Paul and Datta, Suman and Vianello, Elisa and Valentian, Alexandre and Feldmann, Johannes and Li, Xuan and Pernice, Wolfram HP and Bhaskaran, Harish and Furber, Steve and Neftci, Emre and Scherr, Franz and Maass, Wolfgang and Ramaswamy, Srikanth and Tapson, Jonathan and Panda, Priyadarshini and Kim, Youngeun and Tanaka, Gouhei and Thorpe, Simon and Bartolozzi, Chiara and Cleland, Thomas A and Posch, Christoph and Liu, Shih-Chii and Panuccio, Gabriella and Mahmud, Mufti and Mazumder, Arnab Neelim and Hosseini, Morteza and Mohsenin, Tinoosh and Donati, Elisa and Tolu, Silvia and Galeazzi, Roberto and Christensen, Martin Ejsing and Holm, Sune and Ielmini, Daniele and Pryds, Nini},
	year = {2022},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.},
	language = {en},
	number = {7784},
	urldate = {2021-03-23},
	journal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	month = nov,
	year = {2019},
	note = {Number: 7784
Publisher: Nature Publishing Group},
	pages = {607--617},
}

@article{olshausen_sparse_2017,
	title = {Sparse codes from memristor grids},
	volume = {12},
	issn = {1748-3395},
	url = {http://www.nature.com/articles/nnano.2017.112},
	doi = {10.1038/nnano.2017.112},
	abstract = {The adjustable resistive state of memristors makes it possible to implement sparse coding algorithms naturally and efficiently.},
	language = {en},
	number = {8},
	urldate = {2021-11-25},
	journal = {Nature Nanotechnology},
	author = {Olshausen, Bruno A. and Rozell, Christopher J.},
	month = aug,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: News \& Views
Publisher: Nature Publishing Group
Subject\_term: Computer science;Electrical and electronic engineering;Network models
Subject\_term\_id: computer-science;electrical-and-electronic-engineering;network-models},
	keywords = {Computer science, Electrical and electronic engineering, Network models},
	pages = {722--723},
}

@article{carr_processing_1993,
	title = {Processing of {Temporal} {Information} in the {Brain}},
	volume = {16},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.ne.16.030193.001255},
	doi = {10.1146/annurev.ne.16.030193.001255},
	language = {en},
	number = {1},
	urldate = {2022-11-07},
	journal = {Annual Review of Neuroscience},
	author = {Carr, C E},
	month = mar,
	year = {1993},
	pages = {223--243},
}

@article{pena_cochlear_2001,
	title = {Cochlear and {Neural} {Delays} for {Coincidence} {Detection} in {Owls}},
	volume = {21},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6763915/},
	doi = {10.1523/JNEUROSCI.21-23-09455.2001},
	abstract = {The auditory system uses delay lines and coincidence detection to measure the interaural time difference (ITD). Both axons and the cochlea could provide such delays. The stereausis theory assumes that differences in wave propagation time along the basilar membrane can provide the necessary delays, if the coincidence detectors receive input from fibers innervating different loci on the left and right basilar membranes. If this hypothesis were true, the left and right inputs to coincidence detectors should differ in their frequency tuning. The owl's nucleus laminaris contains coincidence detector neurons that receive input from the left and right cochlear nuclei. Monaural frequency-tuning curves of nucleus laminaris neurons showed small interaural differences. In addition, their preferred ITDs were not correlated with the interaural frequency mismatches. Instead, the preferred ITD of the neuron agrees with that predicted from the distribution of axonal delays. Thus, there is no need to invoke mechanisms other than neural delays to explain the detection of ITDs by the barn owl's laminaris neurons.},
	number = {23},
	urldate = {2021-01-07},
	journal = {The Journal of Neuroscience},
	author = {Peña, José Luis and Viete, Svenja and Funabiki, Kazuo and Saberi, Kourosh and Konishi, Masakazu},
	month = dec,
	year = {2001},
	pmid = {11717379},
	pmcid = {PMC6763915},
	keywords = {biology, delay-learning},
	pages = {9455--9459},
}

@article{gerstner_neuronal_1996,
	title = {A neuronal learning rule for sub-millisecond temporal coding},
	volume = {383},
	copyright = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/383076a0},
	doi = {10.1038/383076a0},
	abstract = {A PARADOX that exists in auditory and electrosensory neural systems1,2 is that they encode behaviourally relevant signals in the range of a few microseconds with neurons that are at least one order of magnitude slower. The importance of temporal coding in neural information processing is not clear yet3–8. A central question is whether neuronal firing can be more precise than the time constants of the neuronal processes involved9. Here we address this problem using the auditory system of the barn owl as an example. We present a modelling study based on computer simulations of a neuron in the laminar nucleus. Three observations explain the paradox. First, spiking of an 'integrate-and-fire' neuron driven by excitatory postsynaptic potentials with a width at half-maximum height of 250 μs, has an accuracy of 25 μs if the presynaptic signals arrive coherently. Second, the necessary degree of coherence in the signal arrival times can be attained during ontogenetic development by virtue of an unsupervised hebbian learning rule. Learning selects connections with matching delays from a broad distribution of axons with random delays. Third, the learning rule also selects the correct delays from two independent groups of inputs, for example, from the left and right ear.},
	language = {en},
	number = {6595},
	urldate = {2021-01-07},
	journal = {Nature},
	author = {Gerstner, Wulfram and Kempter, Richard and van Hemmen, J. Leo and Wagner, Hermann},
	month = sep,
	year = {1996},
	note = {Number: 6595
Publisher: Nature Publishing Group},
	pages = {76--78},
}

@misc{hazan_memory_2022,
	title = {Memory via {Temporal} {Delays} in weightless {Spiking} {Neural} {Network}},
	url = {http://arxiv.org/abs/2202.07132},
	abstract = {A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Hazan, Hananel and Caby, Simon and Earl, Christopher and Siegelmann, Hava and Levin, Michael},
	month = feb,
	year = {2022},
	note = {arXiv:2202.07132 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Computation},
}

@article{rueckauer_conversion_2017,
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	doi = {10.3389/fnins.2017.00682},
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	urldate = {2022-10-25},
	journal = {Frontiers in Neuroscience},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	year = {2017},
}

@article{gautrais_rate_1998,
	title = {Rate coding versus temporal order coding: a theoretical approach},
	volume = {48},
	shorttitle = {Rate coding versus temporal order coding},
	doi = {10.1016/S0303-2647(98)00050-1},
	number = {1-3},
	journal = {Biosystems},
	author = {Gautrais, Jacques and Thorpe, Simon},
	year = {1998},
	note = {Publisher: Elsevier},
	pages = {57--65},
}

@article{delorme_spikenet_1999,
	title = {{SpikeNET}: {A} simulator for modeling large networks of integrate and fire neurons},
	volume = {26},
	shorttitle = {{SpikeNET}},
	doi = {10.1016/S0925-2312(99)00095-8},
	journal = {Neurocomputing},
	author = {Delorme, Arnaud and Gautrais, Jacques and Van Rullen, Rufin and Thorpe, Simon},
	year = {1999},
	note = {Publisher: Elsevier},
	pages = {989--996},
}

@article{serre_feedforward_2007,
	title = {A feedforward architecture accounts for rapid categorization},
	volume = {104},
	doi = {10.1073/pnas.0700622104},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Serre, T. and Oliva, A. and Poggio, T.},
	year = {2007},
	note = {tex.bdsk-url-2: https://doi.org/10.1073/pnas.0700622104
tex.date-added: 2022-05-05 19:08:15 +0200
tex.date-modified: 2022-05-05 19:08:15 +0200},
	pages = {6424--6429},
}

@inproceedings{state_training_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Training {Delays} in {Spiking} {Neural} {Networks}},
	isbn = {978-3-030-30487-4},
	doi = {10.1007/978-3-030-30487-4_54},
	abstract = {Spiking Neural Networks (SNNs) are a promising computational paradigm, both to understand biological information processing and for low-power, embedded chips. Although SNNs are known to encode information in the precise timing of spikes, conventional artificial learning algorithms do not take this into account directly. In this work, we implement the spike timing by training the synaptic delays in a single layer SNN. We use two different approaches: a classical gradient descent and a direct algebraic method that is based on a complex-valued encoding of the spikes. Both algorithms are equally able to correctly solve simple detection tasks. Our work provides new optimization methods for the data analysis of highly time-dependent data and training methods for neuromorphic chips.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2019: {Theoretical} {Neural} {Computation}},
	publisher = {Springer International Publishing},
	author = {State, Laura and Vilimelis Aceituno, Pau},
	editor = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
	year = {2019},
	pages = {713--717},
}

@book{datadien_right_2011,
	title = {The {Right} {Delay} - {Detecting} {Specific} {Spike} {Patterns} with {STDP} and {Axonal} {Conduction} {Delays}.},
	abstract = {Axonal conduction delays should not be ignored in simulations of spiking neural networks. Here it is shown that by using axonal
conduction delays, neurons can display sensitivity to a specific spatio-temporal spike pattern. By using delays that complement
the firing times in a pattern, spikes can arrive simultaneously at an output neuron, giving it a high chance of firing in
response to that pattern. An unsupervised learning mechanism called spike-timing-dependent plasticity then increases the weights
for connections used in the pattern, and decreases the others. This allows for an attunement of output neurons to specific
activity patterns, based on temporal aspects of axonal conductivity.},
	author = {Datadien, Arvind and Haselager, Pim and Sprinkhuizen-Kuyper, Ida},
	month = jan,
	year = {2011},
	note = {Pages: 99},
}

@article{kreuz_measuring_2007,
	title = {Measuring spike train synchrony},
	volume = {165},
	issn = {0165-0270},
	doi = {10.1016/j.jneumeth.2007.05.031},
	abstract = {Estimating the degree of synchrony or reliability between two or more spike trains is a frequent task in both experimental and computational neuroscience. In recent years, many different methods have been proposed that typically compare the timing of spikes on a certain time scale to be optimized by the analyst. Here, we propose the ISI-distance, a simple complementary approach that extracts information from the interspike intervals by evaluating the ratio of the instantaneous firing rates. The method is parameter free, time scale independent and easy to visualize as illustrated by an application to real neuronal spike trains obtained in vitro from rat slices. In a comparison with existing approaches on spike trains extracted from a simulated Hindemarsh-Rose network, the ISI-distance performs as well as the best time-scale-optimized measure based on spike timing.},
	language = {eng},
	number = {1},
	journal = {Journal of Neuroscience Methods},
	author = {Kreuz, Thomas and Haas, Julie S. and Morelli, Alice and Abarbanel, Henry D. I. and Politi, Antonio},
	month = sep,
	year = {2007},
	pmid = {17628690},
	keywords = {Action Potentials, Animals, Electrophysiology, Neurons, Rats},
	pages = {151--161},
}

@article{meister_concerted_1995,
	title = {Concerted {Signaling} by {Retinal} {Ganglion} {Cells}},
	volume = {270},
	url = {https://doi.org/dszfrn},
	doi = {10.1126/science.270.5239.1207},
	language = {en},
	number = {5239},
	journal = {Science},
	author = {Meister, Markus and Lagnado, Leon and Baylor, Denis A.},
	month = nov,
	year = {1995},
	pages = {1207--1210},
}

@article{suthers_motor_2002,
	title = {Motor control of birdsong.},
	volume = {12},
	issn = {0959-4388},
	doi = {10.1016/s0959-4388(02)00386-0},
	abstract = {One of the challenges when considering the motor control of birdsong is to understand how such a wide variety of temporally and spectrally diverse vocalizations are learned and produced. A better understanding of central neural processing, together with direct endoscopic observations and physiological studies of peripheral motor function during singing, has resulted in the formation of new theoretical models of song production. Recent work suggests that it may be more profitable to focus on the temporal relationship between control parameters than to attempt to directly correlate neural processing with details of the acoustic output.},
	number = {6},
	journal = {Current opinion in neurobiology},
	author = {Suthers, Roderick A and Margoliash, Daniel},
	year = {2002},
	pages = {684--90},
}

@article{berry_spike_2022,
	title = {Spike {Trains} of {Retinal} {Ganglion} {Cells} {Viewing} a {Repeated} {Natural} {Movie}},
	url = {https://dataspace.princeton.edu/handle/88435/dsp015425kd84r},
	doi = {10.34770/V0V4-3H52},
	abstract = {This archive contains spike trains simultaneously recorded from ganglion cells in the tiger salamander retina with a multi-electrode array while viewing a repeated natural movie clip.  These data have been analyzed in previous papers, notably Puchalla et al. Neuron 2005 and Schneidman et al. Nature 2006.},
	language = {en\_US},
	urldate = {2022-10-04},
	author = {Berry, Michael J.},
	month = mar,
	year = {2022},
}

@article{caporale_spike_2008,
	title = {Spike {Timing}–{Dependent} {Plasticity}: {A} {Hebbian} {Learning} {Rule}},
	volume = {31},
	url = {https://doi.org/fqxrgj},
	doi = {10.1146/annurev.neuro.31.060407.125639},
	abstract = {{\textless}jats:p{\textgreater} Spike timing–dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Caporale, Natalia and Dan, Yang},
	year = {2008},
	pages = {25--46},
}

@article{guise_bayesian_2014,
	title = {A {Bayesian} {Model} of {Polychronicity}},
	volume = {26},
	url = {https://doi.org/f6chbq},
	doi = {10.1162/neco_a_00620},
	abstract = {{\textless}jats:p{\textgreater} A significant feature of spiking neural networks with varying connection delays, such as those in the brain, is the existence of strongly connected groups of neurons known as polychronous neural groups (PNGs). Polychronous groups are found in large numbers in these networks and are proposed by Izhikevich ( 2006a ) to provide a neural basis for representation and memory. When exposed to a familiar stimulus, spiking neural networks produce consistencies in the spiking output data that are the hallmarks of PNG activation. Previous methods for studying the PNG activation response to stimuli have been limited by the template-based methods used to identify PNG activation. In this letter, we outline a new method that overcomes these difficulties by establishing for the first time a probabilistic interpretation of PNG activation. We then demonstrate the use of this method by investigating the claim that PNGs might provide the foundation of a representational system. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {9},
	journal = {Neural Computation},
	author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
	month = sep,
	year = {2014},
	pages = {2052--2073},
}

@article{thorpe_seeking_2001,
	title = {Seeking {Categories} in the {Brain}},
	volume = {291},
	url = {https://doi.org/bzn42k},
	doi = {10.1126/science.1058249},
	language = {en},
	number = {5502},
	journal = {Science},
	author = {Thorpe, Simon J. and Fabre-Thorpe, Michèle},
	month = jan,
	year = {2001},
	pages = {260--263},
}

@article{russo_cell_2017,
	title = {Cell assemblies at multiple time scales with arbitrary lag constellations},
	volume = {6},
	url = {https://doi.org/f9kxd8},
	doi = {10.7554/elife.19428},
	abstract = {{\textless}jats:p{\textgreater}Hebb's idea of a cell assembly as the fundamental unit of neural information processing has dominated neuroscience like no other theoretical concept within the past 60 years. A range of different physiological phenomena, from precisely synchronized spiking to broadly simultaneous rate increases, has been subsumed under this term. Yet progress in this area is hampered by the lack of statistical tools that would enable to extract assemblies with arbitrary constellations of time lags, and at multiple temporal scales, partly due to the severe computational burden. Here we present such a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. Applying this methodology to multiple single unit recordings from various cortical areas, we find that there is no universal cortical coding scheme, but that assembly structure and precision significantly depends on the brain area recorded and ongoing task demands.{\textless}/jats:p{\textgreater}},
	language = {en},
	journal = {eLife},
	author = {Russo, Eleonora and Durstewitz, Daniel},
	month = jan,
	year = {2017},
}

@techreport{rasetto_challenges_2022,
	title = {The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event} {Processors}: {How} {Memristors} {Dynamic} {Properties} {Could} {Revolutionize} {Machine} {Learning}},
	url = {https://arxiv.org/abs/2201.12673},
	abstract = {Neuromorphic engineering has led to the necessary process of rethinking of how we process and integrate information, analyze data, and use the resulting insights to improve computation and avoid the current high power and latency of Artificial Intelligence (AI) hardware. Current neuromorphic processors are, however, limited by digital technologies, which cannot reproduce the abilities of biological neural computation in terms of power, latency and area cost. In this paper, we show that the combined use of the dynamic properties of memristors to implement a model of synaptic integration and the determination of the correct level of abstraction of biological neural networks has the potential to open a new range of capabilities for neuromorphic processors. We test this approach using a novel three-terminal LixWO3 electrochemical memristor, by deriving its conductance model and using it to emulate synaptic temporal kernel computation in the context of a pattern recognition task. We show that these devices allow for robust results with no loss in precision while opening the path for an energy efficient approach to build novel bio-inspired processing units in silicon.},
	number = {2201.12673},
	institution = {arXiv},
	author = {Rasetto, Marco and Wan, Qingzhou and Akolkar, Himanshu and Shi, Bertram and Xiong, Feng and Benosman, Ryad},
	year = {2022},
}

@article{moser_stability_2014,
	title = {On {Stability} of {Distance} {Measures} for {Event} {Sequences} {Induced} by {Level}-{Crossing} {Sampling}},
	volume = {62},
	url = {https://doi.org/gnpb7w},
	doi = {10.1109/tsp.2014.2305642},
	number = {8},
	journal = {IEEE Transactions on Signal Processing},
	author = {Moser, Bernhard A. and Natschlager, Thomas},
	year = {2014},
	pages = {1987--1999},
}

@article{brette_computing_2012,
	title = {Computing with {Neural} {Synchrony}},
	volume = {8},
	url = {https://doi.org/f32tvz},
	doi = {10.1371/journal.pcbi.1002561},
	language = {en},
	number = {6},
	journal = {PLoS Computational Biology},
	author = {Brette, Romain},
	editor = {Sporns, Olaf},
	year = {2012},
	pages = {e1002561},
}

@article{schrader_detecting_2008,
	title = {Detecting {Synfire} {Chain} {Activity} {Using} {Massively} {Parallel} {Spike} {Train} {Recording}},
	volume = {100},
	url = {https://doi.org/cvb22p},
	doi = {10.1152/jn.01245.2007},
	abstract = {{\textless}jats:p{\textgreater} The synfire chain model has been proposed as the substrate that underlies computational processes in the brain and has received extensive theoretical study. In this model cortical tissue is composed of a superposition of feedforward subnetworks (chains) each capable of transmitting packets of synchronized spikes with high reliability. Computations are then carried out by interactions of these chains. Experimental evidence for synfire chains has so far been limited to inference from detection of a few repeating spatiotemporal neuronal firing patterns in multiple single-unit recordings. Demonstration that such patterns actually come from synfire activity would require finding a meta organization among many detected patterns, as yet an untried approach. In contrast we present here a new method that directly visualizes the repetitive occurrence of synfire activity even in very large data sets of multiple single-unit recordings. We achieve reliability and sensitivity by appropriately averaging over neuron space (identities) and time. We test the method with data from a large-scale balanced recurrent network simulation containing 50 randomly activated synfire chains. The sensitivity is high enough to detect synfire chain activity in simultaneous single-unit recordings of 100 to 200 neurons from such data, enabling application to experimental data in the near future. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {4},
	journal = {Journal of Neurophysiology},
	author = {Schrader, Sven and Grün, Sonja and Diesmann, Markus and Gerstein, George L.},
	month = oct,
	year = {2008},
	pages = {2165--2176},
}

@article{pillow_spatio-temporal_2008,
	title = {Spatio-temporal correlations and visual signalling in a complete neuronal population},
	volume = {454},
	url = {https://doi.org/dzvdm3},
	doi = {10.1038/nature07140},
	language = {en},
	number = {7207},
	journal = {Nature},
	author = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
	year = {2008},
	pages = {995--999},
}

@techreport{yin_sata_2022,
	title = {{SATA}: {Sparsity}-{Aware} {Training} {Accelerator} for {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2204.05422},
	abstract = {Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. Recently, SNNs with backpropagation through time (BPTT) have achieved a higher accuracy result on image recognition tasks compared to other SNN training algorithms. Despite the success on the algorithm perspective, prior works neglect the evaluation of the hardware energy overheads of BPTT, due to the lack of a hardware evaluation platform for SNN training algorithm design. Moreover, although SNNs have been long seen as an energy-efficient counterpart of ANNs, a quantitative comparison between the training cost of SNNs and ANNs is missing. To address the above-mentioned issues, in this work, we introduce SATA (Sparsity-Aware Training Accelerator), a BPTT-based training accelerator for SNNs. The proposed SATA provides a simple and re-configurable accelerator architecture for the general-purpose hardware evaluation platform, which makes it easier to analyze the training energy for SNN training algorithms. Based on SATA, we show quantitative analyses on the energy efficiency of SNN training and make a comparison between the training cost of SNNs and ANNs. The results show that SNNs consume \$1.27{\textbackslash}times\$ more total energy with considering sparsity (spikes, gradient of firing function, and gradient of membrane potential) when compared to ANNs. We find that such high training energy cost is from time-repetitive convolution operations and data movements during backpropagation. Moreover, to guide the future SNN training algorithm design, we provide several observations on energy efficiency with respect to different SNN-specific training parameters.},
	number = {2204.05422},
	institution = {arXiv},
	author = {Yin, Ruokai and Moitra, Abhishek and Bhattacharjee, Abhiroop and Kim, Youngeun and Panda, Priyadarshini},
	year = {2022},
}

@techreport{bellec_fitting_2021,
	title = {Fitting summary statistics of neural data with a differentiable spiking network simulator},
	url = {https://arxiv.org/abs/2106.10064},
	abstract = {Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.},
	number = {2106.10064},
	institution = {arXiv},
	author = {Bellec, Guillaume and Wang, Shuqi and Modirshanechi, Alireza and Brea, Johanni and Gerstner, Wulfram},
	month = nov,
	year = {2021},
}

@article{wild_descending_1993,
	title = {Descending projections of the songbird nucleus robustus archistriatalis.},
	volume = {338},
	issn = {0021-9967},
	doi = {10.1002/cne.903380207},
	abstract = {The descending, efferent projections of nucleus robustus archistriatalis were investigated in male zebra finches and greenfinches with injections of either biotinylated dextran amine or cholera toxin B-chain conjugated to horseradish peroxidase. The results show that in addition to the well-known projections to the tracheosyringeal motor nucleus and the dorsomedial nucleus of the intercollicular complex, there are other projections of comparable density to the ipsilateral nucleus ambiguus and nucleus retroambigualis. Within nucleus ambiguus, robustus axons terminate in close proximity to laryngeal motoneurons which were retrogradely labelled in the same bird by injections of cholera B-chain into the laryngeal muscles; and within nucleus retroambigualis robustus axons terminate in relation to bulbospinal neurons previously shown to project to regions of spinal cord containing motoneurons innervating abdominal expiratory muscles (J.M. Wild, Brain Res. 606:119-124, 1993). These projections of nucleus robustus thus seem well placed to coordinate syringeal, laryngeal, and expiratory muscle activity during vocalization. Other relatively sparse, but distinct, projections of nucleus robustus were found to nucleus dorsolateralis anterior thalami, pars medialis, to a narrow region between the superior olivary nucleus and the spinal lemniscus, and to the rostral ventrolateral medulla. Neurons in these last two locations were retrogradely labelled bilaterally following injections of cholera B-chain into nucleus retroambigualis of one side. Together with sparse contralateral projections of nucleus robustus to all brainstem targets receiving ipsilateral projections, potential pathways are thus identified by which the respiratory-vocal activity controlled by one side of the lower medulla can be influenced by the nucleus robustus of either side, thereby possibly bringing about bilateral coordination of respiratory-vocal output.},
	number = {2},
	journal = {The Journal of comparative neurology},
	author = {Wild, J M},
	year = {1993},
	pages = {225--41},
}

@article{wehr_odour_1996,
	title = {Odour encoding by temporal sequences of firing in oscillating neural assemblies},
	volume = {384},
	url = {https://doi.org/b3t32c},
	doi = {10.1038/384162a0},
	language = {en},
	number = {6605},
	journal = {Nature},
	author = {Wehr, Michael and Laurent, Gilles},
	month = nov,
	year = {1996},
	pages = {162--166},
}

@article{branco_dendritic_2010,
	title = {Dendritic {Discrimination} of {Temporal} {Input} {Sequences} in {Cortical} {Neurons}},
	volume = {329},
	url = {https://doi.org/dqx4n4},
	doi = {10.1126/science.1189664},
	language = {en},
	number = {5999},
	journal = {Science},
	author = {Branco, Tiago and Clark, Beverley A. and Häusser, Michael},
	month = sep,
	year = {2010},
	pages = {1671--1675},
}

@article{yu_temporal_1996,
	title = {Temporal hierarchical control of singing in birds.},
	volume = {273},
	issn = {0036-8075},
	doi = {10.1126/science.273.5283.1871},
	abstract = {Songs of birds comprise hierarchical sets of vocal gestures. In zebra finches, songs include notes and syllables (groups of notes) delivered in fixed sequences. During singing, premotor neurons in the forebrain nucleus HVc exhibited reliable changes in activity rates whose patterns were uniquely associated with syllable identity. Neurons in the forebrain nucleus robustus archistriatalis, which receives input from the HVc, exhibited precisely timed and structured bursts of activity that were uniquely associated with note identity. Hence, units of vocal behavior are represented hierarchically in the avian forebrain. The representation of temporal sequences at each level of the hierarchy may be established by means of a decoding process involving interactions of higher level input with intrinsic local circuitry. Behavior is apparently represented by precise temporal patterning of spike trains at lower levels of the hierarchy.},
	number = {5283},
	journal = {Science (New York, N.Y.)},
	author = {Yu, A C and Margoliash, D},
	month = sep,
	year = {1996},
	pages = {1871--5},
}

@article{harris_organization_2003,
	title = {Organization of cell assemblies in the hippocampus},
	volume = {424},
	url = {https://doi.org/bm3vgb},
	doi = {10.1038/nature01834},
	language = {en},
	number = {6948},
	journal = {Nature},
	author = {Harris, Kenneth D. and Csicsvari, Jozsef and Hirase, Hajime and Dragoi, George and Buzsáki, György},
	year = {2003},
	pages = {552--556},
}

@article{stringer_high-precision_2021,
	title = {High-precision coding in visual cortex},
	volume = {184},
	url = {https://doi.org/gjqbjd},
	doi = {10.1016/j.cell.2021.03.042},
	language = {en},
	number = {10},
	journal = {Cell},
	author = {Stringer, Carsen and Michaelos, Michalis and Tsyboulski, Dmitri and Lindo, Sarah E. and Pachitariu, Marius},
	year = {2021},
	pages = {2767--2778.e15},
}

@techreport{williams_point_2020,
	title = {Point process models for sequence detection in high-dimensional neural spike trains},
	url = {https://arxiv.org/abs/2010.04875},
	abstract = {Sparse sequences of neural spikes are posited to underlie aspects of working memory, motor production, and learning. Discovering these sequences in an unsupervised manner is a longstanding problem in statistical neuroscience. Promising recent work utilized a convolutive nonnegative matrix factorization model to tackle this challenge. However, this model requires spike times to be discretized, utilizes a sub-optimal least-squares criterion, and does not provide uncertainty estimates for model predictions or estimated parameters. We address each of these shortcomings by developing a point process model that characterizes fine-scale sequences at the level of individual spikes and represents sequence occurrences as a small number of marked events in continuous time. This ultra-sparse representation of sequence events opens new possibilities for spike train modeling. For example, we introduce learnable time warping parameters to model sequences of varying duration, which have been experimentally observed in neural circuits. We demonstrate these advantages on experimental recordings from songbird higher vocal center and rodent hippocampus.},
	number = {2010.04875},
	institution = {arXiv},
	author = {Williams, Alex H. and Degleris, Anthony and Wang, Yixin and Linderman, Scott W.},
	month = oct,
	year = {2020},
}

@article{thorpe_speed_1996,
	title = {Speed of processing in the human visual system},
	volume = {381},
	url = {https://doi.org/c4v35x},
	doi = {10.1038/381520a0},
	language = {en},
	number = {6582},
	journal = {Nature},
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	year = {1996},
	pages = {520--522},
}

@article{grammont_spike_2003,
	title = {Spike synchronization and firing rate in a population of motor cortical neurons in relation to movement direction and reaction time},
	volume = {88},
	url = {https://doi.org/ctvhsb},
	doi = {10.1007/s00422-002-0385-3},
	number = {5},
	journal = {Biological Cybernetics},
	author = {Grammont, F. and Riehle, A.},
	year = {2003},
	pages = {360--373},
}

@techreport{panahi_generative_2021,
	title = {Generative {Models} of {Brain} {Dynamics} -- {A} review},
	url = {https://arxiv.org/abs/2112.12147},
	abstract = {The principled design and discovery of biologically- and physically-informed models of neuronal dynamics has been advancing since the mid-twentieth century. Recent developments in artificial intelligence (AI) have accelerated this progress. This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, AI, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.},
	number = {2112.12147},
	institution = {arXiv},
	author = {Panahi, Mahta Ramezanian and Abrevaya, Germán and Gagnon-Audet, Jean-Christophe and Voleti, Vikram and Rish, Irina and Dumas, Guillaume},
	year = {2021},
}

@article{perrinet_coherence_2002,
	title = {Coherence detection in a spiking neuron via {Hebbian} learning},
	volume = {44-46},
	url = {https://doi.org/fsk9mk},
	doi = {10.1016/s0925-2312(02)00374-0},
	language = {en},
	journal = {Neurocomputing},
	author = {Perrinet, L. and Samuelides, M.},
	year = {2002},
	pages = {133--139},
}

@article{di_mauro_alfio_sne_2022,
	title = {{SNE}: an {Energy}-{Proportional} {Digital} {Accelerator} for {Sparse} {Event}-{Based} {Convolutions}},
	url = {https://doi.org/gp3m5v},
	doi = {10.3929/ethz-b-000543342},
	abstract = {Event-based sensors are drawing increasing attention due to their high temporal resolution, low power consumption, and low bandwidth. To efficiently extract semantically meaningful information from sparse data streams produced by such sensors, we present a 4.5TOP/s/W digital accelerator capable of performing 4-bits-quantized event-based convolutional neural networks (eCNN). Compared to standard convolutional engines, our accelerator performs a number of operations proportional to the number of events contained into the input data stream, ultimately achieving a high energy-to-information processing proportionality. On the IBM-DVS-Gesture dataset, we report 80uJ/inf to 261uJ/inf, respectively, when the input activity is 1.2\% and 4.9\%. Our accelerator consumes 0.221pJ/SOP, to the best of our knowledge it is the lowest energy/OP reported on a digital neuromorphic engine.},
	language = {en},
	journal = {ETH Zurich},
	author = {{Di Mauro, Alfio} and {Prasad, Arpan Suravi} and {Huang, Zhikai} and {Spallanzani, Matteo} and {Conti, Francesco} and {Benini, Luca}},
	year = {2022},
}

@article{torre_synchronous_2016,
	title = {Synchronous {Spike} {Patterns} in {Macaque} {Motor} {Cortex} during an {Instructed}-{Delay} {Reach}-to-{Grasp} {Task}},
	volume = {36},
	url = {https://doi.org/f82b6j},
	doi = {10.1523/jneurosci.4375-15.2016},
	language = {en},
	number = {32},
	journal = {Journal of Neuroscience},
	author = {Torre, E. and Quaglio, P. and Denker, M. and Brochier, T. and Riehle, A. and Grun, S.},
	year = {2016},
	pages = {8329--8340},
}

@article{singer_visual_1995,
	title = {Visual {Feature} {Integration} and the {Temporal} {Correlation} {Hypothesis}},
	volume = {18},
	url = {https://doi.org/cgx8jp},
	doi = {10.1146/annurev.ne.18.030195.003011},
	language = {en},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Singer, W and Gray, C M},
	month = mar,
	year = {1995},
	pages = {555--586},
}

@article{miller_visual_2014,
	title = {Visual stimuli recruit intrinsically generated cortical ensembles},
	volume = {111},
	url = {https://doi.org/f6htkt},
	doi = {10.1073/pnas.1406077111},
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}This study demonstrates that neuronal groups or ensembles, rather than individual neurons, are emergent functional units of cortical activity. We show that in the presence and absence of visual stimulation, cortical activity is dominated by coactive groups of neurons forming ensembles. These ensembles are flexible and cannot be accounted for by the independent firing properties of neurons in isolation. Intrinsically generated ensembles and stimulus-evoked ensembles are similar, with one main difference: Whereas intrinsic ensembles recur at random time intervals, visually evoked ensembles are time-locked to stimuli. We propose that visual stimuli recruit endogenously generated ensembles to represent visual attributes.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {38},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Miller, Jae-eun Kang and Ayzenshtat, Inbal and Carrillo-Reid, Luis and Yuste, Rafael},
	month = sep,
	year = {2014},
}

@article{stringer_spontaneous_2019,
	title = {Spontaneous behaviors drive multidimensional, brainwide activity},
	volume = {364},
	url = {https://doi.org/gfz6mh},
	doi = {10.1126/science.aav7893},
	language = {en},
	number = {6437},
	journal = {Science},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
	year = {2019},
}

@article{van_kempen_top-down_2021,
	title = {Top-down coordination of local cortical state during selective attention},
	volume = {109},
	url = {https://doi.org/ghvj3k},
	doi = {10.1016/j.neuron.2020.12.013},
	language = {en},
	number = {5},
	journal = {Neuron},
	author = {van Kempen, Jochem and Gieselmann, Marc A. and Boyd, Michael and Steinmetz, Nicholas A. and Moore, Tirin and Engel, Tatiana A. and Thiele, Alexander},
	month = mar,
	year = {2021},
	pages = {894--904.e8},
}

@article{zhang_supervised_2020,
	title = {Supervised learning in spiking neural networks with synaptic delay-weight plasticity},
	volume = {409},
	url = {https://doi.org/ghsm45},
	doi = {10.1016/j.neucom.2020.03.079},
	language = {en},
	journal = {Neurocomputing},
	author = {Zhang, Malu and Wu, Jibin and Belatreche, Ammar and Pan, Zihan and Xie, Xiurui and Chua, Yansong and Li, Guoqi and Qu, Hong and Li, Haizhou},
	month = oct,
	year = {2020},
	pages = {103--118},
}

@article{aviel_embedding_2003,
	title = {On {Embedding} {Synfire} {Chains} in a {Balanced} {Network}},
	volume = {15},
	url = {https://doi.org/fgj3wf},
	doi = {10.1162/089976603321780290},
	abstract = {{\textless}jats:p{\textgreater} We investigate the formation of synfire waves in a balanced network of integrate-and-fire neurons. The synaptic connectivity of this network embodies synfire chains within a sparse random connectivity. This network can exhibit global oscillations but can also operate in an asynchronous activity mode. We analyze the correlations of two neurons in a pool as convenient indicators for the state of the network. We find, using different models, that these indicators depend on a scaling variable. {\textless}/jats:p{\textgreater}{\textless}jats:p{\textgreater} Beyond a critical point, strong correlations and large network oscillations are obtained. We looked for the conditions under which a synfire wave could be propagated on top of an otherwise asynchronous state of the network. This condition was found to be highly restrictive, requiring a large number of neurons for its implementation in our network. The results are based on analytic derivations and simulations. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {6},
	journal = {Neural Computation},
	author = {Aviel, Y. and Mehring, C. and Abeles, M. and Horn, D.},
	year = {2003},
	pages = {1321--1340},
}

@article{weyl_ueber_1916,
	title = {Ueber die {Gleichverteilung} von {Zahlen} mod. {Eins}},
	volume = {77},
	url = {https://doi.org/crprvc},
	doi = {10.1007/bf01475864},
	language = {de},
	number = {3},
	journal = {Mathematische Annalen},
	author = {Weyl, Hermann},
	month = sep,
	year = {1916},
	pages = {313--352},
}

@article{mainen_reliability_1995,
	title = {Reliability of {Spike} {Timing} in {Neocortical} {Neurons}},
	volume = {268},
	url = {https://doi.org/b2mms6},
	doi = {10.1126/science.7770778},
	language = {en},
	number = {5216},
	journal = {Science},
	author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
	year = {1995},
	pages = {1503--1506},
}

@article{tolle_fourth_2011,
	title = {The {Fourth} {Paradigm}: {Data}-{Intensive} {Scientific} {Discovery} [{Point} of {View}]},
	volume = {99},
	url = {https://doi.org/dfggjc},
	doi = {10.1109/jproc.2011.2155130},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Tolle, Kristin M. and Tansley, D. Stewart W. and Hey, Anthony J. G.},
	year = {2011},
	pages = {1334--1337},
}

@article{softky_highly_1993,
	title = {The highly irregular firing of cortical cells is inconsistent with temporal integration of random {EPSPs}},
	volume = {13},
	url = {https://doi.org/ggzph6},
	doi = {10.1523/jneurosci.13-01-00334.1993},
	language = {en},
	number = {1},
	journal = {The Journal of Neuroscience},
	author = {Softky, WR and Koch, C},
	month = jan,
	year = {1993},
	pages = {334--350},
}

@article{gerstner_time_1995,
	title = {Time structure of the activity in neural network models},
	volume = {51},
	url = {https://doi.org/cwcn9d},
	doi = {10.1103/physreve.51.738},
	language = {en},
	number = {1},
	journal = {Physical Review E},
	author = {Gerstner, Wulfram},
	month = jan,
	year = {1995},
	pages = {738--758},
}

@article{pipa_neuroxidence_2008,
	title = {{NeuroXidence}: reliable and efficient analysis of an excess or deficiency of joint-spike events},
	volume = {25},
	url = {https://doi.org/ddh5js},
	doi = {10.1007/s10827-007-0065-3},
	language = {en},
	number = {1},
	journal = {Journal of Computational Neuroscience},
	author = {Pipa, Gordon and Wheeler, Diek W. and Singer, Wolf and Nikolić, Danko},
	month = jan,
	year = {2008},
	pages = {64--88},
}

@article{tavanaei_representation_2018,
	title = {Representation learning using event-based {STDP}},
	volume = {105},
	url = {https://doi.org/gd6tgv},
	doi = {10.1016/j.neunet.2018.05.018},
	language = {en},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Masquelier, Timothée and Maida, Anthony},
	month = sep,
	year = {2018},
	pages = {294--303},
}

@article{stella_comparing_2022,
	title = {Comparing {Surrogates} to {Evaluate} {Precisely} {Timed} {Higher}-{Order} {Spike} {Correlations}},
	volume = {9},
	url = {https://doi.org/gqjvht},
	doi = {10.1523/eneuro.0505-21.2022},
	language = {en},
	number = {3},
	journal = {eneuro},
	author = {Stella, Alessandra and Bouss, Peter and Palm, Günther and Grün, Sonja},
	year = {2022},
	pages = {ENEURO.0505--21.2022},
}

@article{torre_asset_2016,
	title = {{ASSET}: {Analysis} of {Sequences} of {Synchronous} {Events} in {Massively} {Parallel} {Spike} {Trains}},
	volume = {12},
	url = {https://doi.org/gnpx4q},
	doi = {10.1371/journal.pcbi.1004939},
	language = {en},
	number = {7},
	journal = {PLOS Computational Biology},
	author = {Torre, Emiliano and Canova, Carlos and Denker, Michael and Gerstein, George and Helias, Moritz and Grün, Sonja},
	editor = {Sporns, Olaf},
	year = {2016},
	pages = {e1004939},
}

@article{torre_statistical_2013,
	title = {Statistical evaluation of synchronous spike patterns extracted by frequent item set mining},
	volume = {7},
	url = {https://doi.org/gpshgq},
	doi = {10.3389/fncom.2013.00132},
	journal = {Frontiers in Computational Neuroscience},
	author = {Torre, Emiliano and Picado-Muiño, David and Denker, Michael and Borgelt, Christian and Grün, Sonja},
	year = {2013},
}

@article{stringer_high-dimensional_2019,
	title = {High-dimensional geometry of population responses in visual cortex},
	volume = {571},
	url = {https://doi.org/gf4cfj},
	doi = {10.1038/s41586-019-1346-5},
	language = {en},
	number = {7765},
	journal = {Nature},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	year = {2019},
	pages = {361--365},
}

@article{levakova_review_2015,
	title = {A review of the methods for neuronal response latency estimation},
	volume = {136},
	url = {https://doi.org/gpshjz},
	doi = {10.1016/j.biosystems.2015.04.008},
	language = {en},
	journal = {Biosystems},
	author = {Levakova, Marie and Tamborrino, Massimiliano and Ditlevsen, Susanne and Lansky, Petr},
	month = oct,
	year = {2015},
	pages = {23--34},
}

@article{safaie_turning_2020,
	title = {Turning the body into a clock: {Accurate} timing is facilitated by simple stereotyped interactions with the environment},
	volume = {117},
	url = {https://doi.org/gnqsmh},
	doi = {10.1073/pnas.1921226117},
	abstract = {{\textless}jats:p{\textgreater}How animals adapt their behavior according to regular time intervals between events is not well understood, especially when intervals last several seconds. One possibility is that animals use disembodied internal neuronal representations of time to decide when to initiate a given action at the end of an interval. However, animals rarely remain immobile during time intervals but tend to perform stereotyped behaviors, raising the possibility that motor routines improve timing accuracy. To test this possibility, we used a task in which rats, freely moving on a motorized treadmill, could obtain a reward if they approached it after a fixed interval. Most animals took advantage of the treadmill length and its moving direction to develop, by trial-and-error, the same motor routine whose execution resulted in the precise timing of their reward approaches. Noticeably, when proficient animals did not follow this routine, their temporal accuracy decreased. Then, naïve animals were trained in modified versions of the task designed to prevent the development of this routine. Compared to rats trained in the first protocol, these animals didn’t reach a comparable level of timing accuracy. Altogether, our results indicate that timing accuracy in rats is improved when the environment affords cues that animals can incorporate into motor routines.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Safaie, Mostafa and Jurado-Parras, Maria-Teresa and Sarno, Stefania and Louis, Jordane and Karoutchi, Corane and Petit, Ludovic F. and Pasquet, Matthieu O. and Eloy, Christophe and Robbe, David},
	year = {2020},
	pages = {13084--13093},
}

@article{schneidman_weak_2006,
	title = {Weak pairwise correlations imply strongly correlated network states in a neural population},
	volume = {440},
	url = {https://doi.org/b9ph32},
	doi = {10.1038/nature04701},
	language = {en},
	number = {7087},
	journal = {Nature},
	author = {Schneidman, Elad and Berry, Michael J. and Segev, Ronen and Bialek, William},
	year = {2006},
	pages = {1007--1012},
}

@article{stella_3d-spade_2019,
	title = {3d-{SPADE}: {Significance} evaluation of spatio-temporal patterns of various temporal extents},
	volume = {185},
	url = {https://doi.org/gpshj2},
	doi = {10.1016/j.biosystems.2019.104022},
	language = {en},
	journal = {Biosystems},
	author = {Stella, Alessandra and Quaglio, Pietro and Torre, Emiliano and Grün, Sonja},
	month = nov,
	year = {2019},
	pages = {104022},
}

@article{roelfsema_visuomotor_1997,
	title = {Visuomotor integration is associated with zero time-lag synchronization among cortical areas},
	volume = {385},
	url = {https://doi.org/dp8q5v},
	doi = {10.1038/385157a0},
	language = {en},
	number = {6612},
	journal = {Nature},
	author = {Roelfsema, Pieter R. and Engel, Andreas K. and König, Peter and Singer, Wolf},
	month = jan,
	year = {1997},
	pages = {157--161},
}

@article{pfeil_six_2013,
	title = {Six {Networks} on a {Universal} {Neuromorphic} {Computing} {Substrate}},
	volume = {7},
	url = {https://doi.org/gh4jg3},
	doi = {10.3389/fnins.2013.00011},
	journal = {Frontiers in Neuroscience},
	author = {Pfeil, Thomas and Grübl, Andreas and Jeltsch, Sebastian and Müller, Eric and Müller, Paul and Petrovici, Mihai A. and Schmuker, Michael and Brüderle, Daniel and Schemmel, Johannes and Meier, Karlheinz},
	year = {2013},
}

@article{chase_first-spike_2007,
	title = {First-spike latency information in single neurons increases when referenced to population onset},
	volume = {104},
	url = {https://doi.org/cm3b98},
	doi = {10.1073/pnas.0610368104},
	abstract = {{\textless}jats:p{\textgreater}It is well known that many stimulus parameters, such as sound location in the auditory system or contrast in the visual system, can modulate the timing of the first spike in sensory neurons. Could first-spike latency be a candidate neural code? Most studies measuring first-spike latency information assume that the brain has an independent reference for stimulus onset from which to extract latency. This assumption creates an obvious confound that casts doubt on the feasibility of first-spike latency codes. If latency is measured relative to an internal reference of stimulus onset calculated from the responses of the neural population, the information conveyed by the latency of single neurons might decrease because of correlated changes in latency across the population. Here we assess the effects of a realistic model of stimulus onset detection on the first-spike latency information conveyed by single neurons in the auditory system. Contrary to expectation, we find that on average, the information contained in single neurons does not decrease; in fact, the majority of neurons show a slight increase in the information conveyed by latency referenced to a population onset. Our results show that first-spike latency codes are a feasible mechanism for information transfer even when biologically plausible estimates of stimulus onset are taken into account.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chase, Steven M. and Young, Eric D.},
	month = mar,
	year = {2007},
	pages = {5175--5180},
}

@incollection{nowak_timing_1997,
	title = {The {Timing} of {Information} {Transfer} in the {Visual} {System}},
	url = {https://doi.org/gpb33s},
	booktitle = {Extrastriate {Cortex} in {Primates}},
	publisher = {Springer US},
	author = {Nowak, Lionel G. and Bullier, Jean},
	year = {1997},
	pages = {205--241},
}

@article{kerr_delay_2013,
	title = {Delay {Selection} by {Spike}-{Timing}-{Dependent} {Plasticity} in {Recurrent} {Networks} of {Spiking} {Neurons} {Receiving} {Oscillatory} {Inputs}},
	volume = {9},
	url = {https://doi.org/f4rm5j},
	doi = {10.1371/journal.pcbi.1002897},
	language = {en},
	number = {2},
	journal = {PLoS Computational Biology},
	author = {Kerr, Robert R. and Burkitt, Anthony N. and Thomas, Doreen A. and Gilson, Matthieu and Grayden, David B.},
	editor = {Morrison, Abigail},
	year = {2013},
	pages = {e1002897},
}

@article{quaglio_methods_2018,
	title = {Methods for identification of spike patterns in massively parallel spike trains},
	volume = {112},
	url = {https://doi.org/gdgckg},
	doi = {10.1007/s00422-018-0755-0},
	language = {en},
	number = {1-2},
	journal = {Biological Cybernetics},
	author = {Quaglio, Pietro and Rostami, Vahid and Torre, Emiliano and Grün, Sonja},
	year = {2018},
	pages = {57--80},
}

@article{perrinet_networks_2001,
	title = {Networks of integrate-and-fire neuron using rank order coding {A}: {How} to implement spike time dependent {Hebbian} plasticity},
	volume = {38-40},
	url = {https://doi.org/d5p6b2},
	doi = {10.1016/s0925-2312(01)00460-x},
	language = {en},
	journal = {Neurocomputing},
	author = {Perrinet, L. and Delorme, A. and Samuelides, M. and Thorpe, S.J.},
	year = {2001},
	pages = {817--822},
}

@article{johansson_first_2004,
	title = {First spikes in ensembles of human tactile afferents code complex spatial fingertip events},
	volume = {7},
	url = {https://doi.org/dqstpm},
	doi = {10.1038/nn1177},
	language = {en},
	number = {2},
	journal = {Nature Neuroscience},
	author = {Johansson, Roland S and Birznieks, Ingvars},
	month = jan,
	year = {2004},
	pages = {170--177},
}

@article{rucci_temporal_2018,
	title = {Temporal {Coding} of {Visual} {Space}},
	volume = {22},
	url = {https://doi.org/gfcskd},
	doi = {10.1016/j.tics.2018.07.009},
	language = {en},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Rucci, Michele and Ahissar, Ehud and Burr, David},
	month = oct,
	year = {2018},
	pages = {883--895},
}

@article{grun_unitary_2002,
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {I}. {Detection} and {Significance}},
	volume = {14},
	url = {https://doi.org/c63kkn},
	doi = {10.1162/089976602753284455},
	abstract = {{\textless}jats:p{\textgreater} It has been proposed that cortical neurons organize dynamically into functional groups (cell assemblies) by the temporal structure of their joint spiking activity. Here, we describe a novel method to detect conspicuous patterns of coincident joint spike activity among simultaneously recorded single neurons. The statistical significance of these unitary events of coincident joint spike activity is evaluated by the joint-surprise. The method is tested and calibrated on the basis of simulated, stationary spike trains of independently firing neurons, into which coincident joint spike events were inserted under controlled conditions. The sensitivity and specificity of the method are investigated for their dependence on physiological parameters (firing rate, coincidence precision, coincidence pattern complexity) and temporal resolution of the analysis. In the companion article in this issue, we describe an extension of the method, designed to deal with nonstationary firing rates. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Neural Computation},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	month = jan,
	year = {2002},
	pages = {43--80},
}

@article{nowak_influence_1997,
	title = {Influence of low and high frequency inputs on spike timing in visual cortical neurons},
	volume = {7},
	url = {https://doi.org/fvjpx7},
	doi = {10.1093/cercor/7.6.487},
	number = {6},
	journal = {Cerebral Cortex},
	author = {Nowak, L.},
	month = sep,
	year = {1997},
	pages = {487--501},
}

@article{grun_unitary_2002-1,
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {II}. {Nonstationary} {Data}},
	volume = {14},
	url = {https://doi.org/ffvbkp},
	doi = {10.1162/089976602753284464},
	abstract = {{\textless}jats:p{\textgreater} In order to detect members of a functional group (cell assembly) in simultaneously recorded neuronal spiking activity, we adopted the widely used operational definition that membership in a common assembly is expressed in near-simultaneous spike activity. Unitary event analysis, a statistical method to detect the significant occurrence of coincident spiking activity in stationary data, was recently developed (see the companion article in this issue). The technique for the detection of unitary events is based on the assumption that the underlying processes are stationary in time. This requirement, however, is usually not fulfilled in neuronal data. Here we describe a method that properly normalizes for changes of rate: the unitary events by moving window analysis (UEMWA). Analysis for unitary events is performed separately in overlapping time segments by sliding a window of constant width along the data. In each window, stationarity is assumed. Performance and sensitivity are demonstrated by use of simulated spike trains of independently firing neurons, into which coincident events are inserted. If cortical neurons organize dynamically into functional groups, the occurrence of near-simultaneous spike activity should be time varying and related to behavior and stimuli. UEMWA also accounts for these potentially interesting nonstationarities and allows locating them in time. The potential of the new method is illustrated by results from multiple single-unit recordings from frontal and motor cortical areas in awake, behaving monkey. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Neural Computation},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	month = jan,
	year = {2002},
	pages = {81--119},
}

@incollection{grun_unitary_2010,
	title = {Unitary {Event} {Analysis}},
	url = {https://doi.org/dxgxt9},
	booktitle = {Analysis of {Parallel} {Spike} {Trains}},
	publisher = {Springer US},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	year = {2010},
	pages = {191--220},
}

@article{burkitt_predictive_2021,
	title = {Predictive {Visual} {Motion} {Extrapolation} {Emerges} {Spontaneously} and without {Supervision} at {Each} {Layer} of a {Hierarchical} {Neural} {Network} with {Spike}-{Timing}-{Dependent} {Plasticity}},
	volume = {41},
	url = {https://doi.org/gjtwzk},
	doi = {10.1523/jneurosci.2017-20.2021},
	language = {en},
	number = {20},
	journal = {The Journal of Neuroscience},
	author = {Burkitt, Anthony N. and Hogendoorn, Hinze},
	year = {2021},
	pages = {4428--4438},
}

@article{kenyon_theory_2004,
	title = {A theory of the {Benham} {Top} based on center–surround interactions in the parvocellular pathway},
	volume = {17},
	url = {https://doi.org/bjwzzt},
	doi = {10.1016/j.neunet.2004.05.005},
	language = {en},
	number = {5-6},
	journal = {Neural Networks},
	author = {Kenyon, Garrett T and Hill, Dan and Theiler, James and George, John S and Marshak, David W},
	year = {2004},
	pages = {773--786},
}

@article{buzsaki_space_2018,
	title = {Space and {Time}: {The} {Hippocampus} as a {Sequence} {Generator}},
	volume = {22},
	url = {https://doi.org/gfcr76},
	doi = {10.1016/j.tics.2018.07.006},
	language = {en},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Buzsáki, György and Tingley, David},
	month = oct,
	year = {2018},
	pages = {853--869},
}

@article{kremkow_push-pull_2016,
	title = {Push-{Pull} {Receptive} {Field} {Organization} and {Synaptic} {Depression}: {Mechanisms} for {Reliably} {Encoding} {Naturalistic} {Stimuli} in {V1}},
	volume = {10},
	url = {https://doi.org/ggkdkh},
	doi = {10.3389/fncir.2016.00037},
	journal = {Frontiers in Neural Circuits},
	author = {Kremkow, Jens and Perrinet, Laurent U. and Monier, Cyril and Alonso, Jose-Manuel and Aertsen, Ad and Frégnac, Yves and Masson, Guillaume S.},
	year = {2016},
}

@article{denker_lfp_2018,
	title = {{LFP} beta amplitude is linked to mesoscopic spatio-temporal phase patterns},
	volume = {8},
	url = {https://doi.org/gc9xx6},
	doi = {10.1038/s41598-018-22990-7},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Denker, Michael and Zehl, Lyuba and Kilavik, Bjørg E. and Diesmann, Markus and Brochier, Thomas and Riehle, Alexa and Grün, Sonja},
	month = mar,
	year = {2018},
}

@article{keysers_speed_2001,
	title = {The {Speed} of {Sight}},
	volume = {13},
	url = {https://doi.org/cfdjtg},
	doi = {10.1162/089892901564199},
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}
               {\textless}jats:p{\textgreater}Macaque monkeys were presented with continuous rapid serial visual presentation (RSVP) sequences of unrelated naturalistic images at rates of 14-222 msec/image, while neurons that responded selectively to complex patterns (e.g., faces) were recorded in temporal cortex. Stimulus selectivity was preserved for 65\% of these neurons even at surprisingly fast presentation rates (14 msec/image or 72 images/sec). Five human subjects were asked to detect or remember images under equivalent conditions. Their performance in both tasks was above chance at all rates (14-111 msec/image). The performance of single neurons was comparable to that of humans and responded in a similar way to changes in presentation rate. The implications for the role of temporal cortex cells in perception are discussed.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Journal of Cognitive Neuroscience},
	author = {Keysers, C. and Xiao, D.-K. and Földiák, P. and Perrett, D. I.},
	month = jan,
	year = {2001},
	pages = {90--101},
}

@article{pachitariu_robustness_2018,
	title = {Robustness of {Spike} {Deconvolution} for {Neuronal} {Calcium} {Imaging}},
	volume = {38},
	url = {https://doi.org/gd9mcx},
	doi = {10.1523/jneurosci.3339-17.2018},
	language = {en},
	number = {37},
	journal = {The Journal of Neuroscience},
	author = {Pachitariu, Marius and Stringer, Carsen and Harris, Kenneth D.},
	year = {2018},
	pages = {7976--7985},
}

@article{luo_supervised_2022,
	title = {Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks} {With} {Spike} {Temporal} {Error} {Backpropagation}},
	url = {https://doi.org/gpzp26},
	doi = {10.1109/tnnls.2022.3164930},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Luo, Xiaoling and Qu, Hong and Wang, Yuchen and Yi, Zhang and Zhang, Jilun and Zhang, Malu},
	year = {2022},
	pages = {1--13},
}

@article{dahlem_dynamics_2009,
	title = {Dynamics of delay-coupled excitable neural systems},
	volume = {19},
	url = {https://doi.org/d43v5b},
	doi = {10.1142/s0218127409023111},
	abstract = {We study the nonlinear dynamics of two delay-coupled neural systems each modeled by excitable dynamics of FitzHugh–Nagumo type and demonstrate that bistability between the stable fixed point and limit cycle oscillations occurs for sufficiently large delay times τ and coupling strength C. As the mechanism for these delay-induced oscillations, we identify a saddle-node bifurcation of limit cycles.},
	language = {en},
	number = {02},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Dahlem, M. A. and HILLER, G. and PANCHUK, A. and SCHÖLL, E.},
	year = {2009},
	pages = {745--753},
}

@article{lamme_distinct_2000,
	title = {The distinct modes of vision offered by feedforward and recurrent processing},
	volume = {23},
	url = {https://doi.org/ccv3w2},
	doi = {10.1016/s0166-2236(00)01657-x},
	language = {en},
	number = {11},
	journal = {Trends in Neurosciences},
	author = {Lamme, Victor A.F. and Roelfsema, Pieter R.},
	month = nov,
	year = {2000},
	pages = {571--579},
}

@article{luczak_packet-based_2015,
	title = {Packet-based communication in the cortex.},
	volume = {16},
	issn = {1471-0048},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/26507295},
	doi = {10.1038/nrn4026},
	abstract = {Cortical circuits work through the generation of coordinated, large-scale activity patterns. In sensory systems, the onset of a discrete stimulus usually evokes a temporally organized packet of population activity lasting ∼50-200 ms. The structure of these packets is partially stereotypical, and variation in the exact timing and number of spikes within a packet conveys information about the identity of the stimulus. Similar packets also occur during ongoing stimuli and spontaneously. We suggest that such packets constitute the basic building blocks of cortical coding.},
	number = {12},
	journal = {Nature reviews. Neuroscience},
	author = {Luczak, Artur and McNaughton, Bruce L and Harris, Kenneth D},
	month = oct,
	year = {2015},
	pages = {745--55},
}

@article{markram_regulation_1997,
	title = {Regulation of {Synaptic} {Efficacy} by {Coincidence} of {Postsynaptic} {APs} and {EPSPs}},
	volume = {275},
	url = {https://doi.org/ftvvd8},
	doi = {10.1126/science.275.5297.213},
	abstract = {{\textless}jats:p{\textgreater}Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5297},
	journal = {Science},
	author = {Markram, Henry and Lübke, Joachim and Frotscher, Michael and Sakmann, Bert},
	month = jan,
	year = {1997},
	pages = {213--215},
}

@article{bruno_cortex_2006,
	title = {Cortex {Is} {Driven} by {Weak} but {Synchronously} {Active} {Thalamocortical} {Synapses}},
	volume = {312},
	url = {https://doi.org/c5575v},
	doi = {10.1126/science.1124593},
	abstract = {{\textless}jats:p{\textgreater}Sensory stimuli reach the brain via the thalamocortical projection, a group of axons thought to be among the most powerful in the neocortex. Surprisingly, these axons account for only ∼15\% of synapses onto cortical neurons. The thalamocortical pathway might thus achieve its effectiveness via high-efficacy thalamocortical synapses or via amplification within cortical layer 4. In rat somatosensory cortex, we measured in vivo the excitatory postsynaptic potential evoked by a single synaptic connection and found that thalamocortical synapses have low efficacy. Convergent inputs, however, are both numerous and synchronous, and intracortical amplification is not required. Our results suggest a mechanism of cortical activation by which thalamic input alone can drive cortex.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5780},
	journal = {Science},
	author = {Bruno, Randy M. and Sakmann, Bert},
	year = {2006},
	pages = {1622--1627},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	url = {https://doi.org/gc6dqh},
	doi = {10.1016/j.neunet.2017.12.005},
	language = {en},
	journal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	month = mar,
	year = {2018},
	pages = {56--67},
}

@article{gollisch_rapid_2008,
	title = {Rapid {Neural} {Coding} in the {Retina} with {Relative} {Spike} {Latencies}},
	volume = {319},
	url = {https://doi.org/c6czvj},
	doi = {10.1126/science.1149639},
	abstract = {{\textless}jats:p{\textgreater}Natural vision is a highly dynamic process. Frequent body, head, and eye movements constantly bring new images onto the retina for brief periods, challenging our understanding of the neural code for vision. We report that certain retinal ganglion cells encode the spatial structure of a briefly presented image in the relative timing of their first spikes. This code is found to be largely invariant to stimulus contrast and robust to noisy fluctuations in response latencies. Mechanistically, the observed response characteristics result from different kinetics in two retinal pathways (“ON” and “OFF”) that converge onto ganglion cells. This mechanism allows the retina to rapidly and reliably transmit new spatial information with the very first spikes emitted by a neural population.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5866},
	journal = {Science},
	author = {Gollisch, Tim and Meister, Markus},
	year = {2008},
	pages = {1108--1111},
}

@article{kremkow_functional_2010,
	title = {Functional consequences of correlated excitatory and inhibitory conductances in cortical networks},
	volume = {28},
	url = {https://doi.org/c3wrbn},
	doi = {10.1007/s10827-010-0240-9},
	language = {en},
	number = {3},
	journal = {Journal of Computational Neuroscience},
	author = {Kremkow, Jens and Perrinet, Laurent U. and Masson, Guillaume S. and Aertsen, Ad},
	year = {2010},
	pages = {579--594},
}

@article{kilavik_long-term_2009,
	title = {Long-{Term} {Modifications} in {Motor} {Cortical} {Dynamics} {Induced} by {Intensive} {Practice}},
	volume = {29},
	url = {https://doi.org/bf84ps},
	doi = {10.1523/jneurosci.1554-09.2009},
	language = {en},
	number = {40},
	journal = {Journal of Neuroscience},
	author = {Kilavik, B. E. and Roux, S. and Ponce-Alvarez, A. and Confais, J. and Grun, S. and Riehle, A.},
	month = oct,
	year = {2009},
	pages = {12653--12663},
}

@article{grammont_precise_1999,
	title = {Precise spike synchronization in monkey motor cortex involved in preparation for movement},
	volume = {128},
	url = {https://doi.org/b67khx},
	doi = {10.1007/s002210050826},
	number = {1-2},
	journal = {Experimental Brain Research},
	author = {Grammont, Franck and Riehle, Alexa},
	month = sep,
	year = {1999},
	pages = {118--122},
}

@article{gilson_stdp_2010,
	title = {{STDP} in recurrent neuronal networks},
	volume = {4},
	url = {https://doi.org/c8ck59},
	doi = {10.3389/fncom.2010.00023},
	journal = {Frontiers in Computational Neuroscience},
	author = {Gilson, Matthieu},
	year = {2010},
}

@article{celebrini_dynamics_1993,
	title = {Dynamics of orientation coding in area {V1} of the awake primate},
	volume = {10},
	url = {https://doi.org/dqt5cm},
	doi = {10.1017/s0952523800006052},
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}To investigate the importance of feedback loops in visual information processing, we have analyzed the dynamic aspects of neuronal responses to oriented gratings in cortical area V1 of the awake primate. If recurrent feedback is important in generating orientation selectivity, the initial part of the neuronal response should be relatively poorly selective, and full orientation selectivity should only appear after a delay. Thus, by examining the dynamics of the neuronal responses it should be possible to assess the importance of feedback processes in the development of orientation selectivity. The results were base on a sample of 259 cells recorded in two monkeys, of which 89\% were visually responsive. Of these, approximately two-thirds were orientation selective. Response latency varied considerably between neurons, ranging from a minimum of 41 ms to over 150 ms, although most had latencies of 50–70 ms. Orientation tuning (defined as the bandwidth at half-height) ranged from 16 deg to over 90 deg, with a mean value of around 55 deg. By examining the selectivity of these different neurons by 10-ms time slices, starting at the onset of the neuronal response, we found that the orientation selectivity of virtually every neuron was fully developed at the very start of the neuronal response. Indeed, many neurons showed a marked tendency to respond at somewhat longer latencies to stimuli that were nonoptimally oriented, with the result that orientation selectivity was highest at the very start of the neuronal response. Furthermore, there was no evidence that the neurons with the shortest onset latencies were less selective. Such evidence is inconsistent with the hypothesis that recurrent intracortical feedback plays an important role in the generation of orientation selectivity. Instead, we suggest that orientation selectivity is primarily generated using feedforward mechanisms, including feedforward inhibition. Such a strategy has the advantage of allowing orientation to be computed rapidly, and avoids the initially poorly selective neuronal responses that characterize processing involving recurrent loops.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5},
	journal = {Visual Neuroscience},
	author = {Celebrini, Simona and Thorpe, Simon and Trotter, Yves and Imbert, Michel},
	month = sep,
	year = {1993},
	pages = {811--825},
}

@inproceedings{grimaldi_homeostatic_2021,
	title = {A homeostatic gain control mechanism to improve event-driven object recognition},
	url = {https://doi.org/gkzcrv},
	doi = {10.1109/cbmi50038.2021.9461901},
	booktitle = {2021 {International} {Conference} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	publisher = {IEEE},
	author = {Grimaldi, Antoine and Boutin, Victor and Perrinet, Laurent and Ieng, Sio-Hoi and Benosman, Ryad},
	year = {2021},
}

@article{davison_pynn_2008,
	title = {{PyNN}: a common interface for neuronal network simulators},
	volume = {2},
	url = {https://doi.org/fh8h6j},
	doi = {10.3389/neuro.11.011.2008},
	journal = {Frontiers in Neuroinformatics},
	author = {Davison, Andrew P},
	year = {2008},
}

@article{duffy_variation_2019,
	title = {Variation in sequence dynamics improves maintenance of stereotyped behavior in an example from bird song},
	volume = {116},
	url = {https://doi.org/gpfjm6},
	doi = {10.1073/pnas.1815910116},
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}In this work, we show a way by which the nervous system maintains precise, stereotyped behavior in the face of environmental and neural changes. Through a model of bird song learning, we show how instability in neural representation of stable behavior can allow a system to more readily adapt and maintain performance with minimal cost. In this perspective, behaviors are made more robust to environmental change by continually seeking subtly new ways of performing the same task. Thus, one should expect to find variability in neural systems executing stereotyped behaviors, and this variability can serve a constructive role in maintaining skilled behavior.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Duffy, Alison and Abe, Elliott and Perkel, David J. and Fairhall, Adrienne L.},
	year = {2019},
	pages = {9592--9597},
}

@article{gutig_tempotron_2006,
	title = {The tempotron: a neuron that learns spike timing–based decisions},
	volume = {9},
	url = {https://doi.org/ch29r4},
	doi = {10.1038/nn1643},
	language = {en},
	number = {3},
	journal = {Nature Neuroscience},
	author = {Gütig, Robert and Sompolinsky, Haim},
	year = {2006},
	pages = {420--428},
}

@techreport{goltz_fast_2021,
	title = {Fast and energy-efficient neuromorphic deep learning with first-spike times},
	url = {https://arxiv.org/abs/1912.11443},
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems are optimized for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. With time-to-first-spike coding both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of a learning rule for such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how this mechanism can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the system's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	number = {1912.11443},
	institution = {arXiv},
	author = {Göltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	year = {2021},
}

@book{deweese_binary_2003,
	title = {Binary coding in auditory cortex},
	url = {http://papers.nips.cc/paper/2342-binary-coding-in-auditory-cortex},
	abstract = {Cortical neurons have been reported to use both rate and temporal codes. Here we describe a novel mode in which each neuron generates exactly 0 or 1 action potentials, but not more, in response to a stimulus. We used cell-attached recording, which ensured single-unit isolation, to record responses in rat auditory cortex to brief tone pips. Surprisingly, the majority of neurons exhibited binary behavior with few multi-spike responses; several dramatic examples consisted of exactly one spike on 100\% of trials, with no trial-to-trial variability in spike count. Many neurons were tuned to stimulus frequency. Since individual trials yielded at most one spike for most neurons, the information about stimulus frequency was encoded in the population, and would not have been accessible to later stages of processing that only had access to the activity of a single unit. These binary units allow a more efficient population code than is possible with conventional rate coding units, and are consistent with a model of cortical processing in which synchronous packets of spikes propagate stably from one neuronal population to the next.},
	urldate = {2022-10-04},
	publisher = {Neural Information Processing Systems Foundation},
	author = {DeWeese, M. R. and Zador, A. M.},
	year = {2003},
}

@article{berens_fast_2012,
	title = {A {Fast} and {Simple} {Population} {Code} for {Orientation} in {Primate} {V1}},
	volume = {32},
	url = {https://doi.org/f365rn},
	doi = {10.1523/jneurosci.1335-12.2012},
	language = {en},
	number = {31},
	journal = {Journal of Neuroscience},
	author = {Berens, P. and Ecker, A. S. and Cotton, R. J. and Ma, W. J. and Bethge, M. and Tolias, A. S.},
	year = {2012},
	pages = {10618--10626},
}

@article{azouz_stimulus-selective_2008,
	title = {Stimulus-selective spiking is driven by the relative timing of synchronous excitation and disinhibition in cat striate neurons\textit{in vivo}},
	volume = {28},
	url = {https://doi.org/cbcr8h},
	doi = {10.1111/j.1460-9568.2008.06434.x},
	language = {en},
	number = {7},
	journal = {European Journal of Neuroscience},
	author = {Azouz, Rony and Gray, Charles M.},
	month = oct,
	year = {2008},
	pages = {1286--1300},
}

@article{agus_rapid_2010,
	title = {Rapid {Formation} of {Robust} {Auditory} {Memories}: {Insights} from {Noise}},
	volume = {66},
	url = {https://doi.org/dc3r2d},
	doi = {10.1016/j.neuron.2010.04.014},
	language = {en},
	number = {4},
	journal = {Neuron},
	author = {Agus, Trevor R. and Thorpe, Simon J. and Pressnitzer, Daniel},
	year = {2010},
	pages = {610--618},
}

@book{abeles_corticonics_1991,
	address = {Cambridge ; New York},
	title = {Corticonics: neural circuits of the cerebral cortex},
	isbn = {978-0-521-37476-7},
	shorttitle = {Corticonics},
	publisher = {Cambridge University Press},
	author = {Abeles, Moshe},
	year = {1991},
}

@article{sotomayor-gomez_spikeship_2021,
	title = {{SpikeShip}: {A} method for fast, unsupervised discovery of high-dimensional neural spiking patterns},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.03.131573},
	doi = {10.1101/2020.06.03.131573},
	journal = {bioRxiv : the preprint server for biology},
	author = {Sotomayor-Gómez, Boris and Battaglia, Francesco P and Vinck, Martin},
	year = {2021},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2020--06},
}

@article{gewaltig_propagation_2001,
	title = {Propagation of cortical synfire activity: survival probability in single trials and stability in the mean},
	volume = {14},
	issn = {0893-6080},
	shorttitle = {Propagation of cortical synfire activity},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000703},
	doi = {10.1016/S0893-6080(01)00070-3},
	abstract = {The synfire hypothesis states that under appropriate conditions volleys of synchronized spikes (pulse packets) can propagate through the cortical network by traveling along chains of groups of cortical neurons. Here, we present results from network simulations, taking full account of the variability in pulse packet realizations. We repeatedly stimulated a synfire chain of model neurons and estimated activity (a) and temporal jitter (σ) of the spike response for each neuron group in the chain in many trials. The survival probability of the activity was assessed for each point in (a, σ)-space. The results confirm and extend our earlier predictions based on single neuron properties and a deterministic state-space analysis [Diesmann, M., Gewaltig, M.-O., \& Aertsen, A. (1999). Stable propagation of synchronous spiking in cortical neural networks. Nature, 402, 529–533].},
	language = {en},
	number = {6},
	urldate = {2022-10-26},
	journal = {Neural Networks},
	author = {Gewaltig, Marc-Oliver and Diesmann, Markus and Aertsen, Ad},
	month = jul,
	year = {2001},
	keywords = {Action Potentials, Animals, Cell Membrane, Cerebral Cortex, Cortical dynamics, Humans, Integrate-and-fire neurons, Models, Statistical, Nerve Net, Neural Networks, Computer, Neurons, Pulse packets, Single-trial analysis, Spike patterns, Spiking neurons, Synaptic Transmission, Synfire chains, Variability},
	pages = {657--673},
}

@misc{jeremie_ultrafast_2022,
	title = {Ultrafast image categorization in vivo and in silico},
	url = {http://arxiv.org/abs/2205.03635},
	abstract = {Humans are able to categorize images very efficiently, in particular to detect very rapidly the presence of an animal. Recently, deep learning algorithms have achieved higher accuracy than humans for a large set of visual recognition tasks. However, the tasks on which these artificial networks are usually trained and evaluated are usually very specialized which do not generalize well, for example with an accuracy drop following a rotation of the image. In this regard, biological visual systems are more flexible and efficient than artificial systems for more generic tasks, such as detecting an animal. To further the comparison between biological and artificial neural networks, we retrained the standard VGG16 convolutional neural network (CNN) on two independent tasks that are ecologically relevant to humans: detecting the presence of an animal or an artifact. We show that retraining the network achieves a human-like level of performance, comparable to what is reported in psychophysical tasks. Moreover, we show that categorization is better when combining the models' outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). Furthermore, these re-trained models were able to reproduce some unexpected behavioral observations of human psychophysics, such as robustness to rotations (e.g., an upside-down or tilted image) or to a grayscale transformation. Finally, we quantified the number of CNN layers needed to achieve such performance, showing that good accuracy for ultrafast image categorization could be achieved with only a few layers, challenging the belief that image recognition would require a deep sequential analysis of visual objects.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Jérémie, Jean-Nicolas and Perrinet, Laurent U.},
	month = oct,
	year = {2022},
	note = {arXiv:2205.03635 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
}

@misc{yu_stsc-snn_2022,
	title = {{STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with {Temporal} {Convolution} and {Attention} for {Spiking} {Neural} {Networks}},
	shorttitle = {{STSC}-{SNN}},
	url = {http://arxiv.org/abs/2210.05241},
	abstract = {Spiking Neural Networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing the event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a spatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Concretely, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance vias varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05241 [cs, q-bio, stat]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{vanrullen_continuous_2006,
	title = {The {Continuous} {Wagon} {Wheel} {Illusion} {Is} {Associated} with {Changes} in {Electroencephalogram} {Power} at 13 {Hz}},
	volume = {26},
	copyright = {Copyright © 2006 Society for Neuroscience 0270-6474/06/26502-06.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/26/2/502},
	doi = {10.1523/jneurosci.4654-05.2006},
	language = {en},
	number = {2},
	urldate = {2019-11-05},
	journal = {Journal of Neuroscience},
	author = {VanRullen, Rufin and Reddy, Leila and Koch, Christof},
	month = jan,
	year = {2006},
	pmid = {16407547},
	note = {00000 },
	pages = {502--507},
}

@article{ghosh_synchronized_2022,
	title = {The synchronized dynamics of time-varying networks},
	volume = {949},
	issn = {03701573},
	url = {http://arxiv.org/abs/2109.07618},
	doi = {10.1016/j.physrep.2021.10.006},
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	urldate = {2022-10-17},
	journal = {Physics Reports},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	month = feb,
	year = {2022},
	note = {arXiv:2109.07618 [physics]},
	keywords = {Physics - Physics and Society},
	pages = {1--63},
}

@article{lee_combinatorial_2004,
	title = {A {Combinatorial} {Method} for {Analyzing} {Sequential} {Firing} {Patterns} {Involving} an {Arbitrary} {Number} of {Neurons} {Based} on {Relative} {Time} {Order}},
	volume = {92},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.01030.2003},
	doi = {10.1152/jn.01030.2003},
	abstract = {Information processing in the brain is believed to require coordinated activity across many neurons. With the recent development of techniques for simultaneously recording the spiking activity of large numbers of individual neurons, the search for complex multicell firing patterns that could help reveal this neural code has become possible. Here we develop a new approach for analyzing sequential firing patterns involving an arbitrary number of neurons based on relative firing order. Specifically, we develop a combinatorial method for quantifying the degree of matching between a “reference sequence” of N distinct “letters” (representing a particular target order of firing by N cells) and an arbitrarily long “word” composed of any subset of those letters including repeats (representing the relative time order of spikes in an arbitrary firing pattern). The method involves computing the probability that a random permutation of the word's letters would by chance alone match the reference sequence as well as or better than the actual word does, assuming all permutations were equally likely. Lower probabilities thus indicate better matching. The overall degree and statistical significance of sequence matching across a heterogeneous set of words (such as those produced during the course of an experiment) can be computed from the corresponding set of probabilities. This approach can reduce the sample size problem associated with analyzing complex firing patterns. The approach is general and thus applicable to other types of neural data beyond multiple spike trains, such as EEG events or imaging signals from multiple locations. We have recently applied this method to quantify memory traces of sequential experience in the rodent hippocampus during slow wave sleep.},
	language = {en},
	number = {4},
	urldate = {2022-10-17},
	journal = {Journal of Neurophysiology},
	author = {Lee, Albert K. and Wilson, Matthew A.},
	month = oct,
	year = {2004},
	pages = {2555--2573},
}

@article{nadasdy_replay_1999,
	title = {Replay and {Time} {Compression} of {Recurring} {Spike} {Sequences} in the {Hippocampus}},
	volume = {19},
	copyright = {Copyright © 1999 Society for Neuroscience},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/19/21/9497},
	doi = {10.1523/JNEUROSCI.19-21-09497.1999},
	abstract = {Information in neuronal networks may be represented by the spatiotemporal patterns of spikes. Here we examined the temporal coordination of pyramidal cell spikes in the rat hippocampus during slow-wave sleep. In addition, rats were trained to run in a defined position in space (running wheel) to activate a selected group of pyramidal cells. A template-matching method and a joint probability map method were used for sequence search. Repeating spike sequences in excess of chance occurrence were examined by comparing the number of repeating sequences in the original spike trains and in surrogate trains after Monte Carlo shuffling of the spikes. Four different shuffling procedures were used to control for the population dynamics of hippocampal neurons. Repeating spike sequences in the recorded cell assemblies were present in both the awake and sleeping animal in excess of what might be predicted by random variations. Spike sequences observed during wheel running were “replayed” at a faster timescale during single sharp-wave bursts of slow-wave sleep. We hypothesize that the endogenously expressed spike sequences during sleep reflect reactivation of the circuitry modified by previous experience. Reactivation of acquired sequences may serve to consolidate information.},
	language = {en},
	number = {21},
	urldate = {2022-10-17},
	journal = {Journal of Neuroscience},
	author = {Nádasdy, Zoltán and Hirase, Hajime and Czurkó, András and Csicsvari, Jozsef and Buzsáki, György},
	month = nov,
	year = {1999},
	pmid = {10531452},
	note = {Publisher: Society for Neuroscience
Section: ARTICLE},
	keywords = {coding, decoding, memory, network, retrieval, sharp waves, sleep, θ},
	pages = {9497--9507},
}

@article{aronov_non-euclidean_2004,
	title = {Non-{Euclidean} properties of spike train metric spaces},
	volume = {69},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.69.061905},
	doi = {10.1103/PhysRevE.69.061905},
	abstract = {Quantifying the dissimilarity (or distance) between two sequences is essential to the study of action potential (spike) trains in neuroscience and genetic sequences in molecular biology. In neuroscience, traditional methods for sequence comparisons rely on techniques appropriate for multivariate data, which typically assume that the space of sequences is intrinsically Euclidean. More recently, metrics that do not make this assumption have been introduced for comparison of neural activity patterns. These metrics have a formal resemblance to those used in the comparison of genetic sequences. Yet the relationship between such metrics and the traditional Euclidean distances has remained unclear. We show, both analytically and computationally, that the geometries associated with metric spaces of event sequences are intrinsically non-Euclidean. Our results demonstrate that metric spaces enrich the study of neural activity patterns, since accounting for perceptual spaces requires a non-Euclidean geometry.},
	number = {6},
	urldate = {2022-10-17},
	journal = {Physical Review E},
	author = {Aronov, Dmitriy and Victor, Jonathan D.},
	month = jun,
	year = {2004},
	note = {Publisher: American Physical Society},
	pages = {061905},
}

@article{victor_nature_1996,
	title = {Nature and precision of temporal coding in visual cortex: a metric-space analysis},
	volume = {76},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Nature and precision of temporal coding in visual cortex},
	url = {https://www.physiology.org/doi/10.1152/jn.1996.76.2.1310},
	doi = {10.1152/jn.1996.76.2.1310},
	abstract = {1. We recorded single-unit and multi-unit activity in response to transient presentation of texture and grating patterns at 25 sites within the parafoveal representation of V1, V2, and V3 of two awake monkeys trained to perform a fixation task. In grating experiments, stimuli varied in orientation, spatial frequency, or both. In texture experiments, stimuli varied in contrast, check size, texture type, or pairs of these attributes. 2. To examine the nature and precision of temporal coding, we compared individual responses elicited by each set of stimuli in terms of two families of metrics. One family of metrics, D(spike), was sensitive to the absolute spike time (following stimulus onset). The second family of metrics, D(interval), was sensitive to the pattern of interspike intervals. In each family, the metrics depend on a parameter q, which expresses the precision of temporal coding. For q = 0, both metrics collapse into the "spike count" metric D(Count), which is sensitive to the number of impulses but insensitive to their position in time. 3. Each of these metrics, with values of q ranging from 0 to 512/s, was used to calculate the distance between all pairs of spike trains within each dataset. The extent of stimulus-specific clustering manifest in these pairwise distances was quantified by an information measure. Chance clustering was estimated by applying the same procedure to synthetic data sets in which responses were assigned randomly to the input stimuli. 4. Of the 352 data sets, 170 showed evidence of tuning via the spike count (q = 0) metric, 294 showed evidence of tuning via the spike time metric, 272 showed evidence of tuning via the spike interval metric to the stimulus attribute (contrast, check size, orientation, spatial frequency, or texture type) under study. Across the entire dataset, the information not attributable to chance clustering averaged 0.042 bits for the spike count metric, 0.171 bits for the optimal spike time metric, and 0.107 bits for the optimal spike interval metric. 5. The reciprocal of the optimal cost q serves as a measure of the temporal precision of temporal coding. In V1 and V2, with both metrics, temporal precision was highest for contrast (ca. 10-30 ms) and lowest for texture type (ca. 100 ms). This systematic dependence of q on stimulus attribute provides a possible mechanism for the simultaneous representation of multiple stimulus attributes in one spike train. 6. Our findings are inconsistent with Poisson models of spike trains. Synthetic data sets in which firing rate was governed by a time-dependent Poisson process matched to the observed poststimulus time histogram (PSTH) overestimated clustering induced by D(count) and, for low values of q, D(spike)[q] and D(intervals)[q]. Synthetic data sets constructed from a modified Poisson process, which preserved not only the PSTH but also spike count statistics accounted for the clustering induced by D(count) but underestimated the clustering induced by D(spike)[q] and D(interval)[q].},
	language = {en},
	number = {2},
	urldate = {2022-10-17},
	journal = {Journal of Neurophysiology},
	author = {Victor, J. D. and Purpura, K. P.},
	month = aug,
	year = {1996},
	pages = {1310--1326},
}

@article{van_rossum_novel_2001,
	title = {A novel spike distance},
	volume = {13},
	issn = {0899-7667},
	doi = {10.1162/089976601300014321},
	abstract = {The discrimination between two spike trains is a fundamental problem for both experimentalists and the nervous system itself. We introduce a measure for the distance between two spike trains. The distance has a time constant as a parameter. Depending on this parameter, the distance interpolates between a coincidence detector and a rate difference counter. The dependence of the distance on noise is studied with an integrate-and-fire model. For an intermediate range of the time constants, the distance depends linearly on the noise. This property can be used to determine the intrinsic noise of a neuron.},
	language = {eng},
	number = {4},
	journal = {Neural Computation},
	author = {van Rossum, M. C.},
	month = apr,
	year = {2001},
	pmid = {11255567},
	keywords = {Algorithms, Evoked Potentials, Models, Neurological, Neurons, Poisson Distribution},
	pages = {751--763},
}

@article{khoei_motion-based_2013,
	title = {Motion-based prediction explains the role of tracking in motion extrapolation},
	volume = {107},
	issn = {09284257},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092842571300051X},
	doi = {10.1016/j.jphysparis.2013.08.001},
	abstract = {During normal viewing, the continuous stream of visual input is regularly interrupted, for instance by blinks of the eye. Despite these frequents blanks (that is the transient absence of a raw sensory source), the visual system is most often able to maintain a continuous representation of motion. For instance, it maintains the movement of the eye such as to stabilize the image of an object. This ability suggests the existence of a generic neural mechanism of motion extrapolation to deal with fragmented inputs. In this paper, we have modeled how the visual system may extrapolate the trajectory of an object during a blank using motion-based prediction. This implies that using a prior on the coherency of motion, the system may integrate previous motion information even in the absence of a stimulus. In order to compare with experimental results, we simulated tracking velocity responses. We found that the response of the motion integration process to a blanked trajectory pauses at the onset of the blank, but that it quickly recovers the information on the trajectory after reappearance. This is compatible with behavioral and neural observations on motion extrapolation. To understand these mechanisms, we have recorded the response of the model to a noisy stimulus. Crucially, we found that motion-based prediction acted at the global level as a gain control mechanism and that we could switch from a smooth regime to a binary tracking behavior where the dot is tracked or lost. Our results imply that a local prior implementing motion-based prediction is sufﬁcient to explain a large range of neural and behavioral results at a more global level. We show that the tracking behavior deteriorates for sensory noise levels higher than a certain value, where motion coherency and predictability fail to hold longer. In particular, we found that motion-based prediction leads to the emergence of a tracking behavior only when enough information from the trajectory has been accumulated. Then, during tracking, trajectory estimation is robust to blanks even in the presence of relatively high levels of noise. Moreover, we found that tracking is necessary for motion extrapolation, this calls for further experimental work exploring the role of noise in motion extrapolation.},
	language = {en},
	number = {5},
	urldate = {2022-10-17},
	journal = {Journal of Physiology-Paris},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	month = nov,
	year = {2013},
	pages = {409--420},
}

@article{susi_nmnsd-spiking_2021,
	title = {{nMNSD}-{A} {Spiking} {Neuron}-{Based} {Classifier} {That} {Combines} {Weight}-{Adjustment} and {Delay}-{Shift}},
	volume = {15},
	issn = {1662-4548},
	doi = {10.3389/fnins.2021.582608},
	abstract = {The recent "multi-neuronal spike sequence detector" (MNSD) architecture integrates the weight- and delay-adjustment methods by combining heterosynaptic plasticity with the neurocomputational feature spike latency, representing a new opportunity to understand the mechanisms underlying biological learning. Unfortunately, the range of problems to which this topology can be applied is limited because of the low cardinality of the parallel spike trains that it can process, and the lack of a visualization mechanism to understand its internal operation. We present here the nMNSD structure, which is a generalization of the MNSD to any number of inputs. The mathematical framework of the structure is introduced, together with the "trapezoid method," that is a reduced method to analyze the recognition mechanism operated by the nMNSD in response to a specific input parallel spike train. We apply the nMNSD to a classification problem previously faced with the classical MNSD from the same authors, showing the new possibilities the nMNSD opens, with associated improvement in classification performances. Finally, we benchmark the nMNSD on the classification of static inputs (MNIST database) obtaining state-of-the-art accuracies together with advantageous aspects in terms of time- and energy-efficiency if compared to similar classification methods.},
	language = {eng},
	journal = {Frontiers in Neuroscience},
	author = {Susi, Gianluca and Antón-Toro, Luis F. and Maestú, Fernando and Pereda, Ernesto and Mirasso, Claudio},
	year = {2021},
	pmid = {33679293},
	pmcid = {PMC7933525},
	keywords = {MNIST database, MNSD, classification, delay learning, heterosynaptic plasticity, online learning, spike latency},
	pages = {582608},
}

@article{dugue_phase_2011,
	title = {The {Phase} of {Ongoing} {Oscillations} {Mediates} the {Causal} {Relation} between {Brain} {Excitation} and {Visual} {Perception}},
	volume = {31},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1161-11.2011},
	doi = {10.1523/JNEUROSCI.1161-11.2011},
	language = {en},
	number = {33},
	urldate = {2022-10-17},
	journal = {Journal of Neuroscience},
	author = {Dugue, L. and Marque, P. and VanRullen, R.},
	month = aug,
	year = {2011},
	pages = {11889--11893},
}

@article{fries_mechanism_2005,
	title = {A mechanism for cognitive dynamics: neuronal communication through neuronal coherence},
	volume = {9},
	issn = {1364-6613},
	shorttitle = {A mechanism for cognitive dynamics},
	doi = {10.1016/j.tics.2005.08.011},
	abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility.},
	language = {eng},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Fries, Pascal},
	month = oct,
	year = {2005},
	pmid = {16150631},
	keywords = {Action Potentials, Animals, Biological Clocks, Brain, Cognition, Humans, Nerve Net, Neurons, Nonlinear Dynamics, Periodicity, Pyramidal Tracts, Synaptic Transmission},
	pages = {474--480},
}

@book{hebb_organization_1949,
	address = {New York},
	title = {The organization of behavior: {A} neuropsychological theory},
	publisher = {Wiley},
	author = {Hebb, Donald O.},
	year = {1949},
	keywords = {bicv-sparse},
}

@article{benvenuti_anticipatory_2020,
	title = {Anticipatory responses along motion trajectories in awake monkey area {V1}},
	copyright = {All rights reserved},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.26.010017v1},
	doi = {10.1101/2020.03.26.010017},
	abstract = {What are the neural mechanisms underlying motion integration of translating objects? Visual motion integration is generally conceived of as a feedforward, hierarchical, information processing. However, feedforward models fail to account for many contextual effects revealed using natural moving stimuli. In particular, a translating object evokes a sequence of transient feedforward responses in the primary visual cortex but also propagations of activity through horizontal and feedback pathways. We investigated how these pathways shape the representation of a translating bar in monkey V1. We show that, for long trajectories, spiking activity builds-up hundreds of milliseconds before the bar enters the neurons receptive fields. Using VSDI and LFP recordings guided by a phenomenological model of propagation dynamics, we demonstrate that this anticipatory response arises from the interplay between horizontal and feedback networks driving V1 neurons well ahead of their feedforward inputs. This mechanism could subtend several perceptual contextual effects observed with translating objects.},
	language = {english},
	urldate = {2020-03-31},
	journal = {bioRxiv : the preprint server for biology},
	author = {Benvenuti, Giacomo and Chemla, Sandrine and Boonman, Arjan and Perrinet, Laurent U and Masson, Guillaume S and Chavane, Frederic},
	month = mar,
	year = {2020},
	pages = {2020.03.26.010017},
}

@article{le_bec_horizontal_2022,
	title = {Horizontal connectivity in {V1}: {Prediction} of coherence in contour and motion integration},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Horizontal connectivity in {V1}},
	url = {https://dx.plos.org/10.1371/journal.pone.0268351},
	doi = {10.1371/journal.pone.0268351},
	abstract = {This study demonstrates the functional importance of the Surround context relayed laterally in V1 by the horizontal connectivity, in controlling the latency and the gain of the cortical response to the feedforward visual drive. We report here four main findings: 1) a centripetal apparent motion sequence results in a shortening of the spiking latency of V1 cells, when the orientation of the local inducer and the global motion axis are both co-aligned with the RF orientation preference; 2) this contextual effects grows with visual flow speed, peaking at 150–250°/s when it matches the propagation speed of horizontal connectivity (0.15–0.25 mm/ms); 3) For this speed range, the axial sensitivity of V1 cells is tilted by 90° to become co-aligned with the orientation preference axis; 4) the strength of modulation by the surround context correlates with the spatiotemporal coherence of the apparent motion flow. Our results suggest an internally-generated binding process, linking local (orientation /position) and global (motion/direction) features as early as V1. This long-range diffusion process constitutes a plausible substrate in V1 of the human psychophysical bias in speed estimation for collinear motion. Since it is demonstrated in the anesthetized cat, this novel form of contextual control of the cortical gain and phase is a built-in property in V1, whose expression does not require behavioral attention and top-down control from higher cortical areas. We propose that horizontal connectivity participates in the propagation of an internal “prediction” wave, shaped by visual experience, which links contour co-alignment and global axial motion at an apparent speed in the range of saccade-like eye movements.},
	language = {en},
	number = {7},
	urldate = {2022-09-26},
	journal = {PLOS ONE},
	author = {Le Bec, Benoit and Troncoso, Xoana G. and Desbois, Christophe and Passarelli, Yannick and Baudot, Pierre and Monier, Cyril and Pananceau, Marc and Frégnac, Yves},
	editor = {Charpier, Stéphane},
	month = jul,
	year = {2022},
	pages = {e0268351},
}

@article{pearce_marie-jean-pierre_2009,
	title = {Marie-{Jean}-{Pierre} {Flourens} (1794–1867) and {Cortical} {Localization}},
	volume = {61},
	issn = {0014-3022, 1421-9913},
	url = {https://www.karger.com/Article/FullText/206858},
	doi = {10.1159/000206858},
	abstract = {The child prodigy Marie-Jean-Pierre Flourens received his medical degree at Montpellier when aged 19. As a young promising physician Flourens was asked to investigate Gall’s controversial views on cerebral localization. To test Gall’s assertions, Flourens developed ablation as a procedure to explore the workings of the brain. By removing anatomically defined areas of the brain of an animal and watching its behaviour, he thought he might localize certain functions. Flourens did not favour the idea of cerebral localization and concluded that the brain functioned as a whole and thus arose the concept of ‘cerebral equipotentiality’. This culminated in his 1824 Recherches expérimentales sur les propriétés et les fonctions du système nerveux. His techniques were, however, crude and imperfect, and his experiments were mainly on birds. Much criticism and debate ensued. A gifted man, Flourens also advanced the physiology of the vestibular apparatus and described the anaesthetic properties of ether.},
	language = {en},
	number = {5},
	urldate = {2022-10-10},
	journal = {European Neurology},
	author = {Pearce, J.M.S.},
	year = {2009},
	pages = {311--314},
}

@article{adrian_impulses_1926,
	title = {The impulses produced by sensory nerve endings},
	volume = {61},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1514868/},
	abstract = {Images
null},
	number = {4},
	urldate = {2022-10-10},
	journal = {The Journal of Physiology},
	author = {Adrian, E. D. and Zotterman, Yngve},
	month = aug,
	year = {1926},
	pmid = {16993807},
	pmcid = {PMC1514868},
	pages = {465--483},
}

@article{piccolino_luigi_1997,
	title = {Luigi {Galvani} and animal electricity: two centuries after the foundation of electrophysiology},
	volume = {20},
	issn = {0166-2236},
	shorttitle = {Luigi {Galvani} and animal electricity},
	url = {https://www.sciencedirect.com/science/article/pii/S0166223697011016},
	doi = {10.1016/S0166-2236(97)01101-6},
	abstract = {Luigi Galvani and his famous experiments on frogs carried out in the second half of the 18th century belong more to legend than to the history of science. Galvani not only laid the foundations of a new science, electrophysiology, but also opened the way for the invention of the electric battery, and thus for the development of the physical investigations of electricity. However, in spite of the widespread celebration of his work, Galvani's scientific endeavours have been largely misrepresented in the history of science. The scholar of Bologna has a stereotyped image as an `occasional' scientist, who started his studies by chance, largely ignored the scientific theories of his time and wandered aimlessly in mental elaborations until the physicist of Pavia, Alessandro Volta, entered the field, correctly interpreted Galvani's results and eventually developed the electric battery. With the present understanding of electrical phenomena in excitable membranes, it is now time to reconsider the real matter raised by Galvani's discoveries and by his hypothesis of an intrinsic `animal electricity', and to make a clearer evaluation of a revolutionary phase of scientific progress.},
	language = {en},
	number = {10},
	urldate = {2022-10-10},
	journal = {Trends in Neurosciences},
	author = {Piccolino, Marco},
	month = oct,
	year = {1997},
	keywords = {Galvani, Volta, animal electricity, electrophysiology, history of science, nervous signalling},
	pages = {443--448},
}

@article{wang_neuromorphic_2015,
	title = {A neuromorphic implementation of multiple spike-timing synaptic plasticity rules for large-scale neural networks},
	volume = {9},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00180},
	doi = {10.3389/fnins.2015.00180},
	abstract = {We present a neuromorphic implementation of multiple synaptic plasticity learning rules, which include both Spike Timing Dependent Plasticity (STDP) and Spike Timing Dependent Delay Plasticity (STDDP). We present a fully digital implementation as well as a mixed-signal implementation, both of which use a novel dynamic-assignment time-multiplexing approach and support up to 226 (64M) synaptic plasticity elements. Rather than implementing dedicated synapses for particular types of synaptic plasticity, we implemented a more generic synaptic plasticity adaptor array that is separate from the neurons in the neural network. Each adaptor performs synaptic plasticity according to the arrival times of the pre- and post-synaptic spikes assigned to it, and sends out a weighted or delayed pre-synaptic spike to the post-synaptic neuron in the neural network. This strategy provides great flexibility for building complex large-scale neural networks, as a neural network can be configured for multiple synaptic plasticity rules without changing its structure. We validate the proposed neuromorphic implementations with measurement results and illustrate that the circuits are capable of performing both STDP and STDDP. We argue that it is practical to scale the work presented here up to 236 (64G) synaptic adaptors on a current high-end FPGA platform.},
	urldate = {2022-10-06},
	journal = {Frontiers in Neuroscience},
	author = {Wang, Runchun M. and Hamilton, Tara J. and Tapson, Jonathan C. and van Schaik, André},
	year = {2015},
}

@article{isbister_clustering_2021,
	title = {Clustering and control for adaptation uncovers time-warped spike time patterns in cortical networks in vivo},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-94002-0},
	doi = {10.1038/s41598-021-94002-0},
	abstract = {How information in the nervous system is encoded by patterns of action potentials (i.e. spikes) remains an open question. Multi-neuron patterns of single spikes are a prime candidate for spike time encoding but their temporal variability requires further characterisation. Here we show how known sources of spike count variability affect stimulus-evoked spike time patterns between neurons separated over multiple layers and columns of adult rat somatosensory cortex in vivo. On subsets of trials (clusters) and after controlling for stimulus-response adaptation, spike time differences between pairs of neurons are “time-warped” (compressed/stretched) by trial-to-trial changes in shared excitability, explaining why fixed spike time patterns and noise correlations are seldom reported. We show that predicted cortical state is correlated between groups of 4 neurons, introducing the possibility of spike time pattern modulation by population-wide trial-to-trial changes in excitability (i.e. cortical state). Under the assumption of state-dependent coding, we propose an improved potential encoding capacity.},
	language = {en},
	number = {1},
	urldate = {2022-10-06},
	journal = {Scientific Reports},
	author = {Isbister, James B. and Reyes-Puerta, Vicente and Sun, Jyh-Jang and Horenko, Illia and Luhmann, Heiko J.},
	month = jul,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {15066},
}

@article{wang_delay_2019,
	title = {A {Delay} {Learning} {Algorithm} {Based} on {Spike} {Train} {Kernels} for {Spiking} {Neurons}},
	volume = {13},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252},
	abstract = {Neuroscience research confirms that the synaptic delays are not constant, but can be modulated. This paper proposes a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. The proposed algorithm firstly defines spike train kernels to transform discrete spike trains during the learning phase into continuous analog signals so that common mathematical operations can be performed on them, and then deduces the supervised learning rules of synaptic weights and delays by gradient descent method. The proposed algorithm is successfully applied to various spike train learning tasks, and the effects of parameters of synaptic delays are analyzed in detail. Experimental results show that the network with dynamic delays achieves higher learning accuracy and less learning epochs than the network with static delays. The delay learning algorithm is further validated on a practical example of an image classification problem. The results again show that it can achieve a good classification performance with a proper receptive field. Therefore, the synaptic delay learning is significant for practical applications and theoretical researches of spiking neural networks.},
	urldate = {2022-10-04},
	journal = {Frontiers in Neuroscience},
	author = {Wang, Xiangwen and Lin, Xianghong and Dang, Xiaochao},
	year = {2019},
}

@article{perkel_neuronal_1967,
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {I}. {The} {Single} {Spike} {Train}},
	volume = {7},
	issn = {0006-3495},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865962},
	doi = {10.1016/S0006-3495(67)86596-2},
	abstract = {In a growing class of neurophysiological experiments, the train of impulses (“spikes”) produced by a nerve cell is subjected to statistical treatment involving the time intervals between spikes. The statistical techniques available for the analysis of single spike trains are described and related to the underlying mathematical theory, that of stochastic point processes, i.e., of stochastic processes whose realizations may be described as series of point events occurring in time, separated by random intervals. For single stationary spike trains, several orders of complexity of statistical treatment are described; the major distinction is that between statistical measures that depend in an essential way on the serial order of interspike intervals and those that are order-independent. The interrelations among the several types of calculations are shown, and an attempt is made to ameliorate the current nomenclatural confusion in this field. Applications, interpretations, and potential difficulties of the statistical techniques are discussed, with special reference to types of spike trains encountered experimentally. Next, the related types of analysis are described for experiments which involve repeated presentations of a brief, isolated stimulus. Finally, the effects of nonstationarity, e.g. long-term changes in firing rate, on the various statistical measures are discussed. Several commonly observed patterns of spike activity are shown to be differentially sensitive to such changes. A companion paper covers the analysis of simultaneously observed spike trains.},
	language = {en},
	number = {4},
	urldate = {2022-10-04},
	journal = {Biophysical Journal},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	month = jul,
	year = {1967},
	pages = {391--418},
}

@article{perkel_neuronal_1967-1,
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {II}. {Simultaneous} {Spike} {Trains}},
	volume = {7},
	issn = {0006-3495},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865974},
	doi = {10.1016/S0006-3495(67)86597-4},
	abstract = {The statistical analysis of two simultaneously observed trains of neuronal spikes is described, using as a conceptual framework the theory of stochastic point processes. The first statistical question that arises is whether the observed trains are independent; statistical techniques for testing independence are developed around the notion that, under the null hypothesis, the times of spike occurrence in one train represent random instants in time with respect to the other. If the null hypothesis is rejected—if dependence is attributed to the trains—the problem then becomes that of characterizing the nature and source of the observed dependencies. Statistical signs of various classes of dependencies, including direct interaction and shared input, are discussed and illustrated through computer simulations of interacting neurons. The effects of nonstationarities on the statistical measures for simultaneous spike trains are also discussed. For two-train comparisons of irregularly discharging nerve cells, moderate nonstationarities are shown to have little effect on the detection of interactions. Combining repetitive stimulation and simultaneous recording of spike trains from two (or more) neurons yields additional clues as to possible modes of interaction among the monitored neurons; the theory presented is illustrated by an application to experimentally obtained data from auditory neurons. A companion paper covers the analysis of single spike trains.},
	language = {en},
	number = {4},
	urldate = {2022-10-04},
	journal = {Biophysical Journal},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	month = jul,
	year = {1967},
	pages = {419--440},
}

@misc{ghosh_spatiotemporal_2019,
	title = {Spatiotemporal filtering for event-based action recognition},
	url = {http://arxiv.org/abs/1903.07067},
	author = {Ghosh, Rohan and Gupta, Anupam and Silva, Andrei Nakagawa and Soares, Alcimar and Thakor, Nitish V.},
	year = {2019},
}

@article{warner_probabilistic_2022,
	title = {A probabilistic latent variable model for detecting structure in binary data},
	url = {http://arxiv.org/abs/2201.11108},
	abstract = {We introduce a novel, probabilistic binary latent variable model to detect noisy or approximate repeats of patterns in sparse binary data. The model is based on the ”Noisy-OR model” [5], used previously for disease and topic modelling. The model’s capability is demonstrated by extracting structure in recordings from retinal neurons, but it can be widely applied to discover and model latent structure in noisy binary data. In the context of spiking neural data, the task is to “explain” spikes of individual neurons in terms of groups of neurons, ”Cell Assemblies” (CAs), that often fire together, due to mutual interactions or other causes. The model infers sparse activity in a set of binary latent variables, each describing the activity of a cell assembly. When the latent variable of a cell assembly is active, it reduces the probabilities of neurons belonging to this assembly to be inactive. The conditional probability kernels of the latent components are learned from the data in an expectation maximization scheme, involving inference of latent states and parameter adjustments to the model. We thoroughly validate the model on synthesized spike trains constructed to statistically resemble recorded retinal responses to white noise stimulus and natural movie stimulus in data. We also apply our model to spiking responses recorded in retinal ganglion cells (RGCs) during stimulation with a movie and discuss the found structure.},
	language = {en},
	urldate = {2022-02-16},
	journal = {arXiv:2201.11108 [cs, q-bio, stat]},
	author = {Warner, Christopher and Ruda, Kiersten and Sommer, Friedrich T.},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.11108},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{pauli_reproducing_2018,
	title = {Reproducing {Polychronization}: {A} {Guide} to {Maximizing} the {Reproducibility} of {Spiking} {Network} {Models}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {Reproducing {Polychronization}},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00046},
	doi = {10/gd8zj5},
	abstract = {Any modeler who has attempted to reproduce a spiking neural network model from its description in a paper has discovered what a painful endeavor this is. Even when all parameters appear to have been specified, which is rare, typically the initial attempt to reproduce the network does not yield results that are recognizably akin to those in the original publication. Causes include inaccurately reported or hidden parameters (e.g., wrong unit or the existence of an initialization distribution), differences in implementation of model dynamics, and ambiguities in the text description of the network experiment. The very fact that adequate reproduction often cannot be achieved until a series of such causes have been tracked down and resolved is in itself disconcerting, as it reveals unreported model dependencies on specific implementation choices that either were not clear to the original authors, or that they chose not to disclose. In either case, such dependencies diminish the credibility of the model's claims about the behavior of the target system. To demonstrate these issues, we provide a worked example of reproducing a seminal study for which, unusually, source code was provided at time of publication. Despite this seemingly optimal starting position, reproducing the results was time consuming and frustrating. Further examination of the correctly reproduced model reveals that it is highly sensitive to implementation choices such as the realization of background noise, the integration timestep, and the thresholding parameter of the analysis algorithm. From this process, we derive a guideline of best practices that would substantially reduce the investment in reproducing neural network studies, whilst simultaneously increasing their scientific quality. We propose that this guideline can be used by authors and reviewers to assess and improve the reproducibility of future network models.},
	urldate = {2021-10-21},
	journal = {Frontiers in Neuroinformatics},
	author = {Pauli, Robin and Weidel, Philipp and Kunkel, Susanne and Morrison, Abigail},
	year = {2018},
	note = {00000},
	pages = {46},
}

@article{ghosh_synchronization_2021,
	title = {Synchronization in time-varying networks},
	url = {http://arxiv.org/abs/2109.07618},
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	urldate = {2021-09-21},
	journal = {arXiv:2109.07618 [physics]},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	month = sep,
	year = {2021},
	note = {00000 
arXiv: 2109.07618},
}

@article{eurich_dynamics_1999,
	title = {Dynamics of {Self}-{Organized} {Delay} {Adaptation}},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1594},
	doi = {10/dj9c7q},
	abstract = {Adaptation of interaction delays is essential for the functioning of many natural and technical systems. We introduce a novel framework for studying the dynamics of delay adaptation in systems which optimize coincidence of inputs. For the important case of periodically modulated input we derive conditions for the existence and stability of solutions which constrain the set of mechanisms for reliable delay adaptation. Using numerical examples we show that our approach is applicable to more general than periodic input patterns such as Poissonian point processes with coordinated rate fluctuations.},
	number = {7},
	urldate = {2021-09-16},
	journal = {Physical Review Letters},
	author = {Eurich, Christian W. and Pawelzik, Klaus and Ernst, Udo and Cowan, Jack D. and Milton, John G.},
	month = feb,
	year = {1999},
	note = {00082 
Publisher: American Physical Society},
	pages = {1594--1597},
}

@article{izhikevich_polychronization_2006,
	title = {Polychronization: {Computation} with {Spikes}},
	volume = {18},
	issn = {0899-7667},
	shorttitle = {Polychronization},
	url = {https://doi.org/10.1162/089976606775093882},
	doi = {10/bgh4qv},
	abstract = {We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as “attention to memories.”},
	number = {2},
	urldate = {2018-09-24},
	journal = {Neural Computation},
	author = {Izhikevich, Eugene M.},
	month = feb,
	year = {2006},
	note = {00000},
	pages = {245--282},
}

@article{sun_learning_2016,
	title = {Learning polychronous neuronal groups using joint weight-delay spike-timing-dependent plasticity},
	volume = {28},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00879},
	doi = {10.1162/NECO_a_00879},
	abstract = {Polychronous neuronal group (PNG), a type of cell assembly, is one of the putative mechanisms for neural information representation. According to the reader-centric definition, some readout neurons can become selective to the information represented by polychronous neuronal groups under ongoing activity. Here, in computational models, we show that the frequently activated polychronous neuronal groups can be learned by readout neurons with joint weight-delay spike-timing-dependent plasticity. The identity of neurons in the group and their expected spike timing at millisecond scale can be recovered from the incoming weights and delays of the readout neurons. The detection performance can be further improved by two layers of readout neurons. In this way, the detection of polychronous neuronal groups becomes an intrinsic part of the network, and the readout neurons become differentiated members in the group to indicate whether subsets of the group have been activated according to their spike timing. The readout spikes representing this information can be used to analyze how PNGs interact with each other or propagate to downstream networks for higher-level processing.},
	number = {10},
	journal = {Neural Computation},
	author = {Sun, Haoqi and Sourina, Olga and Huang, Guang-Bin},
	month = oct,
	year = {2016},
	note = {tex.eprint: https://direct.mit.edu/neco/article-pdf/28/10/2181/972099/neco{\textbackslash}\_a{\textbackslash}\_00879.pdf},
	pages = {2181--2212},
}

@article{kirchner_ultra-rapid_2006,
	title = {Ultra-rapid object detection with saccadic eye movements: {Visual} processing speed revisited},
	volume = {46},
	issn = {0042-6989},
	shorttitle = {Ultra-rapid object detection with saccadic eye movements},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698905005110},
	doi = {10.1016/j.visres.2005.10.002},
	abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	number = {11},
	journal = {Vision Research},
	author = {Kirchner, H and Thorpe, Sj},
	year = {2006},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Kirchner06},
	pages = {1762--76},
}

@article{muller_cortical_2018,
	title = {Cortical travelling waves: {Mechanisms} and computational principles},
	issn = {1471-003X},
	shorttitle = {Cortical travelling waves},
	url = {http://www.nature.com/doifinder/10.1038/nrn.2018.20},
	doi = {10.1038/nrn.2018.20},
	abstract = {Advanced recording techniques have enabled the identification of travelling waves of neuronal activity in different areas of the cortex. Sejnowski and colleagues review these findings, consider the mechanisms by which travelling waves are generated and evaluate their possible roles in cortical function.},
	journal = {Nature Reviews Neuroscience},
	author = {Muller, Lyle and Chavane, Frédéric and Reynolds, John and Sejnowski, Terrence J.},
	month = mar,
	year = {2018},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Muller18},
}

@article{muller_stimulus-evoked_2014,
	title = {The stimulus-evoked population response in visual cortex of awake monkey is a propagating wave},
	volume = {5},
	issn = {2041-1723},
	doi = {10.1038/ncomms4675},
	abstract = {Propagating waves occur in many excitable media and were recently found in neural systems from retina to neocortex. While propagating waves are clearly present under anaesthesia, whether they also appear during awake and conscious states remains unclear. One possibility is that these waves are systematically missed in trial-averaged data, due to variability. Here we present a method for detecting propagating waves in noisy multichannel recordings. Applying this method to single-trial voltage-sensitive dye imaging data, we show that the stimulus-evoked population response in primary visual cortex of the awake monkey propagates as a travelling wave, with consistent dynamics across trials. A network model suggests that this reliability is the hallmark of the horizontal fibre network of superficial cortical layers. Propagating waves with similar properties occur independently in secondary visual cortex, but maintain precise phase relations with the waves in primary visual cortex. These results show that, in response to a visual stimulus, propagating waves are systematically evoked in several visual areas, generating a consistent spatiotemporal frame for further neuronal interactions.},
	journal = {Nature Communications},
	author = {Muller, Lyle and Reynaud, Alexandre and Chavane, Frédéric and Destexhe, Alain},
	year = {2014},
	note = {00068
Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Muller14},
	pages = {3675},
}

@article{kaplan_anisotropic_2013,
	title = {Anisotropic connectivity implements motion-based prediction in a spiking neural network},
	volume = {7},
	url = {https://laurentperrinet.github.io/publication/kaplan-13},
	doi = {10.3389/fncom.2013.00112},
	abstract = {Predictive coding hypothesizes that the brain explicitly infers upcoming sensory input to establish a coherent representation of the world. Although it is becoming generally accepted, it is not clear on which level spiking neural networks may implement predictive coding and what function their connectivity may have. We present a network model of conductance-based integrate-and-fire neurons inspired by the architecture of retinotopic cortical areas that assumes predictive coding is implemented through network connectivity, namely in the connection delays and in selectiveness for the tuning properties of source and target cells. We show that the applied connection pattern leads to motion-based prediction in an experiment tracking a moving dot. In contrast to our proposed model, a network with random or isotropic connectivity fails to predict the path when the moving dot disappears. Furthermore, we show that a simple linear decoding approach is sufficient to transform neuronal spiking activity into a probabilistic estimate for reading out the target trajectory.},
	number = {112},
	journal = {Frontiers in Computational Neuroscience},
	author = {Kaplan, Bernhard A and Lansner, Anders and Masson, Guillaume S and Perrinet, Laurent U},
	month = sep,
	year = {2013},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Kaplan13},
}

@article{chemla_suppressive_2019,
	title = {Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey {V1}},
	volume = {2792},
	url = {http://www.jneurosci.org/content/early/2019/03/18/JNEUROSCI.2792-18.2019},
	doi = {10.1523/JNEUROSCI.2792-18.2019},
	abstract = {The “apparent motion” illusion is evoked when stationary stimuli are successively flashed in spatially separated positions. It depends on the precise spatial and temporal separations of the stimuli. For large spatiotemporal separation, the long-range apparent motion (lrAM), it remains unclear how the visual system computes unambiguous motion signals. Here we investigated whether intracortical interactions within retinotopic maps could shape a global motion representation at the level of V1 population in response to a lrAM. In fixating monkeys, voltage-sensitive dye imaging revealed the emergence of a spatio-temporal representation of the motion trajectory at the scale of V1 population activity, shaped by systematic backward suppressive waves. We show that these waves are the expected emergent property of a recurrent gain control fed by the horizontal intra-cortical network. Such non-linearities explain away ambiguous correspondence problems of the stimulus along the motion path, preformating V1 population response for an optimal read-out by downstream areas.},
	urldate = {2018-07-27},
	journal = {Journal of Neuroscience},
	author = {Chemla, Sandrine and Reynaud, Alexandre and diVolo, Matteo and Zerlaut, Yann and Perrinet, Laurent U and Destexhe, Alain and Chavane, Frédéric Y},
	month = mar,
	year = {2019},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Chemla19},
	pages = {18},
}

@article{bringuier_horizontal_1999,
	title = {Horizontal {Propagation} of {Visual} {Activity} in the {Synaptic} {Integration} {Field} of {Area} 17 {Neurons}},
	volume = {283},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/283/5402/695},
	doi = {10.1126/science.283.5402.695},
	abstract = {The receptive field of a visual neuron is classically defined as the region of space (or retina) where a visual stimulus evokes a change in its firing activity. At the cortical level, a challenging issue concerns the roles of feedforward, local recurrent, intracortical, and cortico-cortical feedback connectivity in receptive field properties. Intracellular recordings in cat area 17 showed that the visually evoked synaptic integration field extends over a much larger area than that established on the basis of spike activity. Synaptic depolarizing responses to stimuli flashed at increasing distances from the center of the receptive field decreased in strength, whereas their onset latency increased. These findings suggest that subthreshold responses in the unresponsive region surrounding the classical discharge field result from the integration of visual activation waves spread by slowly conducting horizontal axons within primary visual cortex.},
	number = {5402},
	urldate = {2019-02-07},
	journal = {Science},
	author = {Bringuier, Vincent and Chavane, Frédéric and Glaeser, Larry and Frégnac, Yves},
	month = jan,
	year = {1999},
	note = {00535
Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Bringuier99},
	pages = {695--699},
}

@article{pastalkova_internally_2008,
	title = {Internally {Generated} {Cell} {Assembly} {Sequences} in the {Rat} {Hippocampus}},
	volume = {321},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	doi = {10.1126/science.1159775},
	abstract = {A longstanding conjecture in neuroscience is that aspects of cognition depend on the brain's ability to self-generate sequential neuronal activity. We found that reliably and continually-changing cell assemblies in the rat hippocampus appeared not only during spatial navigation but also in the absence of changing environmental or body-derived inputs. During the delay period of a memory task each moment in time was characterized by the activity of a unique assembly of neurons. Identical initial conditions triggered a similar assembly sequence, whereas different conditions gave rise, uniquely, to different sequences, thereby predicting behavioral choices, including errors. Such sequences were not formed in control, non-memory, tasks. We hypothesize that neuronal representations, evolved for encoding distance in spatial navigation, also support episodic recall and the planning of action sequences.},
	number = {5894},
	urldate = {2022-02-23},
	journal = {Science (New York, N.Y.)},
	author = {Pastalkova, Eva and Itskov, Vladimir and Amarasingham, Asohan and Buzsáki, György},
	month = sep,
	year = {2008},
	pages = {1322--1327},
}

@article{riehle_spike_1997,
	title = {Spike synchronization and rate modulation differentially involved in motor cortical function},
	volume = {278},
	doi = {10.1126/science.278.5345.1950},
	number = {5345},
	journal = {Science (New York, N.Y.)},
	author = {Riehle, Alexa and Grun, Sonja and Diesmann, Markus and Aertsen, Ad},
	year = {1997},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1950--1953},
}

@article{gutig_spike_2014,
	series = {Theoretical and computational neuroscience},
	title = {To spike, or when to spike?},
	volume = {25},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438814000129},
	doi = {10.1016/j.conb.2014.01.004},
	abstract = {Recent experimental reports have suggested that cortical networks can operate in regimes were sensory information is encoded by relatively small populations of spikes and their precise relative timing. Combined with the discovery of spike timing dependent plasticity, these findings have sparked growing interest in the capabilities of neurons to encode and decode spike timing based neural representations. To address these questions, a novel family of methodologically diverse supervised learning algorithms for spiking neuron models has been developed. These models have demonstrated the high capacity of simple neural architectures to operate also beyond the regime of the well established independent rate codes and to utilize theoretical advantages of spike timing as an additional coding dimension.},
	language = {en},
	urldate = {2022-05-13},
	journal = {Current Opinion in Neurobiology},
	author = {Gütig, Robert},
	month = apr,
	year = {2014},
	pages = {134--139},
}

@article{luczak_sequential_2007,
	title = {Sequential structure of neocortical spontaneous activity in vivo},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/1/347},
	doi = {10.1073/pnas.0605643104},
	abstract = {Even in the absence of sensory stimulation, the neocortex shows complex spontaneous activity patterns, often consisting of alternating “DOWN” states of generalized neural silence and “UP” states of massive, persistent network activity. To investigate how this spontaneous activity propagates through neuronal assemblies in vivo, we simultaneously recorded populations of 50–200 cortical neurons in layer V of anesthetized and awake rats. Each neuron displayed a virtually unique spike pattern during UP states, with diversity seen amongst both putative pyramidal cells and interneurons, reflecting a complex but stereotypically organized sequential spread of activation through local cortical networks. Spike timing was most precise during the first ≈100 ms after UP state onset, and decayed as UP states progressed. A subset of UP states propagated as traveling waves, but waves passing a given point in either direction initiated similar local sequences, suggesting local networks as the substrate of sequential firing patterns. A search for repeating motifs indicated that their occurrence and structure was predictable from neurons' individual latencies to UP state onset. We suggest that these stereotyped patterns arise from the interplay of intrinsic cellular conductances and local circuit properties.},
	language = {en},
	number = {1},
	urldate = {2022-02-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Luczak, Artur and Barthó, Peter and Marguet, Stephan L. and Buzsáki, György and Harris, Kenneth D.},
	month = jan,
	year = {2007},
	keywords = {microcircuits, neuronal assembly, repeating sequences, slow oscillations, syntire chains},
	pages = {347--352},
}

@article{bohte_evidence_2004,
	title = {The evidence for neural information processing with precise spike-times: {A} survey},
	volume = {3},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	number = {2},
	journal = {Natural Computing},
	author = {Bohte, Sander M},
	year = {2004},
	pages = {195--206},
}

@article{davis_spontaneous_2021,
	title = {Spontaneous traveling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states},
	volume = {12},
	doi = {10.1038/s41467-021-26175-1},
	number = {1},
	journal = {Nature Communications},
	author = {Davis, Zachary W and Benigno, Gabriel B and Fletterman, Charlee and Desbordes, Theo and Steward, Christopher and Sejnowski, Terrence J and H Reynolds, John and Muller, Lyle},
	year = {2021},
	pages = {1--16},
}

@article{perrinet_active_2014,
	title = {Active inference, eye movements and oculomotor delays},
	volume = {108},
	copyright = {All rights reserved},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-014-0620-8},
	doi = {10.1007/s00422-014-0620-8},
	abstract = {This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.},
	number = {6},
	journal = {Biological Cybernetics},
	author = {Perrinet, Laurent U and Adams, Rick A and Friston, Karl J},
	month = dec,
	year = {2014},
	keywords = {Active inference, Bayesian model, Biologically Inspired Computer vision, Generalised coordinates, Oculomotor delays, Smooth pursuit eye movements, Tracking eye movements, Variational free energy, active inference, active-inference, bayesian, bicv-motion, bicv-sparse, delays, eye, eye movements, eye-movements, free energy, free-energy, generalized-coordinates, generalized-filtering, motion detection, oculomotor, perception, perrinetadamsfriston14, smooth-pursuit, tracking-eye-movements, variational-filtering},
	pages = {777--801},
}

@article{villette_internally_2015,
	title = {Internally {Recurring} {Hippocampal} {Sequences} as a {Population} {Template} of {Spatiotemporal} {Information}},
	volume = {88},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627315008417},
	doi = {10/f7whnn},
	abstract = {The hippocampus is essential for spatiotemporal cognition. Sequences of neuronal activation provide a substrate for this fundamental function. At the behavioral timescale, these sequences have been shown to occur either in the presence of successive external landmarks or through internal mechanisms within an episodic memory task. In both cases, activity is externally constrained by the organization of the task and by the size of the environment explored. Therefore, it remains unknown whether hippocampal activity can self-organize into a default mode in the absence of any external memory demand or spatiotemporal boundary. Here we show that, in the presence of self-motion cues, a population code integrating distance naturally emerges in the hippocampus in the form of recurring sequences. These internal dynamics clamp spontaneous travel since run distance distributes into integer multiples of the span of these sequences. These sequences may thus guide navigation when external landmarks are reduced.},
	language = {en},
	number = {2},
	urldate = {2022-01-17},
	journal = {Neuron},
	author = {Villette, Vincent and Malvache, Arnaud and Tressard, Thomas and Dupuy, Nathalie and Cossart, Rosa},
	month = oct,
	year = {2015},
	note = {00085},
	pages = {357--366},
}

@incollection{paugam-moisy_computing_2012,
	title = {Computing with spiking neuron networks},
	booktitle = {Handbook of natural computing},
	publisher = {Springer-Verlag},
	author = {Paugam-Moisy, Hélène and Bohte, Sander M.},
	month = sep,
	year = {2012},
}

@article{haimerl_internal_2019,
	title = {Internal representation of hippocampal neuronal population spans a time-distance continuum},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/15/7477},
	doi = {10.1073/pnas.1718518116},
	abstract = {The hippocampus plays a critical role in episodic memory: the sequential representation of visited places and experienced events. This function is mirrored by hippocampal activity that self organizes into sequences of neuronal activation that integrate spatiotemporal information. What are the underlying mechanisms of such integration is still unknown. Single cell activity was recently shown to combine time and distance information; however, it remains unknown whether a degree of tuning between space and time can be defined at the network level. Here, combining daily calcium imaging of CA1 sequence dynamics in running head-fixed mice and network modeling, we show that CA1 network activity tends to represent a specific combination of space and time at any given moment, and that the degree of tuning can shift within a continuum from 1 day to the next. Our computational model shows that this shift in tuning can happen under the control of the external drive power. We propose that extrinsic global inputs shape the nature of spatiotemporal integration in the hippocampus at the population level depending on the task at hand, a hypothesis which may guide future experimental studies.},
	language = {en},
	number = {15},
	urldate = {2022-01-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Haimerl, Caroline and Angulo-Garcia, David and Villette, Vincent and Reichinnek, Susanne and Torcini, Alessandro and Cossart, Rosa and Malvache, Arnaud},
	month = apr,
	year = {2019},
	keywords = {attractor network, hippocampus, neural model, space representation, time representation},
	pages = {7477--7482},
}

@article{malvache_awake_2016,
	title = {Awake hippocampal reactivations project onto orthogonal neuronal assemblies},
	volume = {353},
	issn = {1095-9203},
	doi = {10/bqpq},
	abstract = {The chained activation of neuronal assemblies is thought to support major cognitive processes, including memory. In the hippocampus, this is observed during population bursts often associated with sharp-wave ripples, in the form of an ordered reactivation of neurons. However, the organization and lifetime of these assemblies remain unknown. We used calcium imaging to map patterns of synchronous neuronal activation in the CA1 region of awake mice during runs on a treadmill. The patterns were composed of the recurring activation of anatomically intermingled, but functionally orthogonal, assemblies. These assemblies reactivated discrete temporal segments of neuronal sequences observed during runs and could be stable across consecutive days. A binding of these assemblies into longer chains revealed temporally ordered replay. These modules may represent the default building blocks for encoding or retrieving experience.},
	language = {eng},
	number = {6305},
	journal = {Science (New York, N.Y.)},
	author = {Malvache, Arnaud and Reichinnek, Susanne and Villette, Vincent and Haimerl, Caroline and Cossart, Rosa},
	month = sep,
	year = {2016},
	pmid = {27634534},
	note = {00105 },
	keywords = {Animals, Brain Mapping, CA1 Region, Hippocampal, Calcium Signaling, Exercise Test, Male, Mice, Nerve Net, Neurons, Running, Wakefulness},
	pages = {1280--1283},
}

@article{khoei_flash-lag_2017,
	title = {The {Flash}-{Lag} {Effect} as a {Motion}-{Based} {Predictive} {Shift}},
	volume = {13},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	doi = {10.1371/journal.pcbi.1005068},
	abstract = {Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object’s motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects’ position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation at the present time of spatial information in the visual pathways.},
	language = {en},
	number = {1},
	urldate = {2022-08-31},
	journal = {PLOS Computational Biology},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	month = jan,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Coding mechanisms, Extrapolation, Motion, Psychophysics, Sensory perception, Velocity, Vision, Visual system},
	pages = {e1005068},
}

@article{abeles_role_1982,
	title = {Role of the cortical neuron: integrator or coincidence detector?},
	volume = {18},
	number = {1},
	journal = {Israel journal of medical sciences},
	author = {Abeles, Moshe},
	year = {1982},
	pages = {83--92},
}

@article{benosman_asynchronous_2012,
	title = {Asynchronous frameless event-based optical flow},
	volume = {27},
	url = {https://doi.org/10/b55t75},
	doi = {10.1016/j.neunet.2011.11.001},
	abstract = {This paper introduces a process to compute optical flow using an asynchronous event-based retina at high speed and low computational load. A new generation of artificial vision sensors has now started to rely on biologically inspired designs for light acquisition. Biological retinas, and their artificial counterparts, are totally asynchronous and data driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework for processing visual data using asynchronous event-based acquisition, providing a method for the evaluation of optical flow. The paper shows that current limitations of optical flow computation can be overcome by using event-based visual acquisition, where high data sparseness and high temporal resolution permit the computation of optical flow with micro-second accuracy and at very low computational cost.},
	language = {english},
	journal = {Neural Networks},
	author = {Benosman, Ryad},
	year = {2012},
	keywords = {Asynchronous acquisition, Event-based vision, Frameless vision, Optical flow, Spikes, Temporal dynamics},
	pages = {6},
}

@article{lagorce_hots_2017,
	title = {{HOTS}: {A} {Hierarchy} of {Event}-{Based} {Time}-{Surfaces} for {Pattern} {Recognition}},
	volume = {39},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2016.2574707},
	abstract = {This paper describes novel event-based spatiotemporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving near 100\% accuracy on each. We introduce a new 7 class moving face recognition task, achieving 79\% accuracy.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lagorce, Xavier and Orchard, Garrick and Galluppi, Francesco and Shi, Bertram E. and Benosman, Ryad B.},
	year = {2017},
	keywords = {Neuromorphic sensing, event-based vision, feature extraction},
	pages = {1346--1359},
}

@article{grimaldi_robust_2022,
	title = {A robust event-driven approach to always-on object recognition},
	doi = {10.36227/techrxiv.18003077.v1},
	abstract = {We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization.},
	urldate = {2022-01-13},
	journal = {TechRxiv preprint},
	author = {Grimaldi, Antoine and Boutin, Victor and Ieng, Sio-Hoi and Benosman, Ryad and Perrinet, Laurent U},
	month = jan,
	year = {2022},
	keywords = {efficient coding, event-based vision, homeostasis, neuromorphic hardware, online classification},
}

@article{hogendoorn_predictive_2019,
	title = {Predictive {Coding} with {Neural} {Transmission} {Delays}: {A} {Real}-{Time} {Temporal} {Alignment} {Hypothesis}},
	volume = {6},
	issn = {2373-2822},
	shorttitle = {Predictive {Coding} with {Neural} {Transmission} {Delays}},
	url = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	doi = {10.1523/eneuro.0412-18.2019},
	language = {en},
	number = {2},
	urldate = {2019-11-12},
	journal = {eneuro},
	author = {Hogendoorn, Hinze and Burkitt, Anthony N.},
	month = mar,
	year = {2019},
	pages = {ENEURO.0412--18.2019},
}

@article{perrinet_motion-based_2012,
	title = {Motion-{Based} {Prediction} {Is} {Sufficient} to {Solve} the {Aperture} {Problem}},
	volume = {24},
	copyright = {All rights reserved},
	issn = {0899-7667},
	doi = {10.1162/neco_a_00332},
	abstract = {In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to physio-logy and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independent of their texture. Second, we observe that incoherent features are explained away, while coherent information diffuses progressively to the global scale. Most previous models included ad hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features as necessary conditions to solve the aperture problem. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This solution may give insights into the role of prediction underlying a large class of sensory computations.},
	number = {10},
	journal = {Neural Computation},
	author = {Perrinet, Laurent U. and Masson, G.S. Guillaume S.},
	month = aug,
	year = {2012},
	keywords = {Bayesian model, aperture, aperture problem, aperture-problem, association field, coding, emergence, khoei12jpp, khoei13jpp, motion detection, motion prediction, perrinet12pred, predictive, predictive coding, predictive-coding, probabilistic, probabilistic representation, problem, representation, thesis, toupate-inpress},
	pages = {2726--2750},
}

@article{bohte_error-backpropagation_2002,
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	volume = {48},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	doi = {10.1016/S0925-2312(01)00658-0},
	abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
	language = {en},
	number = {1},
	urldate = {2022-09-28},
	journal = {Neurocomputing},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutré, Han},
	month = oct,
	year = {2002},
	keywords = {Error-backpropagation, Spiking neurons, Temporal coding},
	pages = {17--37},
}

@article{dardelet_event-by-event_2021,
	title = {An {Event}-by-{Event} {Feature} {Detection} and {Tracking} {Invariant} to {Motion} {Direction} and {Velocity}},
	doi = {10.36227/techrxiv.17013824.v1},
	abstract = {Contour velocity estimation and tracking from a fully event-based perspective.},
	language = {en},
	urldate = {2022-09-28},
	author = {Dardelet, Laurent and Benosman, Ryad and Ieng, Sio-Hoi},
	month = nov,
	year = {2021},
}

@article{zenke_remarkable_2021,
	title = {The {Remarkable} {Robustness} of {Surrogate} {Gradient} {Learning} for {Instilling} {Complex} {Function} in {Spiking} {Neural} {Networks}},
	volume = {33},
	issn = {0899-7667},
	doi = {10.1162/neco_a_01367},
	abstract = {Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.},
	number = {4},
	urldate = {2021-12-02},
	journal = {Neural Computation},
	author = {Zenke, Friedemann and Vogels, Tim P.},
	month = mar,
	year = {2021},
	pages = {899--925},
}

@article{delorme_ultra-rapid_2000,
	title = {Ultra-rapid categorisation of natural scenes does not rely on colour cues: a study in monkeys and humans},
	volume = {40},
	issn = {0042-6989},
	shorttitle = {Ultra-rapid categorisation of natural scenes does not rely on colour cues},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698900000833},
	doi = {10.1016/S0042-6989(00)00083-3},
	abstract = {In a rapid categorisation task, monkeys and humans had to detect a target (animal or food) in briefly flashed (32 ms) and previously unseen natural images. Removing colour cues had very little effect on average performance. Impairments were restricted to a mild accuracy drop (in some human subjects) and a small reaction time mean increase (10–15 ms) observed both in monkeys and humans but only in the detection of food targets. In both tasks, accuracy and latency of the fastest behavioural responses were unaffected, suggesting that such ultra-rapid categorisations could depend on feed-forward processing of early coarse achromatic magnocellular information.},
	language = {en},
	number = {16},
	urldate = {2022-09-21},
	journal = {Vision Research},
	author = {Delorme, A and Richard, G and Fabre-Thorpe, M},
	month = jul,
	year = {2000},
	keywords = {Categorisation, Colour, Natural scenes, Primate, Visual processing},
	pages = {2187--2200},
}

@article{madadi_asl_delay-dependent_2022,
	title = {Delay-dependent transitions of phase synchronization and coupling symmetry between neurons shaped by spike-timing-dependent plasticity},
	issn = {1871-4099},
	url = {https://doi.org/10.1007/s11571-022-09850-x},
	doi = {10.1007/s11571-022-09850-x},
	abstract = {Synchronization plays a key role in learning and memory by facilitating the communication between neurons promoted by synaptic plasticity. Spike-timing-dependent plasticity (STDP) is a form of synaptic plasticity that modifies the strength of synaptic connections between neurons based on the coincidence of pre- and postsynaptic spikes. In this way, STDP simultaneously shapes the neuronal activity and synaptic connectivity in a feedback loop. However, transmission delays due to the physical distance between neurons affect neuronal synchronization and the symmetry of synaptic coupling. To address the question that how transmission delays and STDP can jointly determine the emergent pairwise activity-connectivity patterns, we studied phase synchronization properties and coupling symmetry between two bidirectionally coupled neurons using both phase oscillator and conductance-based neuron models. We show that depending on the range of transmission delays, the activity of the two-neuron motif can achieve an in-phase/anti-phase synchronized state and its connectivity can attain a symmetric/asymmetric coupling regime. The coevolutionary dynamics of the neuronal system and the synaptic weights due to STDP stabilizes the motif in either one of these states by transitions between in-phase/anti-phase synchronization states and symmetric/asymmetric coupling regimes at particular transmission delays. These transitions crucially depend on the phase response curve (PRC) of the neurons, but they are relatively robust to the heterogeneity of transmission delays and potentiation-depression imbalance of the STDP profile.},
	language = {en},
	urldate = {2022-09-18},
	journal = {Cognitive Neurodynamics},
	author = {Madadi Asl, Mojtaba and Ramezani Akbarabadi, Saeideh},
	month = jul,
	year = {2022},
	keywords = {Coupling symmetry, Spike-timing-dependent plasticity, Synaptic plasticity, Synchronization, Transmission delay},
}


@book{Abeles91,
	address = {Cambridge ; New York},
	author = {Abeles, Moshe},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	isbn = {978-0-521-37476-7},
	publisher = {Cambridge University Press},
	shorttitle = {Corticonics},
	title = {Corticonics: neural circuits of the cerebral cortex},
	year = {1991}}

@article{Abeles82,
	author = {Abeles, Moshe},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {Israel journal of medical sciences},
	number = {1},
	pages = {83--92},
	title = {Role of the cortical neuron: integrator or coincidence detector?},
	volume = {18},
	year = {1982}}

@article{Adrian26,
	abstract = {Images
null},
	author = {Adrian, E. D. and Zotterman, Yngve},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {0022-3751},
	journal = {The Journal of Physiology},
	month = aug,
	number = {4},
	pages = {465--483},
	pmcid = {PMC1514868},
	pmid = {16993807},
	title = {The impulses produced by sensory nerve endings},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1514868/},
	urldate = {2022-10-10},
	volume = {61},
	year = {1926},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1514868/}}

@article{Agus10,
	author = {Agus, Trevor R. and Thorpe, Simon J. and Pressnitzer, Daniel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuron.2010.04.014},
	journal = {Neuron},
	language = {en},
	number = {4},
	pages = {610--618},
	title = {Rapid {Formation} of {Robust} {Auditory} {Memories}: {Insights} from {Noise}},
	url = {https://doi.org/dc3r2d},
	volume = {66},
	year = {2010},
	bdsk-url-1 = {https://doi.org/dc3r2d},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2010.04.014}}

@inproceedings{Arnold21,
	abstract = {Noise and temporal dynamics are ubiquitous in neural systems yet the computational consequences of these two phenomena interacting are not well studied. Temporal dynamics in spiking networks are often considered only implicitly as part of membrane time constants or synaptic transfer functions. We explicitly model temporal structure using plastic conduction delays between neuron's and characterise the influence of different kinds of noise on learning including temporal jitter, dropout, pattern size, and pattern presentation frequency. We simplify the conduction delay plasticity (CDP) rule called synaptic delay variance learning (SDVL) and demonstrate it is robust to several kinds of noise including; internal pattern jitter, number of pattern afferents, pattern presentation rate, and pattern spike dropout. In particular, after unsupervised training the simplified version of SDVL can achieve an accuracy of up to 99.7 percent averaged over 100 trials. These results demonstrate that learning algorithms based on explicitly modelling temporal structure in inputs can be functional and robust for unsupervised learning of spatiotemporal patterns across a range of noise conditions.},
	author = {Arnold, Joshua and Stratton, Peter and Wiles, Janet},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/IJCNN52387.2021.9533934},
	keywords = {Biological systems, Conduction delay, Delays, Jitter, Noise measurement, Spatiotemporal phenomena, Training, Transfer functions, delay learning, noise robust, plasticity, spiking neural network, ⛔ No INSPIRE recid found},
	month = jul,
	note = {ISSN: 2161-4407},
	pages = {1--10},
	title = {Conduction delay plasticity can robustly learn spatiotemporal patterns embedded in noise},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN52387.2021.9533934}}

@article{Aronov04,
	abstract = {Quantifying the dissimilarity (or distance) between two sequences is essential to the study of action potential (spike) trains in neuroscience and genetic sequences in molecular biology. In neuroscience, traditional methods for sequence comparisons rely on techniques appropriate for multivariate data, which typically assume that the space of sequences is intrinsically Euclidean. More recently, metrics that do not make this assumption have been introduced for comparison of neural activity patterns. These metrics have a formal resemblance to those used in the comparison of genetic sequences. Yet the relationship between such metrics and the traditional Euclidean distances has remained unclear. We show, both analytically and computationally, that the geometries associated with metric spaces of event sequences are intrinsically non-Euclidean. Our results demonstrate that metric spaces enrich the study of neural activity patterns, since accounting for perceptual spaces requires a non-Euclidean geometry.},
	author = {Aronov, Dmitriy and Victor, Jonathan D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1103/PhysRevE.69.061905},
	journal = {Physical Review E},
	month = jun,
	note = {Publisher: American Physical Society},
	number = {6},
	pages = {061905},
	title = {Non-{Euclidean} properties of spike train metric spaces},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.69.061905},
	urldate = {2022-10-17},
	volume = {69},
	year = {2004},
	bdsk-url-1 = {https://link.aps.org/doi/10.1103/PhysRevE.69.061905},
	bdsk-url-2 = {https://doi.org/10.1103/PhysRevE.69.061905}}

@article{Aviel03,
	abstract = {{\textless}jats:p{\textgreater} We investigate the formation of synfire waves in a balanced network of integrate-and-fire neurons. The synaptic connectivity of this network embodies synfire chains within a sparse random connectivity. This network can exhibit global oscillations but can also operate in an asynchronous activity mode. We analyze the correlations of two neurons in a pool as convenient indicators for the state of the network. We find, using different models, that these indicators depend on a scaling variable. {\textless}/jats:p{\textgreater}{\textless}jats:p{\textgreater} Beyond a critical point, strong correlations and large network oscillations are obtained. We looked for the conditions under which a synfire wave could be propagated on top of an otherwise asynchronous state of the network. This condition was found to be highly restrictive, requiring a large number of neurons for its implementation in our network. The results are based on analytic derivations and simulations. {\textless}/jats:p{\textgreater}},
	author = {Aviel, Y. and Mehring, C. and Abeles, M. and Horn, D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/089976603321780290},
	journal = {Neural Computation},
	language = {en},
	number = {6},
	pages = {1321--1340},
	title = {On {Embedding} {Synfire} {Chains} in a {Balanced} {Network}},
	url = {https://doi.org/fgj3wf},
	volume = {15},
	year = {2003},
	bdsk-url-1 = {https://doi.org/fgj3wf},
	bdsk-url-2 = {https://doi.org/10.1162/089976603321780290}}

@article{Azouz08,
	author = {Azouz, Rony and Gray, Charles M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1111/j.1460-9568.2008.06434.x},
	journal = {European Journal of Neuroscience},
	language = {en},
	month = oct,
	number = {7},
	pages = {1286--1300},
	title = {Stimulus-selective spiking is driven by the relative timing of synchronous excitation and disinhibition in cat striate neurons\textit{in vivo}},
	url = {https://doi.org/cbcr8h},
	volume = {28},
	year = {2008},
	bdsk-url-1 = {https://doi.org/cbcr8h},
	bdsk-url-2 = {https://doi.org/10.1111/j.1460-9568.2008.06434.x}}

@article{Ballard11,
	abstract = {A prominent feature of signaling in cortical neurons is that of randomness in the action potential. The output of a typical pyramidal cell can be well fit with a Poisson model, and variations in the Poisson rate repeatedly have been shown to be correlated with stimuli. However while the rate provides a very useful characterization of neural spike data, it may not be the most fundamental description of the signaling code. Recent data showing γ frequency range multi-cell action potential correlations, together with spike timing dependent plasticity, are spurring a re-examination of the classical model, since precise timing codes imply that the generation of spikes is essentially deterministic. Could the observed Poisson randomness and timing determinism reflect two separate modes of communication, or do they somehow derive from a single process? We investigate in a timing-based model whether the apparent incompatibility between these probabilistic and deterministic observations may be resolved by examining how spikes could be used in the underlying neural circuits. The crucial component of this model draws on dual roles for spike signaling. In learning receptive fields from ensembles of inputs, spikes need to behave probabilistically, whereas for fast signaling of individual stimuli, the spikes need to behave deterministically. Our simulations show that this combination is possible if deterministic signals using γ latency coding are probabilistically routed through different members of a cortical cell population at different times. This model exhibits standard features characteristic of Poisson models such as orientation tuning and exponential interval histograms. In addition, it makes testable predictions that follow from the γ latency coding.},
	author = {Ballard, Dana and Jehee, Janneke},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {1662-5188},
	journal = {Frontiers in Computational Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	title = {Dual {Roles} for {Spike} {Signaling} in {Cortical} {Neural} {Populations}},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2011.00022},
	urldate = {2022-12-16},
	volume = {5},
	year = {2011},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fncom.2011.00022}}

@article{Baraban18,
	abstract = {During myelination, individual oligodendrocytes initially over-produce short myelin sheaths that are either retracted or stabilised. By live imaging oligodendrocyte Ca2+ activity in vivo, we find that high-amplitude long-duration Ca2+ transients in sheaths prefigure retractions, mediated by calpain. Following stabilisation, myelin sheaths grow along axons, and we find that higher frequency Ca2+ transient activity in sheaths precedes faster elongation. Our data implicate local Ca2+ signalling in regulating distinct stages of myelination.},
	author = {Baraban, Marion and Koudelka, Sigrid and Lyons, David A},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41593-017-0040-x},
	issn = {1097-6256},
	journal = {Nature neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	month = jan,
	number = {1},
	pages = {19--23},
	pmcid = {PMC5742537},
	pmid = {29230058},
	title = {Ca2+ activity signatures of myelin sheath formation and growth in vivo},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5742537/},
	urldate = {2022-11-13},
	volume = {21},
	year = {2018},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5742537/},
	bdsk-url-2 = {https://doi.org/10.1038/s41593-017-0040-x}}

@article{Bartolozzi07,
	abstract = {Synapses are crucial elements for computation and information transfer in both real and artificial neural systems. Recent experimental findings and theoretical models of pulse-based neural networks suggest that synaptic dynamics can play a crucial role for learning neural codes and encoding spatio-temporal spike patterns. Within the context of hardware implementations of pulse based neural networks, several analog VLSI circuits modeling synaptic functionality have been proposed. We present an overview of previously proposed circuits and describe a novel analog VLSI synaptic circuit suitable for integration in large VLSI spike-based neural systems. The circuit proposed is based on a computational model that fits the real post-synaptic currents with exponentials. We present experimental data showing how the circuit exhibits realistic dynamics and show how it can be connected to additional modules for implementing a wide range of synaptic properties.},
	author = {Bartolozzi, Chiara and Indiveri, Giacomo},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/neco.2007.19.10.2581},
	issn = {0899-7667, 1530-888X},
	journal = {Neural Computation},
	language = {en},
	month = oct,
	number = {10},
	pages = {2581--2603},
	title = {Synaptic {Dynamics} in {Analog} {VLSI}},
	url = {https://direct.mit.edu/neco/article/19/10/2581-2603/7219},
	urldate = {2022-11-10},
	volume = {19},
	year = {2007},
	bdsk-url-1 = {https://direct.mit.edu/neco/article/19/10/2581-2603/7219},
	bdsk-url-2 = {https://doi.org/10.1162/neco.2007.19.10.2581}}

@techreport{Bellec21,
	abstract = {Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.},
	author = {Bellec, Guillaume and Wang, Shuqi and Modirshanechi, Alireza and Brea, Johanni and Gerstner, Wulfram},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	month = nov,
	number = {2106.10064},
	title = {Fitting summary statistics of neural data with a differentiable spiking network simulator},
	url = {https://arxiv.org/abs/2106.10064},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.10064}}

@article{Ben-Yishai96,
	abstract = {Recent studies have shown that local cortical feedback can have an important effect on the response of neurons in primary visual cortex to the orientation of visual stimuli. In this work, we study the role of the cortical feedback in shaping the spatiotemporal patterns of activity in cortex. Two questions are addressed: one, what are the limitations on the ability of cortical neurons to lock their activity to rotating oriented stimuli within a single receptive field? Two, can the local architecture of visual cortex lead to the generation of spontaneous traveling pulses of activity? We study these issues analytically by a population-dynamic model of a hypercolumn in visual cortex. The order parameter that describes the macroscopic behavior of the network is the time-dependent population vector of the network. We first study the network dynamics under the influence of a weakly tuned input that slowly rotates within the receptive field. We show that if the cortical interactions have strong spatial modulation, the network generates a sharply tuned activity profile that propagates across the hypercolumn in a path that is completely locked to the stimulus rotation. The resultant rotating population vector maintains a constant angular lag relative to the stimulus, the magnitude of which grows with the stimulus rotation frequency. Beyond a critical frequency the population vector does not lock to the stimulus but executes a quasi-periodic motion with an average frequency that is smaller than that of the stimulus. In the second part we consider the stable intrinsic state of the cortex under the influence of isotropic stimulation. We show that if the local inhibitory feedback is sufficiently strong, the network does not settle into a stationary state but develops spontaneous traveling pulses of activity. Unlike recent models of wave propagation in cortical networks, the connectivity pattern in our model is spatially symmetric, hence the direction of propagation of these waves is arbitrary. The interaction of these waves with an external-oriented stimulus is studied. It is shown that the system can lock to a weakly tuned rotating stimulus if the stimulus frequency is close to the frequency of the intrinsic wave.},
	author = {Ben-Yishai, Rani and Hansel, David and Sompolinsky, Haim},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {Journal of Computational Neuroscience},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	language = {en},
	title = {Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs} in a {Cortical} {Network} {Module}},
	year = {1996}}

@article{Ben-yishai97,
	author = {Ben-yishai, Rani and Hansel, David},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {\#nosource, orientation selectivity, population vector, primary visual cortex, ⛔ No INSPIRE recid found},
	note = {00000},
	pages = {57--77},
	title = {Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs} in a {Cortical} {Network} {Module}},
	volume = {77},
	year = {1997}}

@article{Benjamin14,
	abstract = {In this paper, we describe the design of Neurogrid, a neuromorphic system for simulating large-scale neural models in real time. Neuromorphic systems realize the function of biological neural systems by emulating their structure. Designers of such systems face three major design choices: 1) whether to emulate the four neural elements-axonal arbor, synapse, dendritic tree, and soma-with dedicated or shared electronic circuits; 2) whether to implement these electronic circuits in an analog or digital manner; and 3) whether to interconnect arrays of these silicon neurons with a mesh or a tree network. The choices we made were: 1) we emulated all neural elements except the soma with shared electronic circuits; this choice maximized the number of synaptic connections; 2) we realized all electronic circuits except those for axonal arbors in an analog manner; this choice maximized energy efficiency; and 3) we interconnected neural arrays in a tree network; this choice maximized throughput. These three choices made it possible to simulate a million neurons with billions of synaptic connections in real time-for the first time-using 16 Neurocores integrated on a board that consumes three watts.},
	author = {Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R. and Bussat, Jean-Marie and Alvarez-Icaza, Rodrigo and Arthur, John V. and Merolla, Paul A. and Boahen, Kwabena},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/JPROC.2014.2313565},
	issn = {1558-2256},
	journal = {Proceedings of the IEEE},
	keywords = {Analog circuits, Computer architecture, Electronic circuits, Integrated circuit modeling, Nerve fibers, Neural networks, Neuroscience, Random access memory, Synchronous digital hierarchy, application specific integrated circuits, asynchronous circuits, brain modeling, computational neuroscience, interconnection networks, mixed analog-digital integrated circuits, neural network hardware, neuromorphic electronic systems, ⛔ No INSPIRE recid found},
	month = may,
	note = {Conference Name: Proceedings of the IEEE},
	number = {5},
	pages = {699--716},
	shorttitle = {Neurogrid},
	title = {Neurogrid: {A} {Mixed}-{Analog}-{Digital} {Multichip} {System} for {Large}-{Scale} {Neural} {Simulations}},
	volume = {102},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/JPROC.2014.2313565}}

@article{Benosman12,
	abstract = {This paper introduces a process to compute optical flow using an asynchronous event-based retina at high speed and low computational load. A new generation of artificial vision sensors has now started to rely on biologically inspired designs for light acquisition. Biological retinas, and their artificial counterparts, are totally asynchronous and data driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework for processing visual data using asynchronous event-based acquisition, providing a method for the evaluation of optical flow. The paper shows that current limitations of optical flow computation can be overcome by using event-based visual acquisition, where high data sparseness and high temporal resolution permit the computation of optical flow with micro-second accuracy and at very low computational cost.},
	author = {Benosman, Ryad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neunet.2011.11.001},
	journal = {Neural Networks},
	keywords = {Asynchronous acquisition, Event-based vision, Frameless vision, Optical flow, Spikes, Temporal dynamics},
	language = {english},
	pages = {6},
	title = {Asynchronous frameless event-based optical flow},
	url = {https://doi.org/10/b55t75},
	volume = {27},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10/b55t75},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2011.11.001}}

@article{Benosman14,
	abstract = {This paper introduces a new methodology to compute dense visual flow using the precise timings of spikes from an asynchronous event-based retina. Biological retinas, and their artificial counterparts, are totally asynchronous and data-driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework to estimate visual flow from the local properties of events' spatiotemporal space. We will show that precise visual flow orientation and amplitude can be estimated using a local differential approach on the surface defined by coactive events. Experimental results are presented; they show the method adequacy with high data sparseness and temporal resolution of event-based acquisition that allows the computation of motion flow with microsecond accuracy and at very low computational cost.},
	author = {Benosman, Ryad and Clercq, Charles and Lagorce, Xavier and {Sio-Hoi Ieng} and Bartolozzi, Chiara},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/tnnls.2013.2273537},
	issn = {2162-237X, 2162-2388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	language = {en},
	month = feb,
	number = {2},
	pages = {407--417},
	title = {Event-{Based} {Visual} {Flow}},
	url = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	urldate = {2022-02-01},
	volume = {25},
	year = {2014},
	bdsk-url-1 = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	bdsk-url-2 = {https://doi.org/10.1109/tnnls.2013.2273537}}

@article{Benvenuti20,
	abstract = {What are the neural mechanisms underlying motion integration of translating objects? Visual motion integration is generally conceived of as a feedforward, hierarchical, information processing. However, feedforward models fail to account for many contextual effects revealed using natural moving stimuli. In particular, a translating object evokes a sequence of transient feedforward responses in the primary visual cortex but also propagations of activity through horizontal and feedback pathways. We investigated how these pathways shape the representation of a translating bar in monkey V1. We show that, for long trajectories, spiking activity builds-up hundreds of milliseconds before the bar enters the neurons receptive fields. Using VSDI and LFP recordings guided by a phenomenological model of propagation dynamics, we demonstrate that this anticipatory response arises from the interplay between horizontal and feedback networks driving V1 neurons well ahead of their feedforward inputs. This mechanism could subtend several perceptual contextual effects observed with translating objects.},
	author = {Benvenuti, Giacomo and Chemla, Sandrine and Boonman, Arjan and Perrinet, Laurent U and Masson, Guillaume S and Chavane, Frederic},
	copyright = {All rights reserved},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1101/2020.03.26.010017},
	journal = {bioRxiv : the preprint server for biology},
	language = {english},
	month = mar,
	pages = {2020.03.26.010017},
	title = {Anticipatory responses along motion trajectories in awake monkey area {V1}},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.26.010017v1},
	urldate = {2020-03-31},
	year = {2020},
	bdsk-url-1 = {https://www.biorxiv.org/content/10.1101/2020.03.26.010017v1},
	bdsk-url-2 = {https://doi.org/10.1101/2020.03.26.010017}}

@article{Berens12,
	author = {Berens, P. and Ecker, A. S. and Cotton, R. J. and Ma, W. J. and Bethge, M. and Tolias, A. S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.1335-12.2012},
	journal = {Journal of Neuroscience},
	language = {en},
	number = {31},
	pages = {10618--10626},
	title = {A {Fast} and {Simple} {Population} {Code} for {Orientation} in {Primate} {V1}},
	url = {https://doi.org/f365rn},
	volume = {32},
	year = {2012},
	bdsk-url-1 = {https://doi.org/f365rn},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.1335-12.2012}}

@article{Bernert18,
	abstract = {Bio-inspired computing using artificial spiking neural networks promises performances outperforming currently available computational approaches. Yet, the number of applications of such networks remains limited due to the absence of generic training procedures for complex pattern recognition, which require the design of dedicated architectures for each situation. We developed a spike-timing-dependent plasticity (STDP) spiking neural network (SSN) to address spike-sorting, a central pattern recognition problem in neuroscience. This network is designed to process an extracellular neural signal in an online and unsupervised fashion. The signal stream is continuously fed to the network and processed through several layers to output spike trains matching the truth after a short learning period requiring only few data. The network features an attention mechanism to handle the scarcity of action potential occurrences in the signal, and a threshold adaptation mechanism to handle patterns with different sizes. This method outperforms two existing spike-sorting algorithms at low signal-to-noise ratio (SNR) and can be adapted to process several channels simultaneously in the case of tetrode recordings. Such attention-based STDP network applied to spike-sorting opens perspectives to embed neuromorphic processing of neural data in future brain implants.},
	author = {Bernert, Marie and Yvert, Blaise},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1142/s0129065718500594},
	issn = {0129-0657},
	journal = {International Journal of Neural Systems},
	month = dec,
	number = {08},
	pages = {1850059},
	title = {An {Attention}-{Based} {Spiking} {Neural} {Network} for {Unsupervised} {Spike}-{Sorting}},
	url = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	urldate = {2021-01-26},
	volume = {29},
	year = {2018},
	bdsk-url-1 = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	bdsk-url-2 = {https://doi.org/10.1142/s0129065718500594}}

@article{Bernert17,
	abstract = {Spike sorting is a crucial step of neural data processing widely used in neuroscience and neuroprosthetics. However, current methods remain not fully automatic and require heavy computations making them not embeddable in implantable devices. To overcome these limitations, we propose a novel method based on an artificial spiking neural network designed to process neural data online and completely automatically. An input layer continuously encodes the data stream into artificial spike trains, which are then processed by two further layers to output artificial trains of spikes reproducing the real spiking activity present in the input signal. The proposed method can be adapted to process several channels simultaneously in the case of tetrode recordings. It outperforms two existing algorithms at low SNR and has the advantage to be compatible with neuromorphic computing and the perspective of being embedded in very low-power analog systems for future implantable devices serving neurorehabilitation applications.},
	author = {Bernert, Marie and Yvert, Blaise},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1101/236224},
	language = {en},
	month = dec,
	pages = {236224},
	title = {Fully unsupervised online spike sorting based on an artificial spiking neural network},
	url = {https://www.biorxiv.org/content/10.1101/236224v1},
	urldate = {2022-04-08},
	year = {2017},
	bdsk-url-1 = {https://www.biorxiv.org/content/10.1101/236224v1},
	bdsk-url-2 = {https://doi.org/10.1101/236224}}

@article{Berry22,
	abstract = {This archive contains spike trains simultaneously recorded from ganglion cells in the tiger salamander retina with a multi-electrode array while viewing a repeated natural movie clip.  These data have been analyzed in previous papers, notably Puchalla et al. Neuron 2005 and Schneidman et al. Nature 2006.},
	author = {Berry, Michael J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.34770/V0V4-3H52},
	language = {en\_US},
	month = mar,
	title = {Spike {Trains} of {Retinal} {Ganglion} {Cells} {Viewing} a {Repeated} {Natural} {Movie}},
	url = {https://dataspace.princeton.edu/handle/88435/dsp015425kd84r},
	urldate = {2022-10-04},
	year = {2022},
	bdsk-url-1 = {https://dataspace.princeton.edu/handle/88435/dsp015425kd84r},
	bdsk-url-2 = {https://doi.org/10.34770/V0V4-3H52}}

@article{Bienenstock95,
	abstract = {Prompted by considerations about (i) the compositionality of cognitive functions, (ii) the physiology of individual cortical neurons, (iii) the role of accurately timed spike patterns in cortex, and (iv) the regulation of global cortical activity, we suggest that the dynamics of cortex on the 1-ms time scale may be described as the activation of circuits of the synfire-chain type (Abeles 1982, 1991). We suggest that the fundamental computational unit in cortex may be a wave-like spatio-temporal pattern of synfire type, and that the binding mechanism underlying compositionality in cognition may be the accurate synchronization of synfire waves that propagate simultaneously on distinct, weakly coupled, synfire chains. We propose that Hebbian synaptic plasticity may result in a superposition of synfire chains in cortical connectivity, whereby a given neuron participates in many distinct chains. We investigate the behaviour of a much-simplified model of cortical dynamics devised along these principles. Calculations and numerical experiments are performed based on an assumption of randomness of stored chains, in the style of statistical physics. It is demonstrated that: (i) there exists a critical value for the total length of stored chains; (ii) this storage capacity is linear in the network's size; (iii) the behaviour of the network around the critical point is characterized by the self-regulation of the number of synfire waves coactive in the network at any given time.},
	author = {Bienenstock, Elie},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1088/0954-898X_6_2_004},
	issn = {0954-898X},
	journal = {Network: Computation in Neural Systems},
	keywords = {⛔ No INSPIRE recid found},
	month = jan,
	note = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1088/0954-898X\_6\_2\_004},
	number = {2},
	pages = {179--224},
	title = {A model of neocortex},
	url = {https://doi.org/10.1088/0954-898X_6_2_004},
	urldate = {2022-12-16},
	volume = {6},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1088/0954-898X_6_2_004}}

@article{Boerlin11,
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	author = {Boerlin, Martin and Den{\`e}ve, Sophie},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1001080},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	keywords = {Action potentials, Memory, Neural networks, Neuronal tuning, Neurons, Sensory cues, Sensory perception, Working memory, ⛔ No INSPIRE recid found},
	language = {en},
	month = feb,
	number = {2},
	pages = {e1001080},
	title = {Spike-{Based} {Population} {Coding} and {Working} {Memory}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	urldate = {2022-11-14},
	volume = {7},
	year = {2011},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1001080}}

@article{Bohte02,
	abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutr{\'e}, Han},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0925-2312(01)00658-0},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Error-backpropagation, Spiking neurons, Temporal coding},
	language = {en},
	month = oct,
	number = {1},
	pages = {17--37},
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	urldate = {2022-09-28},
	volume = {48},
	year = {2002},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	bdsk-url-2 = {https://doi.org/10.1016/S0925-2312(01)00658-0}}

@article{Bohte04,
	author = {Bohte, Sander M},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	journal = {Natural Computing},
	number = {2},
	pages = {195--206},
	title = {The evidence for neural information processing with precise spike-times: {A} survey},
	volume = {3},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1023/B:NACO.0000027755.02868.60}}

@article{Bonilla22,
	abstract = {Spiking neural networks (SNNs) using time-to-first-spike (TTFS) codes, in which neurons fire at most once, are appealing for rapid and low power processing. In this theoretical paper, we focus on information coding and decoding in those networks, and introduce a new unifying mathematical framework that allows the comparison of various coding schemes. In an early proposal, called rank-order coding (ROC), neurons are maximally activated when inputs arrive in the order of their synaptic weights, thanks to a shunting inhibition mechanism that progressively desensitizes the neurons as spikes arrive. In another proposal, called NoM coding, only the first N spikes of M input neurons are propagated, and these ``first spike patterns'' can be readout by downstream neurons with homogeneous weights and no desensitization: as a result, the exact order between the first spikes does not matter. This paper also introduces a third option---``Ranked-NoM'' (R-NoM), which combines features from both ROC and NoM coding schemes: only the first N input spikes are propagated, but their order is readout by downstream neurons thanks to inhomogeneous weights and linear desensitization. The unifying mathematical framework allows the three codes to be compared in terms of discriminability, which measures to what extent a neuron responds more strongly to its preferred input spike pattern than to random patterns. This discriminability turns out to be much higher for R-NoM than for the other codes, especially in the early phase of the responses. We also argue that R-NoM is much more hardware-friendly than the original ROC proposal, although NoM remains the easiest to implement in hardware because it only requires binary synapses.},
	author = {Bonilla, Lina and Gautrais, Jacques and Thorpe, Simon and Masquelier, Timoth{\'e}e},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2022.971937},
	journal = {Frontiers in Neuroscience},
	note = {Publisher: Frontiers},
	title = {Analyzing time-to-first-spike coding schemes},
	url = {https://hal.archives-ouvertes.fr/hal-03796195},
	urldate = {2022-11-08},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://hal.archives-ouvertes.fr/hal-03796195},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2022.971937}}

@article{Branco10,
	author = {Branco, Tiago and Clark, Beverley A. and H{\"a}usser, Michael},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1189664},
	journal = {Science},
	language = {en},
	month = sep,
	number = {5999},
	pages = {1671--1675},
	title = {Dendritic {Discrimination} of {Temporal} {Input} {Sequences} in {Cortical} {Neurons}},
	url = {https://doi.org/dqx4n4},
	volume = {329},
	year = {2010},
	bdsk-url-1 = {https://doi.org/dqx4n4},
	bdsk-url-2 = {https://doi.org/10.1126/science.1189664}}

@article{Brette12,
	author = {Brette, Romain},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1002561},
	editor = {Sporns, Olaf},
	journal = {PLoS Computational Biology},
	language = {en},
	number = {6},
	pages = {e1002561},
	title = {Computing with {Neural} {Synchrony}},
	url = {https://doi.org/f32tvz},
	volume = {8},
	year = {2012},
	bdsk-url-1 = {https://doi.org/f32tvz},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1002561}}

@article{Brill77,
	abstract = {It has been argued theoretically and confirmed experimentally that conduction velocity (theta) should be proportional to nerve fibre diameter for myelinated fibre tracts, such as normal peripheral nerve, exhibiting 'structural' similarity'. In some axons, however, the nodes of Ranvier are more closely spaced than in normal peripheral nerve. Analytic arguments have suggested that when internodal distance (L) alone is changed, the plot of theta versus L should have a relatively flat maximum. This was confirmed by several previous computer simulations of myelinated axons, but internode lengths of less than half the normal case were not examined. In order to gain insight into impulse propagation in myelinated and remyelinated fibres with short internodal lengths, the present study examines the conduction velocity and spike configuration for a wide range of internodal lengths. As L becomes large, theta falls and finally propagation is blocked; as L becomes small, theta decreases more and more steeply. From this, it is predicted that for fibres with very short internodal lengths, small local changes in L should affect substantially the conduction velocity.},
	author = {Brill, M H and Waxman, S G and Moore, J W and Joyner, R W},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {0022-3050},
	journal = {Journal of Neurology, Neurosurgery, and Psychiatry},
	month = aug,
	number = {8},
	pages = {769--774},
	pmcid = {PMC492833},
	pmid = {925697},
	shorttitle = {Conduction velocity and spike configuration in myelinated fibres},
	title = {Conduction velocity and spike configuration in myelinated fibres: computed dependence on internode distance.},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC492833/},
	urldate = {2022-11-07},
	volume = {40},
	year = {1977},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC492833/}}

@article{Bringuier99,
	abstract = {The receptive field of a visual neuron is classically defined as the region of space (or retina) where a visual stimulus evokes a change in its firing activity. At the cortical level, a challenging issue concerns the roles of feedforward, local recurrent, intracortical, and cortico-cortical feedback connectivity in receptive field properties. Intracellular recordings in cat area 17 showed that the visually evoked synaptic integration field extends over a much larger area than that established on the basis of spike activity. Synaptic depolarizing responses to stimuli flashed at increasing distances from the center of the receptive field decreased in strength, whereas their onset latency increased. These findings suggest that subthreshold responses in the unresponsive region surrounding the classical discharge field result from the integration of visual activation waves spread by slowly conducting horizontal axons within primary visual cortex.},
	author = {Bringuier, Vincent and Chavane, Fr{\'e}d{\'e}ric and Glaeser, Larry and Fr{\'e}gnac, Yves},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.283.5402.695},
	issn = {0036-8075, 1095-9203},
	journal = {Science},
	month = jan,
	note = {00535 Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Bringuier99},
	number = {5402},
	pages = {695--699},
	title = {Horizontal {Propagation} of {Visual} {Activity} in the {Synaptic} {Integration} {Field} of {Area} 17 {Neurons}},
	url = {http://science.sciencemag.org/content/283/5402/695},
	urldate = {2019-02-07},
	volume = {283},
	year = {1999},
	bdsk-url-1 = {http://science.sciencemag.org/content/283/5402/695},
	bdsk-url-2 = {https://doi.org/10.1126/science.283.5402.695}}

@article{Bruno06,
	abstract = {{\textless}jats:p{\textgreater}Sensory stimuli reach the brain via the thalamocortical projection, a group of axons thought to be among the most powerful in the neocortex. Surprisingly, these axons account for only ∼15\% of synapses onto cortical neurons. The thalamocortical pathway might thus achieve its effectiveness via high-efficacy thalamocortical synapses or via amplification within cortical layer 4. In rat somatosensory cortex, we measured in vivo the excitatory postsynaptic potential evoked by a single synaptic connection and found that thalamocortical synapses have low efficacy. Convergent inputs, however, are both numerous and synchronous, and intracortical amplification is not required. Our results suggest a mechanism of cortical activation by which thalamic input alone can drive cortex.{\textless}/jats:p{\textgreater}},
	author = {Bruno, Randy M. and Sakmann, Bert},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1124593},
	journal = {Science},
	language = {en},
	number = {5780},
	pages = {1622--1627},
	title = {Cortex {Is} {Driven} by {Weak} but {Synchronously} {Active} {Thalamocortical} {Synapses}},
	url = {https://doi.org/c5575v},
	volume = {312},
	year = {2006},
	bdsk-url-1 = {https://doi.org/c5575v},
	bdsk-url-2 = {https://doi.org/10.1126/science.1124593}}

@article{Bryant76,
	abstract = {1. Those features of a transmembrane current correlated with spike initiation were examined in Aplysia neurones using a Gaussian white-noise stimulus. This stimulus has the advantages that it presents numerous wave forms in random order without prejudgement as to their efficacies, and that it allows straightforward statistical calculations. 2. Stimulation with a repeating segment of Gaussian white-noise current revealed remarkable invariance in the firing times of the tested neurones and indicated a high degree of reliability of their response. 3. Frequencies (less than 5 Hz) involved in spike triggering propagated faithfully for up to several millimetres, justifying intrasomatic current injection to examine spike initiation at the trigger locus. 4. Examination of current wave forms preceding spikes indicated that a wide variety could be effective. Hence, a statistical analysis was performed, including computation of probability densities, averages, standard deviations and correlation coefficients of pairs of current values. Each statistic was displayed as a function of time before the spike. 5. The average current trajectory preceding a spike was multiphasic and depended on the presence and polarity of a d.c. bias. An early relatively small inward- or outward-going phase was followed by a large outward phase before the spike. The early phase tended to oppose the polarity of the d.c. bias. 6. The late outward phase of the average current trajectory reached a maximum 40--75 msec before triggering the action potential (AP) and returned to near zero values at the moment of triggering. The fact that the current peak occurs in advance of the AP may be partially explained by a phase delay between the transmembrane current and potential. The failure of the average current trajectory to return to control values immediately following the peak argues for a positive role of the declining phase in spike triggering. 7. Probability densities preceding spikes were Gaussian, indicating that the average was also the most probable value. Although the densities were broad, confirming that spikes were preceded by a wide variety of current wave forms, their standard deviations were reduced significantly with respect to controls, suggesting preferred status of the average current trajectory in spike triggering. 8. The matrix of correlation coefficients between current pairs suggested that spikes tended to be preceded by wave forms that in part kept close to the average current trajectory and in part preserved its shape. 9. The average first and second derivatives of spike-evoking epochs revealed that current slope and acceleration, respectively, were most crucial in the last 200 msec before spike triggering, and that these dynamic stimulus components were more important for a cell maintained under a depolarizing, rather than a hyperpolarizing bias. 10...},
	author = {Bryant, H L and Segundo, J P},
	copyright = {{\copyright} 1976 The Physiological Society},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1113/jphysiol.1976.sp011516},
	issn = {1469-7793},
	journal = {The Journal of Physiology},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1976.sp011516},
	number = {2},
	pages = {279--314},
	shorttitle = {Spike initiation by transmembrane current},
	title = {Spike initiation by transmembrane current: a white-noise analysis.},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1976.sp011516},
	urldate = {2022-12-13},
	volume = {260},
	year = {1976},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1976.sp011516},
	bdsk-url-2 = {https://doi.org/10.1113/jphysiol.1976.sp011516}}

@article{Burkitt21,
	author = {Burkitt, Anthony N. and Hogendoorn, Hinze},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.2017-20.2021},
	journal = {The Journal of Neuroscience},
	language = {en},
	number = {20},
	pages = {4428--4438},
	title = {Predictive {Visual} {Motion} {Extrapolation} {Emerges} {Spontaneously} and without {Supervision} at {Each} {Layer} of a {Hierarchical} {Neural} {Network} with {Spike}-{Timing}-{Dependent} {Plasticity}},
	url = {https://doi.org/gjtwzk},
	volume = {41},
	year = {2021},
	bdsk-url-1 = {https://doi.org/gjtwzk},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.2017-20.2021}}

@article{Butts07,
	abstract = {In mammalian visual system, spikes evoked by visual stimuli have millisecond-scale timing even though the relevant timescales of visual processing themselves are much slower. It has therefore long been debated whether spike timing itself carries some form of the neural code. Now experiments in the lateral geniculate nucleus of cats, the part of the brain that is the primary processor of visual information, show that spike timing precision is not absolute for all classes of visual stimuli. Rather, the degree of precision is relative to the timescale of the stimulus, and this relatively high level of precision is required to construct an accurate representation of the stimulus.},
	author = {Butts, Daniel A. and Weng, Chong and Jin, Jianzhong and Yeh, Chun-I. and Lesica, Nicholas A. and Alonso, Jose-Manuel and Stanley, Garrett B.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nature06105},
	issn = {1476-4687},
	journal = {Nature},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	language = {en},
	month = sep,
	note = {Number: 7158 Publisher: Nature Publishing Group},
	number = {7158},
	pages = {92--95},
	title = {Temporal precision in the neural code and the timescales of natural vision},
	url = {http://www.nature.com/articles/nature06105},
	urldate = {2022-12-15},
	volume = {449},
	year = {2007},
	bdsk-url-1 = {http://www.nature.com/articles/nature06105},
	bdsk-url-2 = {https://doi.org/10.1038/nature06105}}

@article{Buzsaki18,
	author = {Buzs{\'a}ki, Gy{\"o}rgy and Tingley, David},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.tics.2018.07.006},
	journal = {Trends in Cognitive Sciences},
	language = {en},
	month = oct,
	number = {10},
	pages = {853--869},
	title = {Space and {Time}: {The} {Hippocampus} as a {Sequence} {Generator}},
	url = {https://doi.org/gfcr76},
	volume = {22},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gfcr76},
	bdsk-url-2 = {https://doi.org/10.1016/j.tics.2018.07.006}}

@article{Camon19,
	abstract = {Whisker-guided decision making in mice is thought to critically depend on
information processing occurring in the primary somatosensory cortex. However,
it is not clear if neuronal activity in this ``early''
sensory region contains information about the timing and speed of motor
response. To address this question we designed a new task in which freely moving
mice learned to associate a whisker stimulus to reward delivery. The task was
tailored in such a way that a wide range of delays between whisker stimulation
and reward collection were observed due to differences of motivation and
perception. After training, mice were anesthetized and neuronal responses evoked
by stimulating trained and untrained whiskers were recorded across several
cortical columns of barrel cortex. We found a strong correlation between the
delay of the mouse behavioral response and the timing of multiunit activity
evoked by the trained whisker, outside its principal cortical column, in layers
4 and 5A but not in layer 2/3. Circuit mapping ex vivo revealed this effect was
associated with a weakening of layer 4 to layer 2/3 projection. We conclude that
the processes controlling the propagation of key sensory inputs to naive
cortical columns and the timing of sensory-guided action are linked.},
	author = {Camon, J{\'e}r{\'e}my and Hugues, Sandrine and Erlandson, Melissa A and Robbe, David and Lagoun, Sabria and Marouane, Emna and Bureau, Ingrid},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1093/cercor/bhy169},
	issn = {1047-3211},
	journal = {Cerebral Cortex},
	keywords = {⛔ No INSPIRE recid found},
	month = jul,
	number = {7},
	pages = {3034--3047},
	title = {The {Timing} of {Sensory}-{Guided} {Behavioral} {Response} is {Represented} in the {Mouse} {Primary} {Somatosensory} {Cortex}},
	url = {https://doi.org/10.1093/cercor/bhy169},
	urldate = {2022-11-16},
	volume = {29},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1093/cercor/bhy169}}

@article{Caporale08,
	abstract = {{\textless}jats:p{\textgreater} Spike timing--dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo. {\textless}/jats:p{\textgreater}},
	author = {Caporale, Natalia and Dan, Yang},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1146/annurev.neuro.31.060407.125639},
	journal = {Annual Review of Neuroscience},
	language = {en},
	number = {1},
	pages = {25--46},
	title = {Spike {Timing}--{Dependent} {Plasticity}: {A} {Hebbian} {Learning} {Rule}},
	url = {https://doi.org/fqxrgj},
	volume = {31},
	year = {2008},
	bdsk-url-1 = {https://doi.org/fqxrgj},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.neuro.31.060407.125639}}

@article{Carandini12,
	author = {Carandini, Matteo and Heeger, David J},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nrn3136},
	journal = {Nature Reviews Neuroscience},
	keywords = {Adaptation, Afferent Pathways, Anim, Physiological, contrast\_response, divisive\_normalization, normalization},
	number = {1},
	pages = {51--62},
	title = {Normalization as a canonical neural computation},
	volume = {13},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1038/nrn3136}}

@article{Carr90,
	abstract = {Detection of interaural time differences underlies azimuthal sound localization in the barn owl Tyto alba. Axons of the cochlear nucleus magnocellularis, and their targets in the binaural nucleus laminaris, form the circuit responsible for encoding these interaural time differences. The nucleus laminaris receives bilateral inputs from the cochlear nucleus magnocellularis such that axons from the ipsilateral cochlear nucleus enter the nucleus laminaris dorsally, while contralateral axons enter from the ventral side. This interdigitating projection to the nucleus laminaris is tonotopic, and the afferents are both sharply tuned and matched in frequency to the neighboring afferents. Recordings of phase-locked spikes in the afferents show an orderly change in the arrival time of the spikes as a function of distance from the point of their entry into the nucleus laminaris. The same range of conduction time (160 mu sec) was found over the 700-mu m depth of the nucleus laminaris for all frequencies examined (4-7.5 kHz) and corresponds to the range of interaural time differences available to the barn owl. The estimated conduction velocity in the axons is low (3-5 m/sec) and may be regulated by short internodal distances (60 mu m) within the nucleus laminaris. Neurons of the nucleus laminaris have large somata and very short dendrites. These cells are frequency selective and phase-lock to both monaural and binaural stimuli. The arrival time of phase-locked spikes in many of these neurons differs between the ipsilateral and contralateral inputs. When this disparity is nullified by imposition of an appropriate interaural time difference, the neurons respond maximally. The number of spikes elicited in response to a favorable interaural time difference is roughly double that elicited by a monaural stimulus. Spike counts for unfavorable interaural time differences fall well below monaural response levels. These findings indicate that the magnocellular afferents work as delay lines, and the laminaris neurons work as co- incidence detectors. The orderly distribution of conduction times, the predictability of favorable interaural time differences from monaural phase responses, and the pattern of the anatomical projection from the nucleus laminaris to the central nucleus of the inferior colliculus suggest that interaural time differences and their phase equivalents are mapped in each frequency band along the dorsoventral axis of the nucleus laminaris.},
	author = {Carr, C. E. and Konishi, M.},
	copyright = {{\copyright} 1990 by Society for Neuroscience},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.10-10-03227.1990},
	issn = {0270-6474, 1529-2401},
	journal = {Journal of Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	note = {Publisher: Society for Neuroscience Section: Articles},
	number = {10},
	pages = {3227--3246},
	pmid = {2213141},
	title = {A circuit for detection of interaural time differences in the brain stem of the barn owl},
	url = {https://www.jneurosci.org/content/10/10/3227},
	urldate = {2022-12-16},
	volume = {10},
	year = {1990},
	bdsk-url-1 = {https://www.jneurosci.org/content/10/10/3227},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.10-10-03227.1990}}

@article{Carr93,
	author = {Carr, C E},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1146/annurev.ne.16.030193.001255},
	issn = {0147-006X, 1545-4126},
	journal = {Annual Review of Neuroscience},
	language = {en},
	month = mar,
	number = {1},
	pages = {223--243},
	title = {Processing of {Temporal} {Information} in the {Brain}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.ne.16.030193.001255},
	urldate = {2022-11-07},
	volume = {16},
	year = {1993},
	bdsk-url-1 = {https://www.annualreviews.org/doi/10.1146/annurev.ne.16.030193.001255},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.ne.16.030193.001255}}

@article{Celebrini93,
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}To investigate the importance of feedback loops in visual information processing, we have analyzed the dynamic aspects of neuronal responses to oriented gratings in cortical area V1 of the awake primate. If recurrent feedback is important in generating orientation selectivity, the initial part of the neuronal response should be relatively poorly selective, and full orientation selectivity should only appear after a delay. Thus, by examining the dynamics of the neuronal responses it should be possible to assess the importance of feedback processes in the development of orientation selectivity. The results were base on a sample of 259 cells recorded in two monkeys, of which 89\% were visually responsive. Of these, approximately two-thirds were orientation selective. Response latency varied considerably between neurons, ranging from a minimum of 41 ms to over 150 ms, although most had latencies of 50--70 ms. Orientation tuning (defined as the bandwidth at half-height) ranged from 16 deg to over 90 deg, with a mean value of around 55 deg. By examining the selectivity of these different neurons by 10-ms time slices, starting at the onset of the neuronal response, we found that the orientation selectivity of virtually every neuron was fully developed at the very start of the neuronal response. Indeed, many neurons showed a marked tendency to respond at somewhat longer latencies to stimuli that were nonoptimally oriented, with the result that orientation selectivity was highest at the very start of the neuronal response. Furthermore, there was no evidence that the neurons with the shortest onset latencies were less selective. Such evidence is inconsistent with the hypothesis that recurrent intracortical feedback plays an important role in the generation of orientation selectivity. Instead, we suggest that orientation selectivity is primarily generated using feedforward mechanisms, including feedforward inhibition. Such a strategy has the advantage of allowing orientation to be computed rapidly, and avoids the initially poorly selective neuronal responses that characterize processing involving recurrent loops.{\textless}/jats:p{\textgreater}},
	author = {Celebrini, Simona and Thorpe, Simon and Trotter, Yves and Imbert, Michel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1017/s0952523800006052},
	journal = {Visual Neuroscience},
	language = {en},
	month = sep,
	number = {5},
	pages = {811--825},
	title = {Dynamics of orientation coding in area {V1} of the awake primate},
	url = {https://doi.org/dqt5cm},
	volume = {10},
	year = {1993},
	bdsk-url-1 = {https://doi.org/dqt5cm},
	bdsk-url-2 = {https://doi.org/10.1017/s0952523800006052}}

@article{Chagnac-Amitai89,
	abstract = {1. Suppression of GABAA receptor-mediated inhibition disrupts the neural activity of neocortex and can lead to synchronized discharges that mimic those of partial epilepsy. We have studied the role of GABAA-mediated inhibition in controlling the synchronization and horizontal (tangential) spread of cortical activity. 2. Slices of rat SmI were maintained in vitro and focally stimulated in layer VI while recording with a horizontal array of extracellular electrodes. Inhibition was slightly suppressed by adding low concentrations of the GABAA antagonists bicuculline or bicuculline methiodide to the bathing medium. Under control conditions neural activity was narrowly confined to a vertical strip of cortex. The horizontal spread of activity expanded about twofold in the presence of antagonist concentrations (less than or equal to 0.5 microM) that were expected to suppress GABAA function by no more than 10-20\%. 3. At antagonist concentrations between 0.4 and 1.0 microM, evoked epileptiform activity appeared. These threshold-dose epileptiform events showed wide variations in size and duration (even at the same recording site), very variable distances of horizontal propagation, specific sites of propagation failure, reversals of propagation direction, and directional asymmetries in their probability of propagation. This contrasts with activity observed previously (Ref. 9) in high bicuculline concentrations (greater than or equal to 10 microM): large, stereotyped events that propagate reliably without decrement or reflection. 4. Intracellular recordings were obtained from pyramidal neurons in layers II/III in the presence of less than or equal to 1 microM bicuculline. Inhibitory postsynaptic potentials (IPSPs) were observed during both primary evoked responses and propagating epileptiform events and were often comparable in size and duration to those in untreated cortex. Epileptiform field potentials were always correlated with synaptic activity in single cells, but the pattern and type of PSPs varied with the form of the field potentials. Large amplitude epileptiform events coincided with an overwhelming inhibition of upper layer neurons. 5. We conclude that 1) the horizontal spread of normal cortical activity is strongly constrained by GABAA-mediated IPSPs, 2) a relatively small reduction in the efficacy of inhibition leads to a large increase in the spread of excitation, 3) initiation and propagation of synchronized epileptiform activity can occur even in the presence of robust cortical inhibition, and 4) the character of epileptiform activity is strongly affected by the influences of inhibition.},
	author = {Chagnac-Amitai, Y. and Connors, B. W.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.1989.61.4.747},
	issn = {0022-3077},
	journal = {Journal of Neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	month = apr,
	note = {Publisher: American Physiological Society},
	number = {4},
	pages = {747--758},
	title = {Horizontal spread of synchronized activity in neocortex and its control by {GABA}-mediated inhibition},
	url = {https://journals.physiology.org/doi/abs/10.1152/jn.1989.61.4.747},
	urldate = {2022-12-16},
	volume = {61},
	year = {1989},
	bdsk-url-1 = {https://journals.physiology.org/doi/abs/10.1152/jn.1989.61.4.747},
	bdsk-url-2 = {https://doi.org/10.1152/jn.1989.61.4.747}}

@article{Chan07,
	abstract = {In this paper, we present an analog integrated circuit containing a matched pair of silicon cochleae and an address event interface. Each section of the cochlea, modeled by a second-order low-pass filter, is followed by a simplified inner hair cell circuit and a spiking neuron circuit. When the neuron spikes, an address event is generated on the asynchronous data bus. We present the results of the chip characterization and the results of an interaural time difference based sound localization experiment using the address event representation (AER) EAR. The chip was fabricated in a 3-metal 2-poly 0.5-mum CMOS process},
	author = {Chan, Vincent and Liu, Shih-Chii and van Schaik, Andr},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TCSI.2006.887979},
	issn = {1558-0806},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {Analog integrated circuits, Biological system modeling, Circuits, Ear, Low pass filters, Neuromorphics, Neurons, Protocols, Silicon, Timing, Transmitters, neuromorphic engineering, silicon cochlea, sound localization},
	month = jan,
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers},
	number = {1},
	pages = {48--59},
	shorttitle = {{AER} {EAR}},
	title = {{AER} {EAR}: {A} {Matched} {Silicon} {Cochlea} {Pair} {With} {Address} {Event} {Representation} {Interface}},
	volume = {54},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/TCSI.2006.887979}}

@article{Chase07,
	abstract = {{\textless}jats:p{\textgreater}It is well known that many stimulus parameters, such as sound location in the auditory system or contrast in the visual system, can modulate the timing of the first spike in sensory neurons. Could first-spike latency be a candidate neural code? Most studies measuring first-spike latency information assume that the brain has an independent reference for stimulus onset from which to extract latency. This assumption creates an obvious confound that casts doubt on the feasibility of first-spike latency codes. If latency is measured relative to an internal reference of stimulus onset calculated from the responses of the neural population, the information conveyed by the latency of single neurons might decrease because of correlated changes in latency across the population. Here we assess the effects of a realistic model of stimulus onset detection on the first-spike latency information conveyed by single neurons in the auditory system. Contrary to expectation, we find that on average, the information contained in single neurons does not decrease; in fact, the majority of neurons show a slight increase in the information conveyed by latency referenced to a population onset. Our results show that first-spike latency codes are a feasible mechanism for information transfer even when biologically plausible estimates of stimulus onset are taken into account.{\textless}/jats:p{\textgreater}},
	author = {Chase, Steven M. and Young, Eric D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.0610368104},
	journal = {Proceedings of the National Academy of Sciences},
	language = {en},
	month = mar,
	number = {12},
	pages = {5175--5180},
	title = {First-spike latency information in single neurons increases when referenced to population onset},
	url = {https://doi.org/cm3b98},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://doi.org/cm3b98},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0610368104}}

@article{Chemla19,
	abstract = {The ``apparent motion'' illusion is evoked when stationary stimuli are successively flashed in spatially separated positions. It depends on the precise spatial and temporal separations of the stimuli. For large spatiotemporal separation, the long-range apparent motion (lrAM), it remains unclear how the visual system computes unambiguous motion signals. Here we investigated whether intracortical interactions within retinotopic maps could shape a global motion representation at the level of V1 population in response to a lrAM. In fixating monkeys, voltage-sensitive dye imaging revealed the emergence of a spatio-temporal representation of the motion trajectory at the scale of V1 population activity, shaped by systematic backward suppressive waves. We show that these waves are the expected emergent property of a recurrent gain control fed by the horizontal intra-cortical network. Such non-linearities explain away ambiguous correspondence problems of the stimulus along the motion path, preformating V1 population response for an optimal read-out by downstream areas.},
	author = {Chemla, Sandrine and Reynaud, Alexandre and diVolo, Matteo and Zerlaut, Yann and Perrinet, Laurent U and Destexhe, Alain and Chavane, Fr{\'e}d{\'e}ric Y},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.2792-18.2019},
	journal = {Journal of Neuroscience},
	month = mar,
	note = {Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Chemla19},
	pages = {18},
	title = {Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey {V1}},
	url = {http://www.jneurosci.org/content/early/2019/03/18/JNEUROSCI.2792-18.2019},
	urldate = {2018-07-27},
	volume = {2792},
	year = {2019},
	bdsk-url-1 = {http://www.jneurosci.org/content/early/2019/03/18/JNEUROSCI.2792-18.2019},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.2792-18.2019}}

@incollection{Liu09,
	abstract = {In this paper, a novel field programmable analog arrays (FPAA) architecture, namely, NueroFPAA, is introduced to utilize nanodevices to build a programmable neuromorphic system. By using nanodevices as programmable components, the proposed FPAA can achieve high-density and low-power operations for neuromorphic applications. The routing and function blocks of the FPAA are specifically designed so that this proposed architecture can support large-scale neuromorphic design as well as various analog circuitries.},
	address = {Berlin, Heidelberg},
	author = {Liu, Ming and Yu, Hua and Wang, Wei},
	booktitle = {Nano-{Net}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	editor = {Cheng, Maggie},
	isbn = {978-3-642-02426-9 978-3-642-02427-6},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	pages = {44--48},
	publisher = {Springer Berlin Heidelberg},
	title = {{FPAA} {Based} on {Integration} of {CMOS} and {Nanojunction} {Devices} for {Neuromorphic} {Applications}},
	url = {http://link.springer.com/10.1007/978-3-642-02427-6_9},
	urldate = {2022-11-14},
	volume = {3},
	year = {2009},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-642-02427-6_9}}

@techreport{Chintaluri22,
	abstract = {So-called spontaneous neuronal activity is a central hallmark of most nervous systems. Such non-causal firing is contrary to the tenet of spikes as a means of communication, and its origin and purpose remain unclear. Here, we propose that non-input driven firing can serve as a release valve to protect neurons from the toxic conditions arising in mitochondria from lower-than-baseline energy consumption. We built a framework of models that incorporate homeostatic control of metabolic products--ATP, ADP, and reactive oxygen species, among others--by way of changes in firing. Our theory can account for key features of neuronal activity observed in many experiments in studies ranging from ion channels function all the way to resting state dynamics. We propose an integrated, crucial role for metabolic spiking that bridges the gap between metabolic homeostasis and neuronal function. Finally, we make testable predictions to validate or falsify our theory.},
	author = {Chintaluri, Chaitanya and Vogels, Tim P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1101/2022.10.16.512428},
	institution = {Neuroscience},
	language = {en},
	month = oct,
	title = {Metabolically driven action potentials serve neuronal energy homeostasis and protect from reactive oxygen species},
	type = {preprint},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.10.16.512428},
	urldate = {2022-11-07},
	year = {2022},
	bdsk-url-1 = {http://biorxiv.org/lookup/doi/10.1101/2022.10.16.512428},
	bdsk-url-2 = {https://doi.org/10.1101/2022.10.16.512428}}

@article{Christensen22,
	abstract = {Modern computation based on the von Neumann architecture is today a mature cutting-edge science. In the Von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this Roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The Roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this Roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.},
	author = {Christensen, Dennis Valbj{\o}rn and Dittmann, Regina and Linares-Barranco, Bernabe and Sebastian, Abu and Le Gallo, Manuel and Redaelli, Andrea and Slesazeck, Stefan and Mikolajick, Thomas and Spiga, Sabina and Menzel, Stephan and Valov, Ilia and Milano, Gianluca and Ricciardi, Carlo and Liang, Shi-Jun and Miao, Feng and Lanza, Mario and Quill, Tyler J. and Keene, Scott Tom and Salleo, Alberto and Grollier, Julie and Markovic, Danijela and Mizrahi, Alice and Yao, Peng and Yang, J. Joshua and Indiveri, Giacomo and Strachan, John Paul and Datta, Suman and Vianello, Elisa and Valentian, Alexandre and Feldmann, Johannes and Li, Xuan and Pernice, Wolfram HP and Bhaskaran, Harish and Furber, Steve and Neftci, Emre and Scherr, Franz and Maass, Wolfgang and Ramaswamy, Srikanth and Tapson, Jonathan and Panda, Priyadarshini and Kim, Youngeun and Tanaka, Gouhei and Thorpe, Simon and Bartolozzi, Chiara and Cleland, Thomas A and Posch, Christoph and Liu, Shih-Chii and Panuccio, Gabriella and Mahmud, Mufti and Mazumder, Arnab Neelim and Hosseini, Morteza and Mohsenin, Tinoosh and Donati, Elisa and Tolu, Silvia and Galeazzi, Roberto and Christensen, Martin Ejsing and Holm, Sune and Ielmini, Daniele and Pryds, Nini},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1088/2634-4386/ac4a83},
	issn = {2634-4386},
	journal = {Neuromorphic Computing and Engineering},
	language = {en},
	title = {2022 roadmap on neuromorphic computing and engineering},
	url = {http://iopscience.iop.org/article/10.1088/2634-4386/ac4a83},
	urldate = {2022-01-13},
	year = {2022},
	bdsk-url-1 = {http://iopscience.iop.org/article/10.1088/2634-4386/ac4a83},
	bdsk-url-2 = {https://doi.org/10.1088/2634-4386/ac4a83}}

@article{Clady14,
	abstract = {A reliable and fast sensing of the environment is a fundamental necessity of mobile platforms. Unfortunately conventional cameras due to the current frame-based acquisition paradigm output low temporal dynamics and redundant data flow leading to high computational costs. It is obviously incompatible with the necessities of mobile platforms where energy consumption and computational load are a major issue. The restrictions of the frame-based paradigm are contradictory with applications requiring high speed sensor-based reactive control. This paper introduces a fast obstacle avoidance using the output of an asynchronous event-based time encoded imaging sensor. The approach is event-based in the sense that every incoming event adds to the computation process thus allowing fast avoidance responses. It introduces an event-based time-to-contact approach relying on the computation of visual event-based motion flows. Experiments on a mobile robot are presented in an indoor environment. Time to contact results are compared with those provided by a laser range finder showing that event-based sensing offers new perspectives for mobile robotics sensing.},
	author = {Clady, Xavier and Clercq, Charles and Ieng, Sio-Hoi and Houseini, Fouzhan and Randazzo, Marco and Natale, Lorenzo and Bartolozzi, Chiara and Benosman, Ryad Benjamin},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2014.00009},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	keywords = {Computer Vision, Event-based Computation, Neuromorphic vision, Robotics, time-to-contact},
	language = {English},
	note = {00000 Publisher: Frontiers},
	title = {Asynchronous visual event-based time-to-contact},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2014.00009/full},
	urldate = {2020-11-04},
	volume = {8},
	year = {2014},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2014.00009/full},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2014.00009}}

@incollection{Cleland14,
	abstract = {Like other sensory systems, the olfactory system transduces specific features of the external environment and must construct an organized sensory representation from these highly fragmented inputs. As with these other systems, this representation is not accurate per se, but is constructed for utility, and emphasizes certain, presumably useful, features over others. I here describe the cellular and circuit mechanisms of the peripheral olfactory system that underlie this process of sensory construction, emphasizing the distinct architectures and properties of the two prominent computational layers in the olfactory bulb. Notably, while the olfactory system solves essentially similar conceptual problems to other sensory systems, such as contrast enhancement, activity normalization, and extending dynamic range, its peculiarities often require qualitatively different computational algorithms than are deployed in other sensory modalities. In particular, the olfactory modality is intrinsically high dimensional, and lacks a simple, externally defined basis analogous to wavelength or pitch on which elemental odor stimuli can be quantitatively compared. Accordingly, the quantitative similarities of the receptive fields of different odorant receptors (ORs) vary according to the statistics of the odor environment. To resolve these unusual challenges, the olfactory bulb appears to utilize unique nontopographical computations and intrinsic learning mechanisms to perform the necessary high-dimensional, similarity-dependent computations. In sum, the early olfactory system implements a coordinated set of early sensory transformations directly analogous to those in other sensory systems, but accomplishes these with unique circuit architectures adapted to the properties of the olfactory modality.},
	author = {Cleland, Thomas A.},
	booktitle = {Progress in {Brain} {Research}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/B978-0-444-63350-7.00007-3},
	isbn = {978-0-444-63350-7},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	pages = {177--203},
	publisher = {Elsevier},
	title = {Construction of {Odor} {Representations} by {Olfactory} {Bulb} {Microcircuits}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444633507000073},
	urldate = {2022-12-16},
	volume = {208},
	year = {2014},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/B9780444633507000073},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-444-63350-7.00007-3}}

@article{Coull22,
	author = {Coull, Jennifer T. and Giersch, Anne},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s44159-022-00038-y},
	journal = {Nature Reviews Psychology},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: Nature Publishing Group},
	number = {5},
	pages = {257--271},
	title = {The distinction between temporal order and duration processing, and implications for schizophrenia},
	volume = {1},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s44159-022-00038-y}}

@article{Cullen21,
	author = {Cullen, Carlie L. and Pepper, Renee E. and Clutterbuck, Mackenzie T. and Pitman, Kimberley A. and Oorschot, Viola and Auderset, Loic and Tang, Alexander D. and Ramm, Georg and Emery, Ben and Rodger, Jennifer and Jolivet, Renaud B. and Young, Kaylene M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.celrep.2020.108641},
	issn = {2211-1247},
	journal = {Cell Reports},
	keywords = {action potential, computational modeling, conduction velocity, myelin, node of Ranvier, oligodendrocyte, periaxonal space, plasticity, spatial learning, transcranial magnetic stimulation, ⛔ No INSPIRE recid found},
	language = {English},
	month = jan,
	note = {Publisher: Elsevier},
	number = {3},
	pmid = {33472075},
	title = {Periaxonal and nodal plasticities modulate action potential conduction in the adult mouse brain},
	url = {https://www.cell.com/cell-reports/abstract/S2211-1247(20)31630-2},
	urldate = {2022-11-13},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://www.cell.com/cell-reports/abstract/S2211-1247(20)31630-2},
	bdsk-url-2 = {https://doi.org/10.1016/j.celrep.2020.108641}}

@article{Dahlem09,
	abstract = {We study the nonlinear dynamics of two delay-coupled neural systems each modeled by excitable dynamics of FitzHugh--Nagumo type and demonstrate that bistability between the stable fixed point and limit cycle oscillations occurs for sufficiently large delay times τ and coupling strength C. As the mechanism for these delay-induced oscillations, we identify a saddle-node bifurcation of limit cycles.},
	author = {Dahlem, M. A. and HILLER, G. and PANCHUK, A. and SCH{\"O}LL, E.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1142/s0218127409023111},
	journal = {International Journal of Bifurcation and Chaos},
	language = {en},
	number = {02},
	pages = {745--753},
	title = {Dynamics of delay-coupled excitable neural systems},
	url = {https://doi.org/d43v5b},
	volume = {19},
	year = {2009},
	bdsk-url-1 = {https://doi.org/d43v5b},
	bdsk-url-2 = {https://doi.org/10.1142/s0218127409023111}}

@article{Dard22,
	abstract = {Early electrophysiological brain oscillations recorded in preterm babies and newborn rodents are initially mostly driven by bottom-up sensorimotor activity and only later can detach from external inputs. This is a hallmark of most developing brain areas, including the hippocampus, which, in the adult brain, functions in integrating external inputs onto internal dynamics. Such developmental disengagement from external inputs is likely a fundamental step for the proper development of cognitive internal models. Despite its importance, the developmental timeline and circuit basis for this disengagement remain unknown. To address this issue, we have investigated the daily evolution of CA1 dynamics and underlying circuits during the first two postnatal weeks of mouse development using two-photon calcium imaging in non-anesthetized pups. We show that the first postnatal week ends with an abrupt shift in the representation of self-motion in CA1. Indeed, most CA1 pyramidal cells switch from activated to inhibited by self-generated movements at the end of the first postnatal week, whereas the majority of GABAergic neurons remain positively modulated throughout this period. This rapid switch occurs within 2 days and follows the rapid anatomical and functional surge of local somatic GABAergic innervation. The observed change in dynamics is consistent with a two-population model undergoing a strengthening of inhibition. We propose that this abrupt developmental transition inaugurates the emergence of internal hippocampal dynamics.},
	author = {Dard, Robin F and Leprince, Erwan and Denis, Julien and Rao Balappa, Shrisha and Suchkov, Dmitrii and Boyce, Richard and Lopez, Catherine and Giorgi-Kurz, Marie and Szwagier, Tom and Dumont, Th{\'e}o and Rouault, Herv{\'e} and Minlebaev, Marat and Baude, Agn{\`e}s and Cossart, Rosa and Picardo, Michel A},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.7554/eLife.78116},
	editor = {Peyrache, Adrien and Colgin, Laura L and Butt, Simon JB},
	issn = {2050-084X},
	journal = {eLife},
	month = jul,
	pages = {e78116},
	title = {The rapid developmental rise of somatic inhibition disengages hippocampal dynamics from self-motion},
	url = {https://doi.org/10.7554/eLife.78116},
	urldate = {2022-10-05},
	volume = {11},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.7554/eLife.78116}}

@article{Dardelet21,
	abstract = {Contour velocity estimation and tracking from a fully event-based perspective.},
	author = {Dardelet, Laurent and Benosman, Ryad and Ieng, Sio-Hoi},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.36227/techrxiv.17013824.v1},
	language = {en},
	month = nov,
	title = {An {Event}-by-{Event} {Feature} {Detection} and {Tracking} {Invariant} to {Motion} {Direction} and {Velocity}},
	urldate = {2022-09-28},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.36227/techrxiv.17013824.v1}}

@book{Datadien11,
	abstract = {Axonal conduction delays should not be ignored in simulations of spiking neural networks. Here it is shown that by using axonal
conduction delays, neurons can display sensitivity to a specific spatio-temporal spike pattern. By using delays that complement
the firing times in a pattern, spikes can arrive simultaneously at an output neuron, giving it a high chance of firing in
response to that pattern. An unsupervised learning mechanism called spike-timing-dependent plasticity then increases the weights
for connections used in the pattern, and decreases the others. This allows for an attunement of output neurons to specific
activity patterns, based on temporal aspects of axonal conductivity.},
	author = {Datadien, Arvind and Haselager, Pim and Sprinkhuizen-Kuyper, Ida},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	month = jan,
	note = {Pages: 99},
	title = {The {Right} {Delay} - {Detecting} {Specific} {Spike} {Patterns} with {STDP} and {Axonal} {Conduction} {Delays}.},
	year = {2011}}

@article{Davies18,
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/MM.2018.112130359},
	issn = {0272-1732, 1937-4143},
	journal = {IEEE Micro},
	keywords = {⛔ No INSPIRE recid found},
	month = jan,
	number = {1},
	pages = {82--99},
	shorttitle = {Loihi},
	title = {Loihi: {A} {Neuromorphic} {Manycore} {Processor} with {On}-{Chip} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8259423/},
	urldate = {2022-11-13},
	volume = {38},
	year = {2018},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8259423/},
	bdsk-url-2 = {https://doi.org/10.1109/MM.2018.112130359}}

@article{Davis21,
	author = {Davis, Zachary W and Benigno, Gabriel B and Fletterman, Charlee and Desbordes, Theo and Steward, Christopher and Sejnowski, Terrence J and H Reynolds, John and Muller, Lyle},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41467-021-26175-1},
	journal = {Nature Communications},
	number = {1},
	pages = {1--16},
	title = {Spontaneous traveling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-021-26175-1}}

@article{Davison08,
	author = {Davison, Andrew P},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/neuro.11.011.2008},
	journal = {Frontiers in Neuroinformatics},
	title = {{PyNN}: a common interface for neuronal network simulators},
	url = {https://doi.org/fh8h6j},
	volume = {2},
	year = {2008},
	bdsk-url-1 = {https://doi.org/fh8h6j},
	bdsk-url-2 = {https://doi.org/10.3389/neuro.11.011.2008}}

@article{deCharms96,
	abstract = {CORTICAL population coding could in principle rely on either the mean rate of neuronal action potentials, or the relative timing of action potentials, or both. When a single sensory stimulus drives many neurons to fire at elevated rates, the spikes of these neurons become tightly synchronized1,2, which could be involved in 'binding' together individual firing-rate feature representations into a unified object percept3. Here we demonstrate that the relative timing of cortical action potentials can signal stimulus features themselves, a function even more basic than feature grouping. Populations of neurons in the primary auditory cortex can coordinate the relative timing of their action potentials such that spikes occur closer together in time during continuous stimuli. In this way cortical neurons can signal stimuli even when their firing rates do not change. Population coding based on relative spike timing can systematically signal stimulus features, it is topographically mapped, and it follows the stimulus time course even where mean firing rate does not.},
	author = {deCharms, R. Christopher and Merzenich, Michael M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/381610a0},
	issn = {1476-4687},
	journal = {Nature},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	language = {en},
	month = jun,
	note = {Number: 6583 Publisher: Nature Publishing Group},
	number = {6583},
	pages = {610--613},
	title = {Primary cortical representation of sounds by the coordination of action-potential timing},
	url = {http://www.nature.com/articles/381610a0},
	urldate = {2022-12-15},
	volume = {381},
	year = {1996},
	bdsk-url-1 = {http://www.nature.com/articles/381610a0},
	bdsk-url-2 = {https://doi.org/10.1038/381610a0}}

@article{Delorme99,
	author = {Delorme, Arnaud and Gautrais, Jacques and Van Rullen, Rufin and Thorpe, Simon},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0925-2312(99)00095-8},
	journal = {Neurocomputing},
	note = {Publisher: Elsevier},
	pages = {989--996},
	shorttitle = {{SpikeNET}},
	title = {{SpikeNET}: {A} simulator for modeling large networks of integrate and fire neurons},
	volume = {26},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1016/S0925-2312(99)00095-8}}

@article{Delorme00,
	abstract = {In a rapid categorisation task, monkeys and humans had to detect a target (animal or food) in briefly flashed (32 ms) and previously unseen natural images. Removing colour cues had very little effect on average performance. Impairments were restricted to a mild accuracy drop (in some human subjects) and a small reaction time mean increase (10--15 ms) observed both in monkeys and humans but only in the detection of food targets. In both tasks, accuracy and latency of the fastest behavioural responses were unaffected, suggesting that such ultra-rapid categorisations could depend on feed-forward processing of early coarse achromatic magnocellular information.},
	author = {Delorme, A and Richard, G and Fabre-Thorpe, M},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0042-6989(00)00083-3},
	issn = {0042-6989},
	journal = {Vision Research},
	keywords = {Categorisation, Colour, Natural scenes, Primate, Visual processing},
	language = {en},
	month = jul,
	number = {16},
	pages = {2187--2200},
	shorttitle = {Ultra-rapid categorisation of natural scenes does not rely on colour cues},
	title = {Ultra-rapid categorisation of natural scenes does not rely on colour cues: a study in monkeys and humans},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698900000833},
	urldate = {2022-09-21},
	volume = {40},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0042698900000833},
	bdsk-url-2 = {https://doi.org/10.1016/S0042-6989(00)00083-3}}

@inproceedings{Deneve04,
	abstract = {We propose a new interpretation of spiking neurons as Bayesian integra-          tors accumulating evidence over time about events in the external world          or the body, and communicating to other neurons their certainties about          these events. In this model, spikes signal the occurrence of new infor-          mation, i.e. what cannot be predicted from the past activity. As a result,          firing statistics are close to Poisson, albeit providing a deterministic rep-          resentation of probabilities. We proceed to develop a theory of Bayesian          inference in spiking neural networks, recurrent interactions implement-          ing a variant of belief propagation.},
	author = {Deneve, Sophie},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	publisher = {MIT Press},
	title = {Bayesian inference in spiking neurons},
	url = {https://papers.nips.cc/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html},
	urldate = {2022-12-19},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://papers.nips.cc/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html}}

@article{Denker18,
	author = {Denker, Michael and Zehl, Lyuba and Kilavik, Bj{\o}rg E. and Diesmann, Markus and Brochier, Thomas and Riehle, Alexa and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41598-018-22990-7},
	journal = {Scientific Reports},
	language = {en},
	month = mar,
	number = {1},
	title = {{LFP} beta amplitude is linked to mesoscopic spatio-temporal phase patterns},
	url = {https://doi.org/gc9xx6},
	volume = {8},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gc9xx6},
	bdsk-url-2 = {https://doi.org/10.1038/s41598-018-22990-7}}

@book{DeWeese03,
	abstract = {Cortical neurons have been reported to use both rate and temporal codes. Here we describe a novel mode in which each neuron generates exactly 0 or 1 action potentials, but not more, in response to a stimulus. We used cell-attached recording, which ensured single-unit isolation, to record responses in rat auditory cortex to brief tone pips. Surprisingly, the majority of neurons exhibited binary behavior with few multi-spike responses; several dramatic examples consisted of exactly one spike on 100\% of trials, with no trial-to-trial variability in spike count. Many neurons were tuned to stimulus frequency. Since individual trials yielded at most one spike for most neurons, the information about stimulus frequency was encoded in the population, and would not have been accessible to later stages of processing that only had access to the activity of a single unit. These binary units allow a more efficient population code than is possible with conventional rate coding units, and are consistent with a model of cortical processing in which synchronous packets of spikes propagate stably from one neuronal population to the next.},
	author = {DeWeese, M. R. and Zador, A. M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	publisher = {Neural Information Processing Systems Foundation},
	title = {Binary coding in auditory cortex},
	url = {http://papers.nips.cc/paper/2342-binary-coding-in-auditory-cortex},
	urldate = {2022-10-04},
	year = {2003},
	bdsk-url-1 = {http://papers.nips.cc/paper/2342-binary-coding-in-auditory-cortex}}

@article{Di-Mauro-Alfio22,
	abstract = {Event-based sensors are drawing increasing attention due to their high temporal resolution, low power consumption, and low bandwidth. To efficiently extract semantically meaningful information from sparse data streams produced by such sensors, we present a 4.5TOP/s/W digital accelerator capable of performing 4-bits-quantized event-based convolutional neural networks (eCNN). Compared to standard convolutional engines, our accelerator performs a number of operations proportional to the number of events contained into the input data stream, ultimately achieving a high energy-to-information processing proportionality. On the IBM-DVS-Gesture dataset, we report 80uJ/inf to 261uJ/inf, respectively, when the input activity is 1.2\% and 4.9\%. Our accelerator consumes 0.221pJ/SOP, to the best of our knowledge it is the lowest energy/OP reported on a digital neuromorphic engine.},
	author = {{Di Mauro, Alfio} and {Prasad, Arpan Suravi} and {Huang, Zhikai} and {Spallanzani, Matteo} and {Conti, Francesco} and {Benini, Luca}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3929/ethz-b-000543342},
	journal = {ETH Zurich},
	language = {en},
	title = {{SNE}: an {Energy}-{Proportional} {Digital} {Accelerator} for {Sparse} {Event}-{Based} {Convolutions}},
	url = {https://doi.org/gp3m5v},
	year = {2022},
	bdsk-url-1 = {https://doi.org/gp3m5v},
	bdsk-url-2 = {https://doi.org/10.3929/ethz-b-000543342}}

@inproceedings{Diehl14,
	abstract = {Recent development of neuromorphic hardware offers great potential to speed up simulations of neural networks. SpiNNaker is a neuromorphic hardware and software system designed to be scalable and flexible enough to implement a variety of different types of simulations of neural systems, including spiking simulations with plasticity and learning. Spiketiming dependent plasticity (STDP) rules are the most common form of learning used in spiking networks. However, to date very few such rules have been implemented on SpiNNaker, in part because implementations must be designed to fit the specialized nature of the hardware. Here we explain how general STDP rules can be efficiently implemented in the SpiNNaker system. We give two examples of applications of the implemented rule: learning of a temporal sequence, and balancing inhibition and excitation of a neural network. Comparing the results from the SpiNNaker system to a conventional double-precision simulation, we find that the network behavior is comparable, and the final weights differ by less than 3\% between the two simulations, while the SpiNNaker simulation runs much faster, since it runs in real time, independent of network size.},
	address = {Beijing, China},
	author = {Diehl, Peter U. and Cook, Matthew},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/IJCNN.2014.6889876},
	isbn = {978-1-4799-1484-5 978-1-4799-6627-1},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jul,
	pages = {4288--4295},
	publisher = {IEEE},
	title = {Efficient implementation of {STDP} rules on {SpiNNaker} neuromorphic hardware},
	url = {https://ieeexplore.ieee.org/document/6889876},
	urldate = {2022-11-14},
	year = {2014},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/6889876},
	bdsk-url-2 = {https://doi.org/10.1109/IJCNN.2014.6889876}}

@article{Diesmann03,
	abstract = {NEST is a framework for simulating large, structured neuronal systems. It is designed to investigate the functional behavior of neuronal systems in the context of their anatomical, morphological, and electrophysiological properties. NEST aims at large networks, while maintaining an appropriate degree of biological detail. This is achieved by combining a broad range of abstraction levels in a single network simulation. Great biological detail is then maintained only at the points of interest, while the rest of the system can be modeled by more abstract components. Here, we describe the conception of NEST and illustrate its key features. We demonstrate that software design and organizational aspects were of equal importance for the success of the project.},
	author = {Diesmann, Markus and Gewaltig, Marc-Oliver},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {GWDG-Bericht Nr. 58 Theo Plesser, Volker Macho (Hrsg.)},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	pages = {29},
	title = {{NEST}: {An} {Environment} for {Neural} {Systems} {Simulations}},
	year = {2003}}

@book{DiLorenzo13,
	abstract = {Neuronal communication forms the basis for all behavior, from the smallest movement to our grandest thought processes. Among the many mechanisms that support these functions, spike timing is among the most powerful and---until recently---perhaps the least studied. In the last two decades, however, the study of spike timing has exploded. The heightened interest is due to several factors. These include the development of physiological tools for measuring the activity of neural ensembles and analytical tools for assessing and characterizing spike timing. These advances are coupled with a growing appreciation of spike timing's theoretical importance for the design principles of the brain.  Spike Timing: Mechanisms and Function examines the function of spike timing in sensory, motor, and integrative processes, providing readers with a broad perspective on how spike timing is produced and used by the nervous system. It brings together the work and ideas of leaders in the field to address current thinking as well as future possibilities.   The first section of the book describes the foundation for quantitative analysis and theory. It examines the information contained in spike timing, how it can be quantified, and how neural systems can extract it. The second section explores how input-output relationships are reflected in spike timing across a range of sensory systems.  Drawing together multiple perspectives, including theoretical and computational studies as well as experimental studies in a range of model systems, the book provides a firm background for investigators to consider spike timing as it applies to their own work. It also offers a glimpse of future advances related to mechanisms of spike timing and its role in neural function, such as the development of novel computational technologies.},
	author = {DiLorenzo, Patricia M. and Victor, Jonathan D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	isbn = {978-1-4398-3815-0},
	keywords = {Computers / Software Development \& Engineering / Systems Analysis \& Design, Medical / Biotechnology, Science / Life Sciences / Biophysics, Science / Life Sciences / Neuroscience, Science / Physics / General, Technology \& Engineering / Biomedical, ⛔ No INSPIRE recid found},
	language = {en},
	month = may,
	note = {Google-Books-ID: KTHUIMUpQCUC},
	publisher = {CRC Press},
	shorttitle = {Spike {Timing}},
	title = {Spike {Timing}: {Mechanisms} and {Function}},
	year = {2013}}

@article{Duffy19,
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}In this work, we show a way by which the nervous system maintains precise, stereotyped behavior in the face of environmental and neural changes. Through a model of bird song learning, we show how instability in neural representation of stable behavior can allow a system to more readily adapt and maintain performance with minimal cost. In this perspective, behaviors are made more robust to environmental change by continually seeking subtly new ways of performing the same task. Thus, one should expect to find variability in neural systems executing stereotyped behaviors, and this variability can serve a constructive role in maintaining skilled behavior.{\textless}/jats:p{\textgreater}},
	author = {Duffy, Alison and Abe, Elliott and Perkel, David J. and Fairhall, Adrienne L.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.1815910116},
	journal = {Proceedings of the National Academy of Sciences},
	language = {en},
	number = {19},
	pages = {9592--9597},
	title = {Variation in sequence dynamics improves maintenance of stereotyped behavior in an example from bird song},
	url = {https://doi.org/gpfjm6},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://doi.org/gpfjm6},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1815910116}}

@article{Dugue11,
	author = {Dugue, L. and Marque, P. and VanRullen, R.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.1161-11.2011},
	issn = {0270-6474, 1529-2401},
	journal = {Journal of Neuroscience},
	language = {en},
	month = aug,
	number = {33},
	pages = {11889--11893},
	title = {The {Phase} of {Ongoing} {Oscillations} {Mediates} the {Causal} {Relation} between {Brain} {Excitation} and {Visual} {Perception}},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1161-11.2011},
	urldate = {2022-10-17},
	volume = {31},
	year = {2011},
	bdsk-url-1 = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1161-11.2011},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.1161-11.2011}}

@article{Duncan21,
	abstract = {The myelination of axons by oligodendrocytes is a highly complex cell-to-cell interaction. Oligodendrocytes and axons have a reciprocal signaling relationship in which oligodendrocytes receive cues from axons that direct their myelination, and oligodendrocytes subsequently shape axonal structure and conduction. Oligodendrocytes are necessary for the maturation of excitatory domains on the axon including nodes of Ranvier, help buffer potassium, and support neuronal energy metabolism. Disruption of the oligodendrocyte-axon unit in traumatic injuries, Alzheimer's disease and demyelinating diseases such as multiple sclerosis results in axonal dysfunction and can culminate in neurodegeneration. In this review, we discuss the mechanisms by which demyelination and loss of oligodendrocytes compromise axons. We highlight the intra-axonal cascades initiated by demyelination that can result in irreversible axonal damage. Both the restoration of oligodendrocyte myelination or neuroprotective therapies targeting these intra-axonal cascades are likely to have therapeutic potential in disorders in which oligodendrocyte support of axons is disrupted.},
	author = {Duncan, Greg J. and Simkins, Tyrell J. and Emery, Ben},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fcell.2021.653101},
	issn = {2296-634X},
	journal = {Frontiers in Cell and Developmental Biology},
	keywords = {⛔ No INSPIRE recid found},
	title = {Neuron-{Oligodendrocyte} {Interactions} in the {Structure} and {Integrity} of {Axons}},
	url = {https://www.frontiersin.org/articles/10.3389/fcell.2021.653101},
	urldate = {2022-11-13},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fcell.2021.653101},
	bdsk-url-2 = {https://doi.org/10.3389/fcell.2021.653101}}

@article{Ermentrout08,
	abstract = {The brain is noisy. Neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular. Much of this activity is spontaneous---uncoupled to overt stimuli or motor outputs---leading to questions about the functional impact of this noise. Although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli, recent theoretical and experimental work has shown that noise can play a constructive role---leading to increased reliability or regularity of neuronal firing in single neurons and across populations. These results raise fundamental questions about how noise can influence neural function and computation.},
	author = {Ermentrout, G. Bard and Gal{\'a}n, Roberto F. and Urban, Nathaniel N.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.tins.2008.06.002},
	issn = {0166-2236},
	journal = {Trends in neurosciences},
	month = aug,
	number = {8},
	pages = {428--434},
	pmcid = {PMC2574942},
	pmid = {18603311},
	title = {Reliability, synchrony and noise},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2574942/},
	urldate = {2022-11-08},
	volume = {31},
	year = {2008},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2574942/},
	bdsk-url-2 = {https://doi.org/10.1016/j.tins.2008.06.002}}

@article{Eurich99,
	abstract = {Adaptation of interaction delays is essential for the functioning of many natural and technical systems. We introduce a novel framework for studying the dynamics of delay adaptation in systems which optimize coincidence of inputs. For the important case of periodically modulated input we derive conditions for the existence and stability of solutions which constrain the set of mechanisms for reliable delay adaptation. Using numerical examples we show that our approach is applicable to more general than periodic input patterns such as Poissonian point processes with coordinated rate fluctuations.},
	author = {Eurich, Christian W. and Pawelzik, Klaus and Ernst, Udo and Cowan, Jack D. and Milton, John G.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/dj9c7q},
	journal = {Physical Review Letters},
	month = feb,
	note = {00082 Publisher: American Physical Society},
	number = {7},
	pages = {1594--1597},
	title = {Dynamics of {Self}-{Organized} {Delay} {Adaptation}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1594},
	urldate = {2021-09-16},
	volume = {82},
	year = {1999},
	bdsk-url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1594},
	bdsk-url-2 = {https://doi.org/10/dj9c7q}}

@misc{Fang21,
	abstract = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
	author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothee and Huang, Tiejun and Tian, Yonghong},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.48550/arXiv.2007.05785},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ⛔ No INSPIRE recid found},
	month = aug,
	note = {arXiv:2007.05785 [cs]},
	publisher = {arXiv},
	title = {Incorporating {Learnable} {Membrane} {Time} {Constant} to {Enhance} {Learning} of {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.05785},
	urldate = {2022-12-15},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2007.05785},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2007.05785}}

@inproceedings{Farquhar06,
	address = {Island of Kos, Greece},
	author = {Farquhar, E. and Gordon, C. and Hasler, P.},
	booktitle = {2006 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ISCAS.2006.1693534},
	isbn = {978-0-7803-9389-9},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	pages = {4114--4117},
	publisher = {IEEE},
	title = {A {Field} {Programmable} {Neural} {Array}},
	url = {http://ieeexplore.ieee.org/document/1693534/},
	urldate = {2022-11-14},
	year = {2006},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/1693534/},
	bdsk-url-2 = {https://doi.org/10.1109/ISCAS.2006.1693534}}

@article{Feller97,
	abstract = {In the developing mammalian retina, spontaneous waves of action potentials are present in the ganglion cell layer weeks before vision. These waves are known to be generated by a synaptically connected network of amacrine cells and retinal ganglion cells, and exhibit complex spatiotemporal patterns, characterized by shifting domains of coactivation. Here, we present a novel dynamical model consisting of two coupled populations of cells that quantitatively reproduces the experimentally observed domain sizes, interwave intervals, and wavefront velocity profiles. Model and experiment together show that the highly correlated activity generated by retinal waves can be explained by a combination of random spontaneous activation of cells and the past history of local retinal activity.},
	author = {Feller, Marla B. and Butts, Daniel A. and Aaron, Holly L. and Rokhsar, Daniel S. and Shatz, Carla J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0896-6273(00)80940-X},
	issn = {0896-6273},
	journal = {Neuron},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	number = {2},
	pages = {293--306},
	title = {Dynamic {Processes} {Shape} {Spatiotemporal} {Properties} of {Retinal} {Waves}},
	url = {https://www.sciencedirect.com/science/article/pii/S089662730080940X},
	urldate = {2022-12-16},
	volume = {19},
	year = {1997},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S089662730080940X},
	bdsk-url-2 = {https://doi.org/10.1016/S0896-6273(00)80940-X}}

@article{Fields20,
	author = {Fields, R. Douglas and Bukalo, Olena},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41593-020-0606-x},
	issn = {1546-1726},
	journal = {Nature Neuroscience},
	keywords = {Animals, Memory, Memory Consolidation, Mice, Myelin Sheath, ⛔ No INSPIRE recid found},
	language = {eng},
	month = apr,
	number = {4},
	pages = {469--470},
	pmid = {32094969},
	title = {Myelin makes memories},
	volume = {23},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s41593-020-0606-x}}

@article{Fields15,
	abstract = {The precise timing of impulse transmission along axons is crucial for synaptic plasticity and brain oscillations, and is partly determined by myelin thickness. In this Opinion article, R. Douglas Fields discusses how electrical activity influences myelin thickness and thus conduction velocity and circuit properties.},
	author = {Fields, R. Douglas},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nrn4023},
	issn = {1471-0048},
	journal = {Nature Reviews Neuroscience},
	keywords = {biology, delay-learning, myelination, ⛔ No INSPIRE recid found},
	language = {en},
	month = dec,
	note = {Number: 12 Publisher: Nature Publishing Group},
	number = {12},
	pages = {756--767},
	shorttitle = {A new mechanism of nervous system plasticity},
	title = {A new mechanism of nervous system plasticity: activity-dependent myelination},
	url = {https://www.nature.com/articles/nrn4023},
	urldate = {2021-01-07},
	volume = {16},
	year = {2015},
	bdsk-url-1 = {https://www.nature.com/articles/nrn4023},
	bdsk-url-2 = {https://doi.org/10.1038/nrn4023}}

@book{Flourens42,
	author = {Flourens, Marie Jean Pierre},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	language = {fr},
	publisher = {J.-B. Balliere},
	title = {Recherches exp{\'e}rimentales sur les propri{\'e}t{\'e}s et les fonctions du syst{\`e}me nerveux, dans les animaux vert{\'e}br{\'e}s},
	year = {1842}}

@article{Foss00,
	abstract = {The dynamics of a recurrent inhibitory neural loop composed of a periodically spiking Aplysia motoneuron reciprocally connected to a computer are investigated as a function of the time delay, τ, for propagation around the loop. It is shown that for certain choices of τ, multiple qualitatively different neural spike trains co-exist. A mathematical model is constructed for the dynamics of this pulsed-coupled recurrent loop in which all parameters are readily measured experimentally: the phase resetting curve of the neuron for a given simulated postsynaptic current and τ. For choices of the parameters for which multiple spiking patterns co-exist in the experimental paradigm, the model exhibits multistability. Numerical simulations suggest that qualitatively similar results will occur if the motoneuron is replaced by several other types of neurons and that once τ becomes sufficiently long, multistability will be the dominant form of dynamical behavior. These observations suggest that great care must be taken in determining the etiology of qualitative changes in neural spiking patterns, particularly when propagation times around polysynaptic loops are long.},
	author = {Foss, Jennifer and Milton, John},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/gmvbsh},
	issn = {0022-3077, 1522-1598},
	journal = {Journal of Neurophysiology},
	language = {en},
	month = aug,
	number = {2},
	pages = {975--985},
	title = {Multistability in {Recurrent} {Neural} {Loops} {Arising} {From} {Delay}},
	url = {https://www.physiology.org/doi/10.1152/jn.2000.84.2.975},
	urldate = {2021-09-16},
	volume = {84},
	year = {2000},
	bdsk-url-1 = {https://www.physiology.org/doi/10.1152/jn.2000.84.2.975},
	bdsk-url-2 = {https://doi.org/10/gmvbsh}}

@article{Fries05,
	abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility.},
	author = {Fries, Pascal},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.tics.2005.08.011},
	issn = {1364-6613},
	journal = {Trends in Cognitive Sciences},
	keywords = {Action Potentials, Animals, Biological Clocks, Brain, Cognition, Humans, Nerve Net, Neurons, Nonlinear Dynamics, Periodicity, Pyramidal Tracts, Synaptic Transmission},
	language = {eng},
	month = oct,
	number = {10},
	pages = {474--480},
	pmid = {16150631},
	shorttitle = {A mechanism for cognitive dynamics},
	title = {A mechanism for cognitive dynamics: neuronal communication through neuronal coherence},
	volume = {9},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1016/j.tics.2005.08.011}}

@article{Furber13,
	author = {Furber, Steve B. and Lester, David R. and Plana, Luis A. and Garside, Jim D. and Painkras, Eustace and Temple, Steve and Brown, Andrew D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TC.2012.142},
	issn = {0018-9340},
	journal = {IEEE Transactions on Computers},
	keywords = {⛔ No INSPIRE recid found},
	month = dec,
	number = {12},
	pages = {2454--2467},
	title = {Overview of the {SpiNNaker} {System} {Architecture}},
	url = {http://ieeexplore.ieee.org/document/6226357/},
	urldate = {2022-11-13},
	volume = {62},
	year = {2013},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/6226357/},
	bdsk-url-2 = {https://doi.org/10.1109/TC.2012.142}}

@book{Furber20,
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1561/9781680836523},
	editor = {Furber, Steve and Bogdan, Petrut},
	isbn = {978-1-68083-652-3 978-1-68083-653-0},
	keywords = {⛔ No INSPIRE recid found},
	publisher = {Now Publishers},
	shorttitle = {{SpiNNaker}},
	title = {{SpiNNaker}: {A} {Spiking} {Neural} {Network} {Architecture}},
	url = {https://nowpublishers.com/article/BookDetails/9781680836523},
	urldate = {2022-11-13},
	year = {2020},
	bdsk-url-1 = {https://nowpublishers.com/article/BookDetails/9781680836523},
	bdsk-url-2 = {https://doi.org/10.1561/9781680836523}}

@article{Gallego22,
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of ms), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
	author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jorg and Daniilidis, Kostas and Scaramuzza, Davide},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TPAMI.2020.3008413},
	issn = {0162-8828, 2160-9292, 1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	language = {en},
	month = jan,
	number = {1},
	pages = {154--180},
	shorttitle = {Event-{Based} {Vision}},
	title = {Event-{Based} {Vision}: {A} {Survey}},
	url = {https://ieeexplore.ieee.org/document/9138762/},
	urldate = {2022-07-19},
	volume = {44},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9138762/},
	bdsk-url-2 = {https://doi.org/10.1109/TPAMI.2020.3008413}}

@article{Gasser39,
	author = {Gasser, H. S. and Grundfest, H.},
	copyright = {Copyright {\copyright} 1939 by American Physiological Society},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/ajplegacy.1939.127.2.393},
	journal = {American Journal of Physiology-Legacy Content},
	language = {en},
	month = aug,
	note = {Publisher: American Physiological Society},
	title = {{AXON} {DIAMETERS} {IN} {RELATION} {TO} {THE} {SPIKE} {DIMENSIONS} {AND} {THE} {CONDUCTION} {VELOCITY} {IN} {MAMMALIAN} {A} {FIBERS}},
	url = {https://journals.physiology.org/doi/10.1152/ajplegacy.1939.127.2.393},
	urldate = {2022-11-07},
	year = {1939},
	bdsk-url-1 = {https://journals.physiology.org/doi/10.1152/ajplegacy.1939.127.2.393},
	bdsk-url-2 = {https://doi.org/10.1152/ajplegacy.1939.127.2.393}}

@article{Gautrais98,
	author = {Gautrais, Jacques and Thorpe, Simon},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0303-2647(98)00050-1},
	journal = {Biosystems},
	note = {Publisher: Elsevier},
	number = {1-3},
	pages = {57--65},
	shorttitle = {Rate coding versus temporal order coding},
	title = {Rate coding versus temporal order coding: a theoretical approach},
	volume = {48},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1016/S0303-2647(98)00050-1}}

@article{Gerstner96,
	abstract = {A PARADOX that exists in auditory and electrosensory neural systems1,2 is that they encode behaviourally relevant signals in the range of a few microseconds with neurons that are at least one order of magnitude slower. The importance of temporal coding in neural information processing is not clear yet3--8. A central question is whether neuronal firing can be more precise than the time constants of the neuronal processes involved9. Here we address this problem using the auditory system of the barn owl as an example. We present a modelling study based on computer simulations of a neuron in the laminar nucleus. Three observations explain the paradox. First, spiking of an 'integrate-and-fire' neuron driven by excitatory postsynaptic potentials with a width at half-maximum height of 250 μs, has an accuracy of 25 μs if the presynaptic signals arrive coherently. Second, the necessary degree of coherence in the signal arrival times can be attained during ontogenetic development by virtue of an unsupervised hebbian learning rule. Learning selects connections with matching delays from a broad distribution of axons with random delays. Third, the learning rule also selects the correct delays from two independent groups of inputs, for example, from the left and right ear.},
	author = {Gerstner, Wulfram and Kempter, Richard and van Hemmen, J. Leo and Wagner, Hermann},
	copyright = {1996 Nature Publishing Group},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/383076a0},
	issn = {1476-4687},
	journal = {Nature},
	language = {en},
	month = sep,
	note = {Number: 6595 Publisher: Nature Publishing Group},
	number = {6595},
	pages = {76--78},
	title = {A neuronal learning rule for sub-millisecond temporal coding},
	url = {https://www.nature.com/articles/383076a0},
	urldate = {2021-01-07},
	volume = {383},
	year = {1996},
	bdsk-url-1 = {https://www.nature.com/articles/383076a0},
	bdsk-url-2 = {https://doi.org/10.1038/383076a0}}

@article{Gerstner95,
	author = {Gerstner, Wulfram},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1103/physreve.51.738},
	journal = {Physical Review E},
	language = {en},
	month = jan,
	number = {1},
	pages = {738--758},
	title = {Time structure of the activity in neural network models},
	url = {https://doi.org/cwcn9d},
	volume = {51},
	year = {1995},
	bdsk-url-1 = {https://doi.org/cwcn9d},
	bdsk-url-2 = {https://doi.org/10.1103/physreve.51.738}}

@article{Gewaltig01,
	abstract = {The synfire hypothesis states that under appropriate conditions volleys of synchronized spikes (pulse packets) can propagate through the cortical network by traveling along chains of groups of cortical neurons. Here, we present results from network simulations, taking full account of the variability in pulse packet realizations. We repeatedly stimulated a synfire chain of model neurons and estimated activity (a) and temporal jitter (σ) of the spike response for each neuron group in the chain in many trials. The survival probability of the activity was assessed for each point in (a, σ)-space. The results confirm and extend our earlier predictions based on single neuron properties and a deterministic state-space analysis [Diesmann, M., Gewaltig, M.-O., \& Aertsen, A. (1999). Stable propagation of synchronous spiking in cortical neural networks. Nature, 402, 529--533].},
	author = {Gewaltig, Marc-Oliver and Diesmann, Markus and Aertsen, Ad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0893-6080(01)00070-3},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Action Potentials, Animals, Cell Membrane, Cerebral Cortex, Cortical dynamics, Humans, Integrate-and-fire neurons, Models, Statistical, Nerve Net, Neural Networks, Computer, Neurons, Pulse packets, Single-trial analysis, Spike patterns, Spiking neurons, Synaptic Transmission, Synfire chains, Variability},
	language = {en},
	month = jul,
	number = {6},
	pages = {657--673},
	shorttitle = {Propagation of cortical synfire activity},
	title = {Propagation of cortical synfire activity: survival probability in single trials and stability in the mean},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000703},
	urldate = {2022-10-26},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608001000703},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(01)00070-3}}

@misc{Ghosh19,
	author = {Ghosh, Rohan and Gupta, Anupam and Silva, Andrei Nakagawa and Soares, Alcimar and Thakor, Nitish V.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	title = {Spatiotemporal filtering for event-based action recognition},
	url = {http://arxiv.org/abs/1903.07067},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1903.07067}}

@article{Ghosh21,
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {arXiv:2109.07618 [physics]},
	month = sep,
	note = {00000 arXiv: 2109.07618},
	title = {Synchronization in time-varying networks},
	url = {http://arxiv.org/abs/2109.07618},
	urldate = {2021-09-21},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2109.07618}}

@article{Ghosh22,
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.physrep.2021.10.006},
	issn = {03701573},
	journal = {Physics Reports},
	keywords = {Physics - Physics and Society},
	month = feb,
	note = {arXiv:2109.07618 [physics]},
	pages = {1--63},
	title = {The synchronized dynamics of time-varying networks},
	url = {http://arxiv.org/abs/2109.07618},
	urldate = {2022-10-17},
	volume = {949},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2109.07618},
	bdsk-url-2 = {https://doi.org/10.1016/j.physrep.2021.10.006}}

@article{Gibson14,
	abstract = {Myelination of the central nervous system requires the generation of functionally mature oligodendrocytes from oligodendrocyte precursor cells (OPCs). Electrically active neurons may influence OPC function and selectively instruct myelination of an active neural circuit. In this work, we use optogenetic stimulation of the premotor cortex in awake, behaving mice to demonstrate that neuronal activity elicits a mitogenic response of neural progenitor cells and OPCs, promotes oligodendrogenesis, and increases myelination within the deep layers of the premotor cortex and subcortical white matter. We further show that this neuronal activity--regulated oligodendrogenesis and myelination is associated with improved motor function of the corresponding limb. Oligodendrogenesis and myelination appear necessary for the observed functional improvement, as epigenetic blockade of oligodendrocyte differentiation and myelin changes prevents the activity-regulated behavioral improvement.},
	author = {Gibson, Erin M. and Purger, David and Mount, Christopher W. and Goldstein, Andrea K. and Lin, Grant L. and Wood, Lauren S. and Inema, Ingrid and Miller, Sarah E. and Bieri, Gregor and Zuchero, J. Bradley and Barres, Ben A. and Woo, Pamelyn J. and Vogel, Hannes and Monje, Michelle},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1252304},
	issn = {0036-8075},
	journal = {Science (New York, N.Y.)},
	keywords = {⛔ No INSPIRE recid found},
	month = may,
	number = {6183},
	pages = {1252304},
	pmcid = {PMC4096908},
	pmid = {24727982},
	title = {Neuronal {Activity} {Promotes} {Oligodendrogenesis} and {Adaptive} {Myelination} in the {Mammalian} {Brain}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096908/},
	urldate = {2022-11-13},
	volume = {344},
	year = {2014},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096908/},
	bdsk-url-2 = {https://doi.org/10.1126/science.1252304}}

@article{Gilson10,
	author = {Gilson, Matthieu},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fncom.2010.00023},
	journal = {Frontiers in Computational Neuroscience},
	title = {{STDP} in recurrent neuronal networks},
	url = {https://doi.org/c8ck59},
	volume = {4},
	year = {2010},
	bdsk-url-1 = {https://doi.org/c8ck59},
	bdsk-url-2 = {https://doi.org/10.3389/fncom.2010.00023}}

@article{Golding02,
	abstract = {Strengthening of synaptic connections following coincident pre- and postsynaptic activity was proposed by Hebb as a cellular mechanism for learning1. Contemporary models assume that multiple synapses must act cooperatively to induce the postsynaptic activity required for hebbian synaptic plasticity2,3,4,5. One mechanism for the implementation of this cooperation is action potential firing, which begins in the axon, but which can influence synaptic potentiation following active backpropagation into dendrites6. Backpropagation is limited, however, and action potentials often fail to invade the most distal dendrites7,8,9,10. Here we show that long-term potentiation of synapses on the distal dendrites of hippocampal CA1 pyramidal neurons does require cooperative synaptic inputs, but does not require axonal action potential firing and backpropagation. Rather, locally generated and spatially restricted regenerative potentials (dendritic spikes) contribute to the postsynaptic depolarization and calcium entry necessary to trigger potentiation of distal synapses. We find that this mechanism can also function at proximal synapses, suggesting that dendritic spikes participate generally in a form of synaptic potentiation that does not require postsynaptic action potential firing in the axon.},
	author = {Golding, Nace L. and Staff, Nathan P. and Spruston, Nelson},
	copyright = {2002 Macmillan Magazines Ltd.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nature00854},
	issn = {1476-4687},
	journal = {Nature},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	language = {en},
	month = jul,
	note = {Number: 6895 Publisher: Nature Publishing Group},
	number = {6895},
	pages = {326--331},
	title = {Dendritic spikes as a mechanism for cooperative long-term potentiation},
	url = {https://www.nature.com/articles/nature00854},
	urldate = {2022-12-16},
	volume = {418},
	year = {2002},
	bdsk-url-1 = {https://www.nature.com/articles/nature00854},
	bdsk-url-2 = {https://doi.org/10.1038/nature00854}}

@article{Gollisch08,
	abstract = {{\textless}jats:p{\textgreater}Natural vision is a highly dynamic process. Frequent body, head, and eye movements constantly bring new images onto the retina for brief periods, challenging our understanding of the neural code for vision. We report that certain retinal ganglion cells encode the spatial structure of a briefly presented image in the relative timing of their first spikes. This code is found to be largely invariant to stimulus contrast and robust to noisy fluctuations in response latencies. Mechanistically, the observed response characteristics result from different kinetics in two retinal pathways (``ON'' and ``OFF'') that converge onto ganglion cells. This mechanism allows the retina to rapidly and reliably transmit new spatial information with the very first spikes emitted by a neural population.{\textless}/jats:p{\textgreater}},
	author = {Gollisch, Tim and Meister, Markus},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1149639},
	journal = {Science},
	language = {en},
	number = {5866},
	pages = {1108--1111},
	title = {Rapid {Neural} {Coding} in the {Retina} with {Relative} {Spike} {Latencies}},
	url = {https://doi.org/c6czvj},
	volume = {319},
	year = {2008},
	bdsk-url-1 = {https://doi.org/c6czvj},
	bdsk-url-2 = {https://doi.org/10.1126/science.1149639}}

@article{Goltz20,
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems also strive for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. In the time-to-first-spike-coding framework, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of learning such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how it can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the chip's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	author = {G{\"o}ltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {arXiv:1912.11443 [cs, q-bio, stat]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	month = nov,
	note = {arXiv: 1912.11443},
	shorttitle = {Fast and deep},
	title = {Fast and deep: energy-efficient neuromorphic learning with first-spike times},
	url = {http://arxiv.org/abs/1912.11443},
	urldate = {2021-03-02},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/1912.11443}}

@techreport{Goltz21,
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems are optimized for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. With time-to-first-spike coding both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of a learning rule for such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how this mechanism can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the system's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	author = {G{\"o}ltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	number = {1912.11443},
	title = {Fast and energy-efficient neuromorphic deep learning with first-spike times},
	url = {https://arxiv.org/abs/1912.11443},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/1912.11443}}

@article{Gouras60,
	abstract = {Images
null},
	author = {Gouras, P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {0022-3751},
	journal = {The Journal of Physiology},
	keywords = {⛔ No INSPIRE recid found},
	month = jul,
	number = {3},
	pages = {487--505},
	pmcid = {PMC1363334},
	pmid = {13828605},
	title = {Graded potentials of bream retina},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363334/},
	urldate = {2022-12-14},
	volume = {152},
	year = {1960},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363334/}}

@article{Grammont99,
	author = {Grammont, Franck and Riehle, Alexa},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s002210050826},
	journal = {Experimental Brain Research},
	month = sep,
	number = {1-2},
	pages = {118--122},
	title = {Precise spike synchronization in monkey motor cortex involved in preparation for movement},
	url = {https://doi.org/b67khx},
	volume = {128},
	year = {1999},
	bdsk-url-1 = {https://doi.org/b67khx},
	bdsk-url-2 = {https://doi.org/10.1007/s002210050826}}

@article{Grammont03,
	author = {Grammont, F. and Riehle, A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s00422-002-0385-3},
	journal = {Biological Cybernetics},
	number = {5},
	pages = {360--373},
	title = {Spike synchronization and firing rate in a population of motor cortical neurons in relation to movement direction and reaction time},
	url = {https://doi.org/ctvhsb},
	volume = {88},
	year = {2003},
	bdsk-url-1 = {https://doi.org/ctvhsb},
	bdsk-url-2 = {https://doi.org/10.1007/s00422-002-0385-3}}

@inproceedings{Grimaldi21,
	author = {Grimaldi, Antoine and Boutin, Victor and Perrinet, Laurent and Ieng, Sio-Hoi and Benosman, Ryad},
	booktitle = {2021 {International} {Conference} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/cbmi50038.2021.9461901},
	publisher = {IEEE},
	title = {A homeostatic gain control mechanism to improve event-driven object recognition},
	url = {https://doi.org/gkzcrv},
	year = {2021},
	bdsk-url-1 = {https://doi.org/gkzcrv},
	bdsk-url-2 = {https://doi.org/10.1109/cbmi50038.2021.9461901}}

@inproceedings{Grimaldi22,
	abstract = {The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. However, most artificial neuronal models do not take advantage of this minute temporal dimension and here, we develop a model for the efficient detection of temporal spiking motifs based on a layer of neurons with hetero-synaptic delays. Indeed, the variety of synaptic delays on the dendritic tree allows to synchronize synaptic inputs as they reach the basal dendritic tree. We show this can be formalized as a time-invariant logistic regression which can be trained using labelled data. We apply this model to solve the specific computer vision problem of motion detection, and demonstrate its application to synthetic naturalistic videos transformed into event streams similar to the output of event-based cameras. In particular, we quantify how its accuracy can vary with the total computational load. This end-to-end event-driven computational brick could help improve the performance of future Spiking Neural Network (SNN) algorithms and their prospective use in neuromorphic chips.},
	author = {Grimaldi, Antoine and Perrinet, Laurent U},
	booktitle = {2022 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ICIP46576.2022.9897394},
	keywords = {Biological neural networks, Cameras, Delays, Motion detection, Neuromorphics, Neurons, Synchronization, efficient coding, event-based computations, logistic regression, motion detection, spiking neural networks, time code},
	month = oct,
	note = {ISSN: 2381-8549},
	pages = {3591--3595},
	title = {Learning hetero-synaptic delays for motion detection in a single layer of spiking neurons},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/ICIP46576.2022.9897394}}

@article{Grimaldi22a,
	abstract = {We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization.},
	author = {Grimaldi, Antoine and Boutin, Victor and Ieng, Sio-Hoi and Benosman, Ryad and Perrinet, Laurent U},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.36227/techrxiv.18003077.v1},
	journal = {TechRxiv preprint},
	keywords = {efficient coding, event-based vision, homeostasis, neuromorphic hardware, online classification},
	month = jan,
	title = {A robust event-driven approach to always-on object recognition},
	urldate = {2022-01-13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.36227/techrxiv.18003077.v1}}

@article{Grossberger18,
	abstract = {Temporally ordered multi-neuron patterns likely encode information in the brain. We introduce an unsupervised method, SPOTDisClust (Spike Pattern Optimal Transport Dissimilarity Clustering), for their detection from high-dimensional neural ensembles. SPOTDisClust measures similarity between two ensemble spike patterns by determining the minimum transport cost of transforming their corresponding normalized cross-correlation matrices into each other (SPOTDis). Then, it performs density-based clustering based on the resulting inter-pattern dissimilarity matrix. SPOTDisClust does not require binning and can detect complex patterns (beyond sequential activation) even when high levels of out-of-pattern ``noise'' spiking are present. Our method handles efficiently the additional information from increasingly large neuronal ensembles and can detect a number of patterns that far exceeds the number of recorded neurons. In an application to neural ensemble data from macaque monkey V1 cortex, SPOTDisClust can identify different moving stimulus directions on the sole basis of temporal spiking patterns.},
	author = {Grossberger, Lukas and Battaglia, Francesco P. and Vinck, Martin},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/gdvbsx},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	language = {en},
	number = {7},
	pages = {e1006283},
	title = {Unsupervised clustering of temporal patterns in high-dimensional neuronal ensembles using a novel dissimilarity measure},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006283},
	urldate = {2021-11-30},
	volume = {14},
	year = {2018},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006283},
	bdsk-url-2 = {https://doi.org/10/gdvbsx}}

@article{Grun02,
	abstract = {{\textless}jats:p{\textgreater} It has been proposed that cortical neurons organize dynamically into functional groups (cell assemblies) by the temporal structure of their joint spiking activity. Here, we describe a novel method to detect conspicuous patterns of coincident joint spike activity among simultaneously recorded single neurons. The statistical significance of these unitary events of coincident joint spike activity is evaluated by the joint-surprise. The method is tested and calibrated on the basis of simulated, stationary spike trains of independently firing neurons, into which coincident joint spike events were inserted under controlled conditions. The sensitivity and specificity of the method are investigated for their dependence on physiological parameters (firing rate, coincidence precision, coincidence pattern complexity) and temporal resolution of the analysis. In the companion article in this issue, we describe an extension of the method, designed to deal with nonstationary firing rates. {\textless}/jats:p{\textgreater}},
	author = {Gr{\"u}n, Sonja and Diesmann, Markus and Aertsen, Ad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/089976602753284455},
	journal = {Neural Computation},
	language = {en},
	month = jan,
	number = {1},
	pages = {43--80},
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {I}. {Detection} and {Significance}},
	url = {https://doi.org/c63kkn},
	volume = {14},
	year = {2002},
	bdsk-url-1 = {https://doi.org/c63kkn},
	bdsk-url-2 = {https://doi.org/10.1162/089976602753284455}}

@article{Grun02a,
	abstract = {{\textless}jats:p{\textgreater} In order to detect members of a functional group (cell assembly) in simultaneously recorded neuronal spiking activity, we adopted the widely used operational definition that membership in a common assembly is expressed in near-simultaneous spike activity. Unitary event analysis, a statistical method to detect the significant occurrence of coincident spiking activity in stationary data, was recently developed (see the companion article in this issue). The technique for the detection of unitary events is based on the assumption that the underlying processes are stationary in time. This requirement, however, is usually not fulfilled in neuronal data. Here we describe a method that properly normalizes for changes of rate: the unitary events by moving window analysis (UEMWA). Analysis for unitary events is performed separately in overlapping time segments by sliding a window of constant width along the data. In each window, stationarity is assumed. Performance and sensitivity are demonstrated by use of simulated spike trains of independently firing neurons, into which coincident events are inserted. If cortical neurons organize dynamically into functional groups, the occurrence of near-simultaneous spike activity should be time varying and related to behavior and stimuli. UEMWA also accounts for these potentially interesting nonstationarities and allows locating them in time. The potential of the new method is illustrated by results from multiple single-unit recordings from frontal and motor cortical areas in awake, behaving monkey. {\textless}/jats:p{\textgreater}},
	author = {Gr{\"u}n, Sonja and Diesmann, Markus and Aertsen, Ad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/089976602753284464},
	journal = {Neural Computation},
	language = {en},
	month = jan,
	number = {1},
	pages = {81--119},
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {II}. {Nonstationary} {Data}},
	url = {https://doi.org/ffvbkp},
	volume = {14},
	year = {2002},
	bdsk-url-1 = {https://doi.org/ffvbkp},
	bdsk-url-2 = {https://doi.org/10.1162/089976602753284464}}

@incollection{Grun10,
	author = {Gr{\"u}n, Sonja and Diesmann, Markus and Aertsen, Ad},
	booktitle = {Analysis of {Parallel} {Spike} {Trains}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	pages = {191--220},
	publisher = {Springer US},
	title = {Unitary {Event} {Analysis}},
	url = {https://doi.org/dxgxt9},
	year = {2010},
	bdsk-url-1 = {https://doi.org/dxgxt9}}

@article{Guise14,
	abstract = {{\textless}jats:p{\textgreater} A significant feature of spiking neural networks with varying connection delays, such as those in the brain, is the existence of strongly connected groups of neurons known as polychronous neural groups (PNGs). Polychronous groups are found in large numbers in these networks and are proposed by Izhikevich ( 2006a ) to provide a neural basis for representation and memory. When exposed to a familiar stimulus, spiking neural networks produce consistencies in the spiking output data that are the hallmarks of PNG activation. Previous methods for studying the PNG activation response to stimuli have been limited by the template-based methods used to identify PNG activation. In this letter, we outline a new method that overcomes these difficulties by establishing for the first time a probabilistic interpretation of PNG activation. We then demonstrate the use of this method by investigating the claim that PNGs might provide the foundation of a representational system. {\textless}/jats:p{\textgreater}},
	author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/neco_a_00620},
	journal = {Neural Computation},
	language = {en},
	month = sep,
	number = {9},
	pages = {2052--2073},
	title = {A {Bayesian} {Model} of {Polychronicity}},
	url = {https://doi.org/f6chbq},
	volume = {26},
	year = {2014},
	bdsk-url-1 = {https://doi.org/f6chbq},
	bdsk-url-2 = {https://doi.org/10.1162/neco_a_00620}}

@article{Gutig14,
	abstract = {Recent experimental reports have suggested that cortical networks can operate in regimes were sensory information is encoded by relatively small populations of spikes and their precise relative timing. Combined with the discovery of spike timing dependent plasticity, these findings have sparked growing interest in the capabilities of neurons to encode and decode spike timing based neural representations. To address these questions, a novel family of methodologically diverse supervised learning algorithms for spiking neuron models has been developed. These models have demonstrated the high capacity of simple neural architectures to operate also beyond the regime of the well established independent rate codes and to utilize theoretical advantages of spike timing as an additional coding dimension.},
	author = {G{\"u}tig, Robert},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.conb.2014.01.004},
	issn = {0959-4388},
	journal = {Current Opinion in Neurobiology},
	language = {en},
	month = apr,
	pages = {134--139},
	series = {Theoretical and computational neuroscience},
	title = {To spike, or when to spike?},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438814000129},
	urldate = {2022-05-13},
	volume = {25},
	year = {2014},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0959438814000129},
	bdsk-url-2 = {https://doi.org/10.1016/j.conb.2014.01.004}}

@article{Gutig06,
	author = {G{\"u}tig, Robert and Sompolinsky, Haim},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nn1643},
	journal = {Nature Neuroscience},
	language = {en},
	number = {3},
	pages = {420--428},
	title = {The tempotron: a neuron that learns spike timing--based decisions},
	url = {https://doi.org/ch29r4},
	volume = {9},
	year = {2006},
	bdsk-url-1 = {https://doi.org/ch29r4},
	bdsk-url-2 = {https://doi.org/10.1038/nn1643}}

@article{Haessig20,
	abstract = {Precise spike timing and temporal coding are used extensively within the nervous system of insects and in the sensory periphery of higher order animals. However, conventional Artificial Neural Networks (ANNs) and machine learning algorithms cannot take advantage of this coding strategy, due to their rate-based representation of signals. Even in the case of artificial Spiking Neural Networks (SNNs), identifying applications where temporal coding outperforms the rate coding strategies of ANNs is still an open challenge. Neuromorphic sensory-processing systems provide an ideal context for exploring the potential advantages of temporal coding, as they are able to efficiently extract the information required to cluster or classify spatio-temporal activity patterns from relative spike timing. Here we propose a neuromorphic model inspired by the sand scorpion to explore the benefits of temporal coding, and validate it in an event-based sensory-processing task. The task consists in localizing a target using only the relative spike timing of eight spatially-separated vibration sensors. We propose two different approaches in which the SNNs learns to cluster spatio-temporal patterns in an unsupervised manner and we demonstrate how the task can be solved both analytically and through numerical simulation of multiple SNN models. We argue that the models presented are optimal for spatio-temporal pattern classification using precise spike timing in a task that could be used as a standard benchmark for evaluating event-based sensory processing models based on temporal coding.},
	author = {Haessig, Germain and Milde, Moritz B. and Aceituno, Pau Vilimelis and Oubari, Omar and Knight, James C. and van Schaik, Andr{\'e} and Benosman, Ryad B. and Indiveri, Giacomo},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/ghvnxj},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	pages = {420},
	title = {Event-{Based} {Computation} for {Touch} {Localization} {Based} on {Precise} {Spike} {Timing}},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00420},
	urldate = {2021-10-20},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.frontiersin.org/article/10.3389/fnins.2020.00420},
	bdsk-url-2 = {https://doi.org/10/ghvnxj}}

@article{Haimerl19,
	abstract = {The hippocampus plays a critical role in episodic memory: the sequential representation of visited places and experienced events. This function is mirrored by hippocampal activity that self organizes into sequences of neuronal activation that integrate spatiotemporal information. What are the underlying mechanisms of such integration is still unknown. Single cell activity was recently shown to combine time and distance information; however, it remains unknown whether a degree of tuning between space and time can be defined at the network level. Here, combining daily calcium imaging of CA1 sequence dynamics in running head-fixed mice and network modeling, we show that CA1 network activity tends to represent a specific combination of space and time at any given moment, and that the degree of tuning can shift within a continuum from 1 day to the next. Our computational model shows that this shift in tuning can happen under the control of the external drive power. We propose that extrinsic global inputs shape the nature of spatiotemporal integration in the hippocampus at the population level depending on the task at hand, a hypothesis which may guide future experimental studies.},
	author = {Haimerl, Caroline and Angulo-Garcia, David and Villette, Vincent and Reichinnek, Susanne and Torcini, Alessandro and Cossart, Rosa and Malvache, Arnaud},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.1718518116},
	issn = {0027-8424, 1091-6490},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {attractor network, hippocampus, neural model, space representation, time representation},
	language = {en},
	month = apr,
	number = {15},
	pages = {7477--7482},
	title = {Internal representation of hippocampal neuronal population spans a time-distance continuum},
	url = {https://www.pnas.org/content/116/15/7477},
	urldate = {2022-01-17},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/content/116/15/7477},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1718518116}}

@article{Harris03,
	author = {Harris, Kenneth D. and Csicsvari, Jozsef and Hirase, Hajime and Dragoi, George and Buzs{\'a}ki, Gy{\"o}rgy},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nature01834},
	journal = {Nature},
	language = {en},
	number = {6948},
	pages = {552--556},
	title = {Organization of cell assemblies in the hippocampus},
	url = {https://doi.org/bm3vgb},
	volume = {424},
	year = {2003},
	bdsk-url-1 = {https://doi.org/bm3vgb},
	bdsk-url-2 = {https://doi.org/10.1038/nature01834}}

@article{Hazan18,
	abstract = {The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared toward machine learning and reinforcement learning. Our software, called BindsNET1, enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on the PyTorch deep neural networks library, facilitating the implementation of spiking neural networks on fast CPU and GPU computational platforms. Moreover, the BindsNET framework can be adjusted to utilize other existing computing and hardware backends; e.g., TensorFlow and SpiNNaker. We provide an interface with the OpenAI gym library, allowing for training and evaluation of spiking networks on reinforcement learning environments. We argue that this package facilitates the use of spiking networks for large-scale machine learning problems and show some simple examples by using BindsNET in practice.},
	author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fninf.2018.00089},
	issn = {1662-5196},
	journal = {Frontiers in Neuroinformatics},
	keywords = {⛔ No INSPIRE recid found},
	shorttitle = {{BindsNET}},
	title = {{BindsNET}: {A} {Machine} {Learning}-{Oriented} {Spiking} {Neural} {Networks} {Library} in {Python}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00089},
	urldate = {2022-11-14},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00089},
	bdsk-url-2 = {https://doi.org/10.3389/fninf.2018.00089}}

@misc{Hazan22,
	abstract = {A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection.},
	author = {Hazan, Hananel and Caby, Simon and Earl, Christopher and Siegelmann, Hava and Levin, Michael},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Computation},
	month = feb,
	note = {arXiv:2202.07132 [cs, q-bio, stat]},
	publisher = {arXiv},
	title = {Memory via {Temporal} {Delays} in weightless {Spiking} {Neural} {Network}},
	url = {http://arxiv.org/abs/2202.07132},
	urldate = {2022-10-27},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2202.07132}}

@book{Hebb49,
	address = {New York},
	author = {Hebb, Donald O.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {bicv-sparse},
	publisher = {Wiley},
	title = {The organization of behavior: {A} neuropsychological theory},
	year = {1949}}

@article{Hidalgo-Carrio20,
	abstract = {Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous events instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50\% improvement in average depth error with respect to previous event-based methods.},
	author = {Hidalgo-Carri{\'o}, Javier and Gehrig, Daniel and Scaramuzza, Davide},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {arXiv:2010.08350 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	month = oct,
	note = {arXiv: 2010.08350},
	title = {Learning {Monocular} {Dense} {Depth} from {Events}},
	url = {http://arxiv.org/abs/2010.08350},
	urldate = {2021-01-22},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/2010.08350}}

@inproceedings{Hill17,
	abstract = {Unlike general purpose computer architectures that are comprised of complex processor cores and sequential computation, the brain is innately parallel and contains highly complex connections between computational units (neurons). Key to the architecture of the brain is a functionality enabled by the combined effect of spiking communication and sparse connectivity with unique variable efficacies and temporal latencies. Utilizing these neuroscience principles, we have developed the Spiking Temporal Processing Unit (STPU) architecture which is well-suited for areas such as pattern recognition and natural language processing. In this paper, we formally describe the STPU, implement the STPU on a field programmable gate array, and show measured performance data.},
	author = {Hill, Aaron J. and Donaldson, Jonathon W. and Rothganger, Fredrick H. and Vineyard, Craig M. and Follett, David R. and Follett, Pamela L. and Smith, Michael R. and Verzi, Stephen J. and Severa, William and Wang, Felix and Aimone, James B. and Naegle, John H. and James, Conrad D.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Rebooting} {Computing} ({ICRC})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ICRC.2017.8123631},
	keywords = {Biological system modeling, Computer architecture, Delays, Neuromorphics, Neurons},
	month = nov,
	pages = {1--8},
	title = {A {Spike}-{Timing} {Neuromorphic} {Architecture}},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/ICRC.2017.8123631}}

@article{Hogendoorn19,
	author = {Hogendoorn, Hinze and Burkitt, Anthony N.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/eneuro.0412-18.2019},
	issn = {2373-2822},
	journal = {eneuro},
	language = {en},
	month = mar,
	number = {2},
	pages = {ENEURO.0412--18.2019},
	shorttitle = {Predictive {Coding} with {Neural} {Transmission} {Delays}},
	title = {Predictive {Coding} with {Neural} {Transmission} {Delays}: {A} {Real}-{Time} {Temporal} {Alignment} {Hypothesis}},
	url = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	urldate = {2019-11-12},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	bdsk-url-2 = {https://doi.org/10.1523/eneuro.0412-18.2019}}

@article{Hubel68,
	author = {Hubel, David H and Wiesel, Torsten N},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1113/jphysiol.1968.sp008455},
	issn = {1469-7793},
	journal = {The Journal of Physiology},
	keywords = {area-v1, bicv-motion, bicv-sparse},
	language = {english},
	number = {1},
	pages = {215--243},
	title = {Receptive fields and functional architecture of monkey striate cortex},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1968.sp008455},
	volume = {195},
	year = {1968},
	bdsk-url-1 = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1968.sp008455},
	bdsk-url-2 = {https://doi.org/10.1113/jphysiol.1968.sp008455}}

@article{Huning98,
	abstract = {We present rules for the unsupervised learning of coincidence between excitatory postsynaptic potentials (EPSPs) by the adjustment of post-synaptic delays between the transmitter binding and the opening of ion channels. Starting from a gradient descent scheme, we develop a robust and more biological threshold rule by which EPSPs from different synapses can be gradually pulled into coincidence. The synaptic delay changes are determined from the summed potential---at the site where the coincidence is to be established---and from postulated synaptic learning functions that accompany the individual EPSPs. According to our scheme, templates for the detection of spatiotemporal patterns of synaptic activation can be learned, which is demonstrated by computer simulation. Finally, we discuss possible relations to biological mechanisms.},
	author = {H{\"u}ning, Harald and Gl{\"u}nder, Helmut and Palm, G{\"u}nther},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/cthfps},
	issn = {0899-7667},
	journal = {Neural Computation},
	month = apr,
	number = {3},
	pages = {555--565},
	title = {Synaptic {Delay} {Learning} in {Pulse}-{Coupled} {Neurons}},
	url = {https://doi.org/10.1162/089976698300017665},
	urldate = {2021-09-16},
	volume = {10},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1162/089976698300017665},
	bdsk-url-2 = {https://doi.org/10/cthfps}}

@inproceedings{Hussain12,
	abstract = {We present a neuromorphic spiking neural network, the DELTRON, that can remember and store patterns by changing the delays of every connection as opposed to modifying the weights. The advantage of this architecture over traditional weight based ones is simpler hardware implementation without multipliers or digital-analog converters (DACs). The name is derived due to similarity in the learning rule with an earlier architecture called Tempotron. We present simulations of memory capacity of the DELTRON for different random spatio-temporal spike patterns and also present SPICE simulation results of the core circuits involved in a reconfigurable mixed signal implementation of this architecture.},
	author = {Hussain, Shaista and Basu, Arindam and Wang, Mark and Hamilton, Tara Julia},
	booktitle = {2012 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/APCCAS.2012.6419032},
	keywords = {Computer architecture, Delay, Nerve fibers, Neuromorphics, Registers, Training},
	month = dec,
	pages = {304--307},
	shorttitle = {{DELTRON}},
	title = {{DELTRON}: {Neuromorphic} architectures for delay based learning},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/APCCAS.2012.6419032}}

@article{Ikegaya04,
	author = {Ikegaya, Yuji and Aaron, Gloster and Cossart, Rosa and Aronov, Dmitriy and Lampl, Ilan and Ferster, David and Yuste, Rafael},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/djckcn},
	journal = {Science},
	keywords = {polychronization},
	month = apr,
	number = {5670},
	pages = {559--564},
	shorttitle = {Synfire {Chains} and {Cortical} {Songs}},
	title = {Synfire {Chains} and {Cortical} {Songs}: {Temporal} {Modules} of {Cortical} {Activity}},
	url = {http://www.science.org/doi/10.1126/science.1093173},
	urldate = {2021-11-29},
	volume = {304},
	year = {2004},
	bdsk-url-1 = {http://www.science.org/doi/10.1126/science.1093173},
	bdsk-url-2 = {https://doi.org/10/djckcn}}

@article{Isbister21,
	abstract = {How information in the nervous system is encoded by patterns of action potentials (i.e. spikes) remains an open question. Multi-neuron patterns of single spikes are a prime candidate for spike time encoding but their temporal variability requires further characterisation. Here we show how known sources of spike count variability affect stimulus-evoked spike time patterns between neurons separated over multiple layers and columns of adult rat somatosensory cortex in vivo. On subsets of trials (clusters) and after controlling for stimulus-response adaptation, spike time differences between pairs of neurons are ``time-warped'' (compressed/stretched) by trial-to-trial changes in shared excitability, explaining why fixed spike time patterns and noise correlations are seldom reported. We show that predicted cortical state is correlated between groups of 4 neurons, introducing the possibility of spike time pattern modulation by population-wide trial-to-trial changes in excitability (i.e. cortical state). Under the assumption of state-dependent coding, we propose an improved potential encoding capacity.},
	author = {Isbister, James B. and Reyes-Puerta, Vicente and Sun, Jyh-Jang and Horenko, Illia and Luhmann, Heiko J.},
	copyright = {2021 The Author(s)},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41598-021-94002-0},
	issn = {2045-2322},
	journal = {Scientific Reports},
	language = {en},
	month = jul,
	note = {Number: 1 Publisher: Nature Publishing Group},
	number = {1},
	pages = {15066},
	title = {Clustering and control for adaptation uncovers time-warped spike time patterns in cortical networks in vivo},
	url = {https://www.nature.com/articles/s41598-021-94002-0},
	urldate = {2022-10-06},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://www.nature.com/articles/s41598-021-94002-0},
	bdsk-url-2 = {https://doi.org/10.1038/s41598-021-94002-0}}

@article{Izhikevich06,
	abstract = {We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as ``attention to memories.''},
	author = {Izhikevich, Eugene M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/bgh4qv},
	issn = {0899-7667},
	journal = {Neural Computation},
	month = feb,
	note = {00000},
	number = {2},
	pages = {245--282},
	shorttitle = {Polychronization},
	title = {Polychronization: {Computation} with {Spikes}},
	url = {https://doi.org/10.1162/089976606775093882},
	urldate = {2018-09-24},
	volume = {18},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1162/089976606775093882},
	bdsk-url-2 = {https://doi.org/10/bgh4qv}}

@article{Izhikevich09,
	abstract = {There is great interest in methods for computing that do not involve digital machines. Many computational paradigms were inspired by brain research, such as Boolean neuronal logic [McCulloch \& Pitts, 1943], the perceptron [Rosenblatt, 1958], attractor neural networks [Hopfield, 1982] and cellular neural nets [Chua \& Yang, 1988]. All these paradigms abstract biological circuits to artificial neural networks, i.e. interconnected units (neurons) that perform computations based on the connections between the units (synapses). Here we present a novel computational framework based on polychronous wavefront dynamics. It is entirely different from an artificial neural network paradigm, rather it is based on temporal and spatial patterns of activity in pulse-propagating media and their interaction with transponders, which create pulses in response to receiving appropriate inputs, e.g. two coincident input pulses. A pulse propagates as a circular wave from its source to other transponders. Computations result from interactions between transponders, and they are encoded by the exact physical locations of transponders and by precise timings of pulses. We illustrate temporal pattern recognition, reverberating memory, temporal signal analysis and basic logical operations using polychronous wavefront computations. This work reveals novel principles for designing nanoscale computational devices.},
	author = {Izhikevich, Eugene M. and Hoppensteadt, Frank C.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/db98d7},
	issn = {0218-1274, 1793-6551},
	journal = {International Journal of Bifurcation and Chaos},
	keywords = {polychronization},
	language = {en},
	month = may,
	number = {05},
	pages = {1733--1739},
	title = {Polychronous {Wavefront} {Computations}},
	url = {https://www.izhikevich.org/publications/polychronous_wavefront_computations.htm},
	urldate = {2019-09-10},
	volume = {19},
	year = {2009},
	bdsk-url-1 = {https://www.izhikevich.org/publications/polychronous_wavefront_computations.htm},
	bdsk-url-2 = {https://doi.org/10/db98d7}}

@article{Javanshir22,
	abstract = {Artificial neural networks (ANNs) have experienced a rapid advancement for their success in various application domains, including autonomous driving and drone vision. Researchers have been improving the performance efficiency and computational requirement of ANNs inspired by the mechanisms of the biological brain. Spiking neural networks (SNNs) provide a power-efficient and brain-inspired computing paradigm for machine learning applications. However, evaluating large-scale SNNs on classical von Neumann architectures (central processing units/graphics processing units) demands a high amount of power and time. Therefore, hardware designers have developed neuromorphic platforms to execute SNNs in and approach that combines fast processing and low power consumption. Recently, field-programmable gate arrays (FPGAs) have been considered promising candidates for implementing neuromorphic solutions due to their varied advantages, such as higher flexibility, shorter design, and excellent stability. This review aims to describe recent advances in SNNs and the neuromorphic hardware platforms (digital, analog, hybrid, and FPGA based) suitable for their implementation. We present that biological background of SNN learning, such as neuron models and information encoding techniques, followed by a categorization of SNN training. In addition, we describe state-of-the-art SNN simulators. Furthermore, we review and present FPGA-based hardware implementation of SNNs. Finally, we discuss some future directions for research in this field.},
	author = {Javanshir, Amirhossein and Nguyen, Thanh Thi and Mahmud, M. A. Parvez and Kouzani, Abbas Z.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/neco_a_01499},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {⛔ No INSPIRE recid found},
	month = may,
	number = {6},
	pages = {1289--1328},
	title = {Advancements in {Algorithms} and {Neuromorphic} {Hardware} for {Spiking} {Neural} {Networks}},
	url = {https://doi.org/10.1162/neco_a_01499},
	urldate = {2022-12-16},
	volume = {34},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_01499}}

@article{Jazayeri06,
	author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nn1691},
	journal = {Nature neuroscience},
	note = {Publisher: Nature Publishing Group},
	number = {5},
	pages = {690--696},
	title = {Optimal representation of sensory information by neural populations},
	volume = {9},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1038/nn1691}}

@article{Jeffress48,
	abstract = {The author presents a place theory of sound localization based upon the time difference of stimulation of the 2 ears. The hypothesis depends upon the known slow rate of conduction in small nerve fibers and the phenomenon of spatial summation. It assumes that some secondary fibers of the auditory tract divide, sending branches homolaterally and contralaterally. There is a further assumption that these neurons make synaptic connection with other fibers on each side, the latter neurones synapsing with both contralateral and homolateral neurones. Then, if the sound is in a median plane, the summation effect would be maximal in a central group of the tertiary fibers on each side. If the sound source is shifted, the summation effect would result in a shifting of the transmission through synapses in the tertiary zone. This provides a spatial change in the pattern of nerve discharge as a consequence of a temporal change in the binaural stimulation. The anatomical location of such a center is suggested either in the inferior colliculus or the medial geniculate body. Possible experimental procedures are suggested. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	author = {Jeffress, Lloyd A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1037/h0061495},
	issn = {0021-9940},
	journal = {Journal of Comparative and Physiological Psychology},
	keywords = {Auditory Localization, Ear (Anatomy), Neurons, Temporal Frequency, ⛔ No INSPIRE recid found},
	note = {Place: US Publisher: American Psychological Association},
	pages = {35--39},
	title = {A place theory of sound localization},
	volume = {41},
	year = {1948},
	bdsk-url-1 = {https://doi.org/10.1037/h0061495}}

@misc{Jeremie22,
	abstract = {Humans are able to categorize images very efficiently, in particular to detect very rapidly the presence of an animal. Recently, deep learning algorithms have achieved higher accuracy than humans for a large set of visual recognition tasks. However, the tasks on which these artificial networks are usually trained and evaluated are usually very specialized which do not generalize well, for example with an accuracy drop following a rotation of the image. In this regard, biological visual systems are more flexible and efficient than artificial systems for more generic tasks, such as detecting an animal. To further the comparison between biological and artificial neural networks, we retrained the standard VGG16 convolutional neural network (CNN) on two independent tasks that are ecologically relevant to humans: detecting the presence of an animal or an artifact. We show that retraining the network achieves a human-like level of performance, comparable to what is reported in psychophysical tasks. Moreover, we show that categorization is better when combining the models' outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). Furthermore, these re-trained models were able to reproduce some unexpected behavioral observations of human psychophysics, such as robustness to rotations (e.g., an upside-down or tilted image) or to a grayscale transformation. Finally, we quantified the number of CNN layers needed to achieve such performance, showing that good accuracy for ultrafast image categorization could be achieved with only a few layers, challenging the belief that image recognition would require a deep sequential analysis of visual objects.},
	author = {J{\'e}r{\'e}mie, Jean-Nicolas and Perrinet, Laurent U.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	month = oct,
	note = {arXiv:2205.03635 [cs, q-bio]},
	publisher = {arXiv},
	title = {Ultrafast image categorization in vivo and in silico},
	url = {http://arxiv.org/abs/2205.03635},
	urldate = {2022-10-26},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2205.03635}}

@article{Johansson04,
	author = {Johansson, Roland S and Birznieks, Ingvars},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nn1177},
	journal = {Nature Neuroscience},
	language = {en},
	month = jan,
	number = {2},
	pages = {170--177},
	title = {First spikes in ensembles of human tactile afferents code complex spatial fingertip events},
	url = {https://doi.org/dqstpm},
	volume = {7},
	year = {2004},
	bdsk-url-1 = {https://doi.org/dqstpm},
	bdsk-url-2 = {https://doi.org/10.1038/nn1177}}

@article{Kaiser22,
	abstract = {BrainScaleS-2 is an accelerated and highly configurable neuromorphic system with physical models of neurons and synapses. Beyond networks of spiking point neurons, it allows for the implementation of user-defined neuron morphologies. Both passive propagation of electric signals between compartments as well as dendritic spikes and plateau potentials can be emulated. In this paper, three multi-compartment neuron morphologies are chosen to demonstrate passive propagation of postsynaptic potentials, spatio-temporal coincidence detection of synaptic inputs in a dendritic branch, and the replication of the BAC burst firing mechanism found in layer 5 pyramidal neurons of the neocortex.},
	author = {Kaiser, Jakob and Billaudelle, Sebastian and M{\"u}ller, Eric and Tetzlaff, Christian and Schemmel, Johannes and Schmitt, Sebastian},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuroscience.2021.08.013},
	issn = {0306-4522},
	journal = {Neuroscience},
	keywords = {AdEx neuron model, accelerated technology, mixed-signal neuromorphic, multi-compartmental models, physical model},
	language = {en},
	month = may,
	pages = {290--300},
	series = {Dendritic contributions to biological and artificial computations},
	title = {Emulating {Dendritic} {Computing} {Paradigms} on {Analog} {Neuromorphic} {Hardware}},
	url = {https://www.sciencedirect.com/science/article/pii/S0306452221004218},
	urldate = {2022-11-10},
	volume = {489},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0306452221004218},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuroscience.2021.08.013}}

@article{Kaplan13,
	abstract = {Predictive coding hypothesizes that the brain explicitly infers upcoming sensory input to establish a coherent representation of the world. Although it is becoming generally accepted, it is not clear on which level spiking neural networks may implement predictive coding and what function their connectivity may have. We present a network model of conductance-based integrate-and-fire neurons inspired by the architecture of retinotopic cortical areas that assumes predictive coding is implemented through network connectivity, namely in the connection delays and in selectiveness for the tuning properties of source and target cells. We show that the applied connection pattern leads to motion-based prediction in an experiment tracking a moving dot. In contrast to our proposed model, a network with random or isotropic connectivity fails to predict the path when the moving dot disappears. Furthermore, we show that a simple linear decoding approach is sufficient to transform neuronal spiking activity into a probabilistic estimate for reading out the target trajectory.},
	author = {Kaplan, Bernhard A and Lansner, Anders and Masson, Guillaume S and Perrinet, Laurent U},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fncom.2013.00112},
	journal = {Frontiers in Computational Neuroscience},
	month = sep,
	note = {Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Kaplan13},
	number = {112},
	title = {Anisotropic connectivity implements motion-based prediction in a spiking neural network},
	url = {https://laurentperrinet.github.io/publication/kaplan-13},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://laurentperrinet.github.io/publication/kaplan-13},
	bdsk-url-2 = {https://doi.org/10.3389/fncom.2013.00112}}

@article{Kashiwadani99,
	abstract = {Individual glomeruli in the mammalian olfactory bulb represent a single or a few type(s) of odorant receptors. Signals from different types of receptors are thus sorted out into different glomeruli. How does the neuronal circuit in the olfactory bulb contribute to the combination and integration of signals received by different glomeruli? Here we examined electrophysiologically whether there were functional interactions between mitral/tufted cells associated with different glomeruli in the rabbit olfactory bulb. First, we made simultaneous recordings of extracellular single-unit spike responses of mitral/tufted cells and oscillatory local field potentials in the dorsomedial fatty acid--responsive region of the olfactory bulb in urethan-anesthetized rabbits. Using periodic artificial inhalation, the olfactory epithelium was stimulated with a homologous series ofn-fatty acids or n-aliphatic aldehydes. The odor-evoked spike discharges of mitral/tufted cells tended to phase-lock to the oscillatory local field potential, suggesting that spike discharges of many cells occur synchronously during odor stimulation. We then made simultaneous recordings of spike discharges from pairs of mitral/tufted cells located 300--500 μm apart and performed a cross-correlation analysis of their spike responses to odor stimulation. In ∼27\% of cell pairs examined, two cells with distinct molecular receptive ranges showed synchronized oscillatory discharges when olfactory epithelium was stimulated with one or a mixture of odorant(s) effective in activating both. The results suggest that the neuronal circuit in the olfactory bulb causes synchronized spike discharges of specific pairs of mitral/tufted cells associated with different glomeruli and the synchronization of odor-evoked spike discharges may contribute to the temporal binding of signals derived from different types of odorant receptor.},
	author = {Kashiwadani, Hideki and Sasaki, Yasnory F. and Uchida, Naoshige and Mori, Kensaku},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.1999.82.4.1786},
	issn = {0022-3077},
	journal = {Journal of Neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	month = oct,
	note = {Publisher: American Physiological Society},
	number = {4},
	pages = {1786--1792},
	title = {Synchronized {Oscillatory} {Discharges} of {Mitral}/{Tufted} {Cells} {With} {Different} {Molecular} {Receptive} {Ranges} in the {Rabbit} {Olfactory} {Bulb}},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.1999.82.4.1786},
	urldate = {2022-12-16},
	volume = {82},
	year = {1999},
	bdsk-url-1 = {https://journals.physiology.org/doi/full/10.1152/jn.1999.82.4.1786},
	bdsk-url-2 = {https://doi.org/10.1152/jn.1999.82.4.1786}}

@article{Kass05,
	author = {Kass, Robert E. and Ventura, Val{\'e}rie and Brown, Emery N.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.00648.2004},
	journal = {Journal of neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: American Physiological Society},
	number = {1},
	pages = {8--25},
	title = {Statistical issues in the analysis of neuronal data},
	volume = {94},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1152/jn.00648.2004}}

@article{Kenyon04,
	author = {Kenyon, Garrett T and Hill, Dan and Theiler, James and George, John S and Marshak, David W},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neunet.2004.05.005},
	journal = {Neural Networks},
	language = {en},
	number = {5-6},
	pages = {773--786},
	title = {A theory of the {Benham} {Top} based on center--surround interactions in the parvocellular pathway},
	url = {https://doi.org/bjwzzt},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://doi.org/bjwzzt},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2004.05.005}}

@article{Kerr13,
	author = {Kerr, Robert R. and Burkitt, Anthony N. and Thomas, Doreen A. and Gilson, Matthieu and Grayden, David B.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1002897},
	editor = {Morrison, Abigail},
	journal = {PLoS Computational Biology},
	language = {en},
	number = {2},
	pages = {e1002897},
	title = {Delay {Selection} by {Spike}-{Timing}-{Dependent} {Plasticity} in {Recurrent} {Networks} of {Spiking} {Neurons} {Receiving} {Oscillatory} {Inputs}},
	url = {https://doi.org/f4rm5j},
	volume = {9},
	year = {2013},
	bdsk-url-1 = {https://doi.org/f4rm5j},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1002897}}

@article{Keysers01,
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}
               {\textless}jats:p{\textgreater}Macaque monkeys were presented with continuous rapid serial visual presentation (RSVP) sequences of unrelated naturalistic images at rates of 14-222 msec/image, while neurons that responded selectively to complex patterns (e.g., faces) were recorded in temporal cortex. Stimulus selectivity was preserved for 65\% of these neurons even at surprisingly fast presentation rates (14 msec/image or 72 images/sec). Five human subjects were asked to detect or remember images under equivalent conditions. Their performance in both tasks was above chance at all rates (14-111 msec/image). The performance of single neurons was comparable to that of humans and responded in a similar way to changes in presentation rate. The implications for the role of temporal cortex cells in perception are discussed.{\textless}/jats:p{\textgreater}},
	author = {Keysers, C. and Xiao, D.-K. and F{\"o}ldi{\'a}k, P. and Perrett, D. I.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/089892901564199},
	journal = {Journal of Cognitive Neuroscience},
	language = {en},
	month = jan,
	number = {1},
	pages = {90--101},
	title = {The {Speed} of {Sight}},
	url = {https://doi.org/cfdjtg},
	volume = {13},
	year = {2001},
	bdsk-url-1 = {https://doi.org/cfdjtg},
	bdsk-url-2 = {https://doi.org/10.1162/089892901564199}}

@article{Kheradpisheh18,
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timoth{\'e}e},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neunet.2017.12.005},
	journal = {Neural Networks},
	language = {en},
	month = mar,
	pages = {56--67},
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	url = {https://doi.org/gc6dqh},
	volume = {99},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gc6dqh},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2017.12.005}}

@article{Khoei17,
	abstract = {Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object's motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects' position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation at the present time of spatial information in the visual pathways.},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1005068},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	keywords = {Coding mechanisms, Extrapolation, Motion, Psychophysics, Sensory perception, Velocity, Vision, Visual system},
	language = {en},
	month = jan,
	note = {Publisher: Public Library of Science},
	number = {1},
	pages = {e1005068},
	title = {The {Flash}-{Lag} {Effect} as a {Motion}-{Based} {Predictive} {Shift}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	urldate = {2022-08-31},
	volume = {13},
	year = {2017},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1005068}}

@article{Khoei13,
	abstract = {During normal viewing, the continuous stream of visual input is regularly interrupted, for instance by blinks of the eye. Despite these frequents blanks (that is the transient absence of a raw sensory source), the visual system is most often able to maintain a continuous representation of motion. For instance, it maintains the movement of the eye such as to stabilize the image of an object. This ability suggests the existence of a generic neural mechanism of motion extrapolation to deal with fragmented inputs. In this paper, we have modeled how the visual system may extrapolate the trajectory of an object during a blank using motion-based prediction. This implies that using a prior on the coherency of motion, the system may integrate previous motion information even in the absence of a stimulus. In order to compare with experimental results, we simulated tracking velocity responses. We found that the response of the motion integration process to a blanked trajectory pauses at the onset of the blank, but that it quickly recovers the information on the trajectory after reappearance. This is compatible with behavioral and neural observations on motion extrapolation. To understand these mechanisms, we have recorded the response of the model to a noisy stimulus. Crucially, we found that motion-based prediction acted at the global level as a gain control mechanism and that we could switch from a smooth regime to a binary tracking behavior where the dot is tracked or lost. Our results imply that a local prior implementing motion-based prediction is sufficient to explain a large range of neural and behavioral results at a more global level. We show that the tracking behavior deteriorates for sensory noise levels higher than a certain value, where motion coherency and predictability fail to hold longer. In particular, we found that motion-based prediction leads to the emergence of a tracking behavior only when enough information from the trajectory has been accumulated. Then, during tracking, trajectory estimation is robust to blanks even in the presence of relatively high levels of noise. Moreover, we found that tracking is necessary for motion extrapolation, this calls for further experimental work exploring the role of noise in motion extrapolation.},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.jphysparis.2013.08.001},
	issn = {09284257},
	journal = {Journal of Physiology-Paris},
	language = {en},
	month = nov,
	number = {5},
	pages = {409--420},
	title = {Motion-based prediction explains the role of tracking in motion extrapolation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092842571300051X},
	urldate = {2022-10-17},
	volume = {107},
	year = {2013},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S092842571300051X},
	bdsk-url-2 = {https://doi.org/10.1016/j.jphysparis.2013.08.001}}

@article{Kilavik09,
	author = {Kilavik, B. E. and Roux, S. and Ponce-Alvarez, A. and Confais, J. and Grun, S. and Riehle, A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.1554-09.2009},
	journal = {Journal of Neuroscience},
	language = {en},
	month = oct,
	number = {40},
	pages = {12653--12663},
	title = {Long-{Term} {Modifications} in {Motor} {Cortical} {Dynamics} {Induced} by {Intensive} {Practice}},
	url = {https://doi.org/bf84ps},
	volume = {29},
	year = {2009},
	bdsk-url-1 = {https://doi.org/bf84ps},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.1554-09.2009}}

@inproceedings{Kim16,
	abstract = {We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.},
	address = {Cham},
	author = {Kim, Hanme and Leutenegger, Stefan and Davison, Andrew J.},
	booktitle = {Computer {Vision} -- {ECCV} 2016},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/978-3-319-46466-4_21},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	isbn = {978-3-319-46466-4},
	keywords = {3D reconstruction, 6-DoF tracking, Event-based camera, Intensity reconstruction, SLAM, Visual odometry},
	language = {en},
	pages = {349--364},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {3D} {Reconstruction} and 6-{DoF} {Tracking} with an {Event} {Camera}},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-46466-4_21}}

@article{Kirchner06,
	abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	author = {Kirchner, H and Thorpe, Sj},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.visres.2005.10.002},
	issn = {0042-6989},
	journal = {Vision Research},
	note = {Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Kirchner06},
	number = {11},
	pages = {1762--76},
	shorttitle = {Ultra-rapid object detection with saccadic eye movements},
	title = {Ultra-rapid object detection with saccadic eye movements: {Visual} processing speed revisited},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698905005110},
	volume = {46},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0042698905005110},
	bdsk-url-2 = {https://doi.org/10.1016/j.visres.2005.10.002}}

@misc{Kohn16,
	author = {Kohn, A. and Smith, M.A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.6080/K0NC5Z4X},
	keywords = {Macaque, Neuroscience, Primary visual cortex, ⛔ No INSPIRE recid found},
	language = {en},
	publisher = {CRCNS.org},
	title = {Utah array extracellular recordings of spontaneous and visually evoked activity from anesthetized macaque primary visual cortex ({V1}).},
	url = {http://crcns.org/data-sets/vc/pvc-11},
	urldate = {2022-12-19},
	year = {2016},
	bdsk-url-1 = {http://crcns.org/data-sets/vc/pvc-11},
	bdsk-url-2 = {https://doi.org/10.6080/K0NC5Z4X}}

@article{Konishi03,
	abstract = {Behavioral, anatomical, and physiological approaches can be integrated in the study of sound localization in barn owls. Space representation in owls provides a useful example for discussion of place and ensemble coding. Selectivity for space is broad and ambiguous in low-order neurons. Parallel pathways for binaural cues and for different frequency bands converge on high-order space-specific neurons, which encode space more precisely. An ensemble of broadly tuned place-coding neurons may converge on a single high-order neuron to create an improved labeled line. Thus, the two coding schemes are not alternate methods. Owls can localize sounds by using either the isomorphic map of auditory space in the midbrain or forebrain neural networks in which space is not mapped.},
	author = {Konishi, Masakazu},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1146/annurev.neuro.26.041002.131123},
	issn = {0147-006X},
	journal = {Annual Review of Neuroscience},
	keywords = {Animals, Auditory Pathways, Auditory Perception, Behavior, Animal, Brain, Brain Mapping, Forms and Records Control, Models, Neurological, Nerve Net, Neural Inhibition, Sound Localization, Space Perception, Strigiformes, ⛔ No INSPIRE recid found},
	language = {eng},
	pages = {31--55},
	pmid = {14527266},
	title = {Coding of auditory space},
	volume = {26},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1146/annurev.neuro.26.041002.131123}}

@article{Kremkow10,
	author = {Kremkow, Jens and Perrinet, Laurent U. and Masson, Guillaume S. and Aertsen, Ad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s10827-010-0240-9},
	journal = {Journal of Computational Neuroscience},
	language = {en},
	number = {3},
	pages = {579--594},
	title = {Functional consequences of correlated excitatory and inhibitory conductances in cortical networks},
	url = {https://doi.org/c3wrbn},
	volume = {28},
	year = {2010},
	bdsk-url-1 = {https://doi.org/c3wrbn},
	bdsk-url-2 = {https://doi.org/10.1007/s10827-010-0240-9}}

@article{Kremkow16,
	author = {Kremkow, Jens and Perrinet, Laurent U. and Monier, Cyril and Alonso, Jose-Manuel and Aertsen, Ad and Fr{\'e}gnac, Yves and Masson, Guillaume S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fncir.2016.00037},
	journal = {Frontiers in Neural Circuits},
	title = {Push-{Pull} {Receptive} {Field} {Organization} and {Synaptic} {Depression}: {Mechanisms} for {Reliably} {Encoding} {Naturalistic} {Stimuli} in {V1}},
	url = {https://doi.org/ggkdkh},
	volume = {10},
	year = {2016},
	bdsk-url-1 = {https://doi.org/ggkdkh},
	bdsk-url-2 = {https://doi.org/10.3389/fncir.2016.00037}}

@article{Kreuz07,
	abstract = {Estimating the degree of synchrony or reliability between two or more spike trains is a frequent task in both experimental and computational neuroscience. In recent years, many different methods have been proposed that typically compare the timing of spikes on a certain time scale to be optimized by the analyst. Here, we propose the ISI-distance, a simple complementary approach that extracts information from the interspike intervals by evaluating the ratio of the instantaneous firing rates. The method is parameter free, time scale independent and easy to visualize as illustrated by an application to real neuronal spike trains obtained in vitro from rat slices. In a comparison with existing approaches on spike trains extracted from a simulated Hindemarsh-Rose network, the ISI-distance performs as well as the best time-scale-optimized measure based on spike timing.},
	author = {Kreuz, Thomas and Haas, Julie S. and Morelli, Alice and Abarbanel, Henry D. I. and Politi, Antonio},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.jneumeth.2007.05.031},
	issn = {0165-0270},
	journal = {Journal of Neuroscience Methods},
	keywords = {Action Potentials, Animals, Electrophysiology, Neurons, Rats},
	language = {eng},
	month = sep,
	number = {1},
	pages = {151--161},
	pmid = {17628690},
	title = {Measuring spike train synchrony},
	volume = {165},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1016/j.jneumeth.2007.05.031}}

@article{Kuhn19,
	abstract = {Oligodendrocytes are the myelinating cells of the central nervous system (CNS) that are generated from oligodendrocyte progenitor cells (OPC). OPC are distributed throughout the CNS and represent a pool of migratory and proliferative adult progenitor cells that can differentiate into oligodendrocytes. The central function of oligodendrocytes is to generate myelin, which is an extended membrane from the cell that wraps tightly around axons. Due to this energy consuming process and the associated high metabolic turnover oligodendrocytes are vulnerable to cytotoxic and excitotoxic factors. Oligodendrocyte pathology is therefore evident in a range of disorders including multiple sclerosis, schizophrenia and Alzheimer's disease. Deceased oligodendrocytes can be replenished from the adult OPC pool and lost myelin can be regenerated during remyelination, which can prevent axonal degeneration and can restore function. Cell population studies have recently identified novel immunomodulatory functions of oligodendrocytes, the implications of which, e.g., for diseases with primary oligodendrocyte pathology, are not yet clear. Here, we review the journey of oligodendrocytes from the embryonic stage to their role in homeostasis and their fate in disease. We will also discuss the most common models used to study oligodendrocytes and describe newly discovered functions of oligodendrocytes.},
	author = {Kuhn, Sarah and Gritti, Laura and Crooks, Daniel and Dombrowski, Yvonne},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3390/cells8111424},
	issn = {2073-4409},
	journal = {Cells},
	keywords = {⛔ No INSPIRE recid found},
	month = nov,
	number = {11},
	pages = {1424},
	pmcid = {PMC6912544},
	pmid = {31726662},
	title = {Oligodendrocytes in {Development}, {Myelin} {Generation} and {Beyond}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6912544/},
	urldate = {2022-11-13},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6912544/},
	bdsk-url-2 = {https://doi.org/10.3390/cells8111424}}

@article{Lagorce17,
	abstract = {This paper describes novel event-based spatiotemporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving near 100\% accuracy on each. We introduce a new 7 class moving face recognition task, achieving 79\% accuracy.},
	author = {Lagorce, Xavier and Orchard, Garrick and Galluppi, Francesco and Shi, Bertram E. and Benosman, Ryad B.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TPAMI.2016.2574707},
	issn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Neuromorphic sensing, event-based vision, feature extraction},
	number = {7},
	pages = {1346--1359},
	title = {{HOTS}: {A} {Hierarchy} of {Event}-{Based} {Time}-{Surfaces} for {Pattern} {Recognition}},
	volume = {39},
	year = {2017},
	bdsk-url-2 = {https://doi.org/10.1109/TPAMI.2016.2574707}}

@article{Lamme00,
	author = {Lamme, Victor A.F. and Roelfsema, Pieter R.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/s0166-2236(00)01657-x},
	journal = {Trends in Neurosciences},
	language = {en},
	month = nov,
	number = {11},
	pages = {571--579},
	title = {The distinct modes of vision offered by feedforward and recurrent processing},
	url = {https://doi.org/ccv3w2},
	volume = {23},
	year = {2000},
	bdsk-url-1 = {https://doi.org/ccv3w2},
	bdsk-url-2 = {https://doi.org/10.1016/s0166-2236(00)01657-x}}

@article{Lazar04,
	abstract = {Time encoding is a formal method of mapping amplitude information into a time sequence. We show that under simple conditions, bandlimited stimuli encoded with an integrate-and-{\"y}re neuron with an absolute refractory period can be recovered loss-free from the neural spike train at its output. We provide an algorithm for perfect recovery and derive conditions for its convergence. c 2003 Elsevier B.V. All rights reserved.},
	author = {Lazar, Aurel A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neucom.2004.01.022},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jun,
	pages = {53--58},
	title = {Time encoding with an integrate-and-fire neuron with a refractory period},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231204000177},
	urldate = {2022-12-19},
	volume = {58-60},
	year = {2004},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0925231204000177},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2004.01.022}}

@article{Le-Bec22,
	abstract = {This study demonstrates the functional importance of the Surround context relayed laterally in V1 by the horizontal connectivity, in controlling the latency and the gain of the cortical response to the feedforward visual drive. We report here four main findings: 1) a centripetal apparent motion sequence results in a shortening of the spiking latency of V1 cells, when the orientation of the local inducer and the global motion axis are both co-aligned with the RF orientation preference; 2) this contextual effects grows with visual flow speed, peaking at 150--250$\,^{\circ}$/s when it matches the propagation speed of horizontal connectivity (0.15--0.25 mm/ms); 3) For this speed range, the axial sensitivity of V1 cells is tilted by 90$\,^{\circ}$ to become co-aligned with the orientation preference axis; 4) the strength of modulation by the surround context correlates with the spatiotemporal coherence of the apparent motion flow. Our results suggest an internally-generated binding process, linking local (orientation /position) and global (motion/direction) features as early as V1. This long-range diffusion process constitutes a plausible substrate in V1 of the human psychophysical bias in speed estimation for collinear motion. Since it is demonstrated in the anesthetized cat, this novel form of contextual control of the cortical gain and phase is a built-in property in V1, whose expression does not require behavioral attention and top-down control from higher cortical areas. We propose that horizontal connectivity participates in the propagation of an internal ``prediction'' wave, shaped by visual experience, which links contour co-alignment and global axial motion at an apparent speed in the range of saccade-like eye movements.},
	author = {Le Bec, Benoit and Troncoso, Xoana G. and Desbois, Christophe and Passarelli, Yannick and Baudot, Pierre and Monier, Cyril and Pananceau, Marc and Fr{\'e}gnac, Yves},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pone.0268351},
	editor = {Charpier, St{\'e}phane},
	issn = {1932-6203},
	journal = {PLOS ONE},
	language = {en},
	month = jul,
	number = {7},
	pages = {e0268351},
	shorttitle = {Horizontal connectivity in {V1}},
	title = {Horizontal connectivity in {V1}: {Prediction} of coherence in contour and motion integration},
	url = {https://dx.plos.org/10.1371/journal.pone.0268351},
	urldate = {2022-09-26},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://dx.plos.org/10.1371/journal.pone.0268351},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0268351}}

@article{Lee04,
	abstract = {Information processing in the brain is believed to require coordinated activity across many neurons. With the recent development of techniques for simultaneously recording the spiking activity of large numbers of individual neurons, the search for complex multicell firing patterns that could help reveal this neural code has become possible. Here we develop a new approach for analyzing sequential firing patterns involving an arbitrary number of neurons based on relative firing order. Specifically, we develop a combinatorial method for quantifying the degree of matching between a ``reference sequence'' of N distinct ``letters'' (representing a particular target order of firing by N cells) and an arbitrarily long ``word'' composed of any subset of those letters including repeats (representing the relative time order of spikes in an arbitrary firing pattern). The method involves computing the probability that a random permutation of the word's letters would by chance alone match the reference sequence as well as or better than the actual word does, assuming all permutations were equally likely. Lower probabilities thus indicate better matching. The overall degree and statistical significance of sequence matching across a heterogeneous set of words (such as those produced during the course of an experiment) can be computed from the corresponding set of probabilities. This approach can reduce the sample size problem associated with analyzing complex firing patterns. The approach is general and thus applicable to other types of neural data beyond multiple spike trains, such as EEG events or imaging signals from multiple locations. We have recently applied this method to quantify memory traces of sequential experience in the rodent hippocampus during slow wave sleep.},
	author = {Lee, Albert K. and Wilson, Matthew A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.01030.2003},
	issn = {0022-3077, 1522-1598},
	journal = {Journal of Neurophysiology},
	language = {en},
	month = oct,
	number = {4},
	pages = {2555--2573},
	title = {A {Combinatorial} {Method} for {Analyzing} {Sequential} {Firing} {Patterns} {Involving} an {Arbitrary} {Number} of {Neurons} {Based} on {Relative} {Time} {Order}},
	url = {https://www.physiology.org/doi/10.1152/jn.01030.2003},
	urldate = {2022-10-17},
	volume = {92},
	year = {2004},
	bdsk-url-1 = {https://www.physiology.org/doi/10.1152/jn.01030.2003},
	bdsk-url-2 = {https://doi.org/10.1152/jn.01030.2003}}

@article{Levakova15,
	author = {Levakova, Marie and Tamborrino, Massimiliano and Ditlevsen, Susanne and Lansky, Petr},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.biosystems.2015.04.008},
	journal = {Biosystems},
	language = {en},
	month = oct,
	pages = {23--34},
	title = {A review of the methods for neuronal response latency estimation},
	url = {https://doi.org/gpshjz},
	volume = {136},
	year = {2015},
	bdsk-url-1 = {https://doi.org/gpshjz},
	bdsk-url-2 = {https://doi.org/10.1016/j.biosystems.2015.04.008}}

@article{Liang22,
	abstract = {There is a growing interest in neuromorphic hardware since it offers a more intuitive way to achieve bio-inspired algorithms. This paper presents a neuromorphic model for intelligently processing continuous electrocardiogram (ECG) signal. This model aims to develop a hardware-based signal processing model and avoid employing digitally intensive operations, such as signal segmentation and feature extraction, which are not desired in an analogue neuromorphic system. We apply delay-based reservoir computing as the information processing core, along with a novel training and labelling method. Different from the conventional ECG classification techniques, this computation model is a end-to-end dynamic system that mimics the real-time signal flow in neuromorphic hardware. The input is the raw ECG stream, while the amplitude of the output represents the risk factor of a ventricular ectopic heartbeat. The intrinsic memristive property of the reservoir empowers the system to retain the historical ECG information for high-dimensional mapping. This model was evaluated with the MIT-BIH database under the inter-patient paradigm and yields 81\% sensitivity and 98\% accuracy. Under this architecture, the minimum size of memory required in the inference process can be as low as 3.1 MegaByte(MB) because the majority of the computation takes place in the analogue domain. Such computational modelling boosts memory efficiency by simplifying the computing procedure and minimizing the required memory for future wearable devices.},
	author = {Liang, Xiangpeng and Li, Haobo and Vuckovic, Aleksandra and Mercer, John and Heidari, Hadi},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TBME.2021.3129306},
	issn = {1558-2531},
	journal = {IEEE Transactions on Biomedical Engineering},
	keywords = {Computational modeling, Continuous ventricular heartbeat detection, Databases, Electrocardiography, Heart beat, Memory efficient analogue computing, Neuromorphics, Neurons, Physical neural network, Reservoirs, delay-based reservoir computing},
	month = jun,
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	number = {6},
	pages = {1837--1849},
	title = {A {Neuromorphic} {Model} {With} {Delay}-{Based} {Reservoir} for {Continuous} {Ventricular} {Heartbeat} {Detection}},
	volume = {69},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TBME.2021.3129306}}

@article{Lichtsteiner08,
	abstract = {This paper describes a 128 times 128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1\% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is \&amp;gt; 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at \&amp;gt; 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum\&lt;sup\&gt;2\&lt;/sup\&gt; pixels with 9.4\% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.},
	author = {Lichtsteiner, P. and Posch, C. and Delbruck, T.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/JSSC.2007.914337},
	issn = {0018-9200, 1558-173X},
	journal = {IEEE Journal of Solid-State Circuits},
	language = {English},
	number = {43},
	pages = {566--576},
	title = {A 128x128, 120 {dB} 15 ms {Latency} {Asynchronous} {Temporal} {Contrast} {Vision} {Sensor}},
	url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000004444573},
	urldate = {2022-11-08},
	volume = {2},
	year = {2008},
	bdsk-url-1 = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000004444573},
	bdsk-url-2 = {https://doi.org/10.1109/JSSC.2007.914337}}

@article{Linden21,
	abstract = {{\textless}jats:title{\textgreater}ABSTRACT{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. Since flexor- and extensor-muscles alternate during rhythmic movements like walking, it is often assumed that the responsible neural circuitry is similarly displaying alternating activity. Here, we present ensemble-recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional ``rotation'' in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behavior. The radius of rotation correlates with the intended muscle force and a perturbation of the low-dimensional trajectory can modify the motor behavior. Since existing models of spinal motor control offer an inadequate explanation of rotation, we propose a new theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control, and multi-functionalism, are readily explained.{\textless}/jats:p{\textgreater}},
	author = {Lind{\'e}n, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	month = sep,
	title = {Movement is governed by rotational population dynamics in spinal motor networks},
	url = {https://doi.org/gqg6rb},
	year = {2021},
	bdsk-url-1 = {https://doi.org/gqg6rb}}

@article{Linden22,
	abstract = {Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. As flexor and extensor muscle activities alternate during rhythmic movements such as walking, it is often assumed that the responsible neural circuitry is similarly exhibiting alternating activity1. Here we present ensemble recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional `rotation' in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behaviour. The radius of rotation correlates with the intended muscle force, and a perturbation of the low-dimensional trajectory can modify the motor behaviour. As existing models of spinal motor control do not offer an adequate explanation of rotation1,2, we propose a theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control and multifunctionalism, are readily explained.},
	author = {Lind{\'e}n, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41586-022-05293-w},
	issn = {1476-4687},
	journal = {Nature},
	keywords = {Central pattern generators, Network models, ⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	number = {7932},
	pages = {526--531},
	title = {Movement is governed by rotational neural dynamics in spinal motor networks},
	url = {https://www.nature.com/articles/s41586-022-05293-w},
	urldate = {2022-11-16},
	volume = {610},
	year = {2022},
	bdsk-url-1 = {https://www.nature.com/articles/s41586-022-05293-w},
	bdsk-url-2 = {https://doi.org/10.1038/s41586-022-05293-w}}

@article{Luczak15,
	abstract = {Cortical circuits work through the generation of coordinated, large-scale activity patterns. In sensory systems, the onset of a discrete stimulus usually evokes a temporally organized packet of population activity lasting ∼50-200 ms. The structure of these packets is partially stereotypical, and variation in the exact timing and number of spikes within a packet conveys information about the identity of the stimulus. Similar packets also occur during ongoing stimuli and spontaneously. We suggest that such packets constitute the basic building blocks of cortical coding.},
	author = {Luczak, Artur and McNaughton, Bruce L and Harris, Kenneth D},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nrn4026},
	issn = {1471-0048},
	journal = {Nature reviews. Neuroscience},
	month = oct,
	number = {12},
	pages = {745--55},
	title = {Packet-based communication in the cortex.},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/26507295},
	volume = {16},
	year = {2015},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/26507295},
	bdsk-url-2 = {https://doi.org/10.1038/nrn4026}}

@article{Luczak07,
	abstract = {Even in the absence of sensory stimulation, the neocortex shows complex spontaneous activity patterns, often consisting of alternating ``DOWN'' states of generalized neural silence and ``UP'' states of massive, persistent network activity. To investigate how this spontaneous activity propagates through neuronal assemblies in vivo, we simultaneously recorded populations of 50--200 cortical neurons in layer V of anesthetized and awake rats. Each neuron displayed a virtually unique spike pattern during UP states, with diversity seen amongst both putative pyramidal cells and interneurons, reflecting a complex but stereotypically organized sequential spread of activation through local cortical networks. Spike timing was most precise during the first ≈100 ms after UP state onset, and decayed as UP states progressed. A subset of UP states propagated as traveling waves, but waves passing a given point in either direction initiated similar local sequences, suggesting local networks as the substrate of sequential firing patterns. A search for repeating motifs indicated that their occurrence and structure was predictable from neurons' individual latencies to UP state onset. We suggest that these stereotyped patterns arise from the interplay of intrinsic cellular conductances and local circuit properties.},
	author = {Luczak, Artur and Barth{\'o}, Peter and Marguet, Stephan L. and Buzs{\'a}ki, Gy{\"o}rgy and Harris, Kenneth D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.0605643104},
	issn = {0027-8424, 1091-6490},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {microcircuits, neuronal assembly, repeating sequences, slow oscillations, syntire chains},
	language = {en},
	month = jan,
	number = {1},
	pages = {347--352},
	title = {Sequential structure of neocortical spontaneous activity in vivo},
	url = {https://www.pnas.org/content/104/1/347},
	urldate = {2022-02-23},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://www.pnas.org/content/104/1/347},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0605643104}}

@article{Luo22,
	author = {Luo, Xiaoling and Qu, Hong and Wang, Yuchen and Yi, Zhang and Zhang, Jilun and Zhang, Malu},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/tnnls.2022.3164930},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	pages = {1--13},
	title = {Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks} {With} {Spike} {Temporal} {Error} {Backpropagation}},
	url = {https://doi.org/gpzp26},
	year = {2022},
	bdsk-url-1 = {https://doi.org/gpzp26},
	bdsk-url-2 = {https://doi.org/10.1109/tnnls.2022.3164930}}

@article{Maass97,
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. {\copyright} 1997 Elsevier Science Ltd. All rights reserved.},
	author = {Maass, Wolfgang},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {08936080},
	journal = {Neural Networks},
	language = {en-US},
	number = {9},
	pages = {1659--1671},
	shorttitle = {Networks of spiking neurons},
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	urldate = {2020-07-06},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117}}

@article{Madadi-Asl22,
	abstract = {Synchronization plays a key role in learning and memory by facilitating the communication between neurons promoted by synaptic plasticity. Spike-timing-dependent plasticity (STDP) is a form of synaptic plasticity that modifies the strength of synaptic connections between neurons based on the coincidence of pre- and postsynaptic spikes. In this way, STDP simultaneously shapes the neuronal activity and synaptic connectivity in a feedback loop. However, transmission delays due to the physical distance between neurons affect neuronal synchronization and the symmetry of synaptic coupling. To address the question that how transmission delays and STDP can jointly determine the emergent pairwise activity-connectivity patterns, we studied phase synchronization properties and coupling symmetry between two bidirectionally coupled neurons using both phase oscillator and conductance-based neuron models. We show that depending on the range of transmission delays, the activity of the two-neuron motif can achieve an in-phase/anti-phase synchronized state and its connectivity can attain a symmetric/asymmetric coupling regime. The coevolutionary dynamics of the neuronal system and the synaptic weights due to STDP stabilizes the motif in either one of these states by transitions between in-phase/anti-phase synchronization states and symmetric/asymmetric coupling regimes at particular transmission delays. These transitions crucially depend on the phase response curve (PRC) of the neurons, but they are relatively robust to the heterogeneity of transmission delays and potentiation-depression imbalance of the STDP profile.},
	author = {Madadi Asl, Mojtaba and Ramezani Akbarabadi, Saeideh},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s11571-022-09850-x},
	issn = {1871-4099},
	journal = {Cognitive Neurodynamics},
	keywords = {Coupling symmetry, Spike-timing-dependent plasticity, Synaptic plasticity, Synchronization, Transmission delay},
	language = {en},
	month = jul,
	title = {Delay-dependent transitions of phase synchronization and coupling symmetry between neurons shaped by spike-timing-dependent plasticity},
	url = {https://doi.org/10.1007/s11571-022-09850-x},
	urldate = {2022-09-18},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s11571-022-09850-x}}

@article{Madadi-Asl18,
	abstract = {Biological neuronal networks are highly adaptive and plastic. For instance, spike-timing-dependent plasticity (STDP) is a core mechanism which adapts the synaptic strengths based on the relative timing of pre- and postsynaptic spikes. In various fields of physiology, time delays cause a plethora of biologically relevant dynamical phenomena. However, time delays increase the complexity of model systems together with the computational and theoretical analysis burden. Accordingly, in computational neuronal network studies propagation delays were often neglected. As a downside, a classic STDP rule in oscillatory neurons without propagation delays is unable to give rise to bidirectional synaptic couplings, i.e., loops or uncoupled states. This is at variance with basic experimental results. In this mini review, we focus on recent theoretical studies focusing on how things change in the presence of propagation delays. Realistic propagation delays may lead to the emergence of neuronal activity and synaptic connectivity patterns, which cannot be captured by classic STDP models. In fact, propagation delays determine the inventory of attractor states and shape their basins of attractions. The results reviewed here enable to overcome fundamental discrepancies between theory and experiments. Furthermore, these findings are relevant for the development of therapeutic brain stimulation techniques aiming at shifting the diseased brain to more favorable attractor states.},
	author = {Madadi Asl, Mojtaba and Valizadeh, Alireza and Tass, Peter A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fphys.2018.01849},
	issn = {1664-042X},
	journal = {Frontiers in Physiology},
	keywords = {living systems, mathematical modeling, propagation delays, spike-timing-dependent plasticity, synchronization, ⛔ No INSPIRE recid found},
	language = {eng},
	pages = {1849},
	pmcid = {PMC6307091},
	pmid = {30618847},
	title = {Dendritic and {Axonal} {Propagation} {Delays} {May} {Shape} {Neuronal} {Networks} {With} {Plastic} {Synapses}},
	volume = {9},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3389/fphys.2018.01849}}

@article{Mainen95,
	author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.7770778},
	journal = {Science},
	language = {en},
	number = {5216},
	pages = {1503--1506},
	title = {Reliability of {Spike} {Timing} in {Neocortical} {Neurons}},
	url = {https://doi.org/b2mms6},
	volume = {268},
	year = {1995},
	bdsk-url-1 = {https://doi.org/b2mms6},
	bdsk-url-2 = {https://doi.org/10.1126/science.7770778}}

@article{Malvache16,
	abstract = {The chained activation of neuronal assemblies is thought to support major cognitive processes, including memory. In the hippocampus, this is observed during population bursts often associated with sharp-wave ripples, in the form of an ordered reactivation of neurons. However, the organization and lifetime of these assemblies remain unknown. We used calcium imaging to map patterns of synchronous neuronal activation in the CA1 region of awake mice during runs on a treadmill. The patterns were composed of the recurring activation of anatomically intermingled, but functionally orthogonal, assemblies. These assemblies reactivated discrete temporal segments of neuronal sequences observed during runs and could be stable across consecutive days. A binding of these assemblies into longer chains revealed temporally ordered replay. These modules may represent the default building blocks for encoding or retrieving experience.},
	author = {Malvache, Arnaud and Reichinnek, Susanne and Villette, Vincent and Haimerl, Caroline and Cossart, Rosa},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/bqpq},
	issn = {1095-9203},
	journal = {Science (New York, N.Y.)},
	keywords = {Animals, Brain Mapping, CA1 Region, Hippocampal, Calcium Signaling, Exercise Test, Male, Mice, Nerve Net, Neurons, Running, Wakefulness},
	language = {eng},
	month = sep,
	note = {00105},
	number = {6305},
	pages = {1280--1283},
	pmid = {27634534},
	title = {Awake hippocampal reactivations project onto orthogonal neuronal assemblies},
	volume = {353},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10/bqpq}}

@article{Markovic20,
	abstract = {Neuromorphic computing takes inspiration from the brain to create energy-efficient hardware for information processing, capable of highly sophisticated tasks. Systems built with standard electronics achieve gains in speed and energy by mimicking the distributed topology of the brain. Scaling-up such systems and improving their energy usage, speed and performance by several orders of magnitude requires a revolution in hardware. We discuss how including more physics in the algorithms and nanoscale materials used for data processing could have a major impact in the field of neuromorphic computing. We review striking results that leverage physics to enhance the computing capabilities of artificial neural networks, using resistive switching materials, photonics, spintronics and other technologies. We discuss the paths that could lead these approaches to maturity, towards low-power, miniaturized chips that could infer and learn in real time.},
	author = {Markovi{\'c}, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
	copyright = {2020 Springer Nature Limited},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s42254-020-0208-2},
	issn = {2522-5820},
	journal = {Nature Reviews Physics},
	keywords = {Electronics, Nanoscale devices, photonics and device physics},
	language = {en},
	month = sep,
	note = {Number: 9 Publisher: Nature Publishing Group},
	number = {9},
	pages = {499--510},
	title = {Physics for neuromorphic computing},
	url = {http://www.nature.com/articles/s42254-020-0208-2},
	urldate = {2022-11-10},
	volume = {2},
	year = {2020},
	bdsk-url-1 = {http://www.nature.com/articles/s42254-020-0208-2},
	bdsk-url-2 = {https://doi.org/10.1038/s42254-020-0208-2}}

@article{Markram11,
	abstract = {The Human Brain Project (HBP) is a candidate project in the European Union's FET Flagship Program, funded by the ICT Program in the Seventh Framework Program. The project will develop a new integrated strategy for understanding the human brain and a novel research platform that will integrate all the data and knowledge we can acquire about the structure and function of the brain and use it to build unifying models that can be validated by simulations running on supercomputers. The project will drive the development of supercomputing for the life sciences, generate new neuroscientific data as a benchmark for modeling, develop radically new tools for informatics, modeling and simulation, and build virtual laboratories for collaborative basic and clinical studies, drug simulation and virtual prototyping of neuroprosthetic, neuromorphic, and robotic devices.},
	author = {Markram, Henry and Meier, Karlheinz and Lippert, Thomas and Grillner, Sten and Frackowiak, Richard and Dehaene, Stanislas and Knoll, Alois and Sompolinsky, Haim and Verstreken, Kris and DeFelipe, Javier and Grant, Seth and Changeux, Jean-Pierre and Saria, Alois},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.procs.2011.12.015},
	issn = {1877-0509},
	journal = {Procedia Computer Science},
	keywords = {HPC, Human brain, medicine, modeling, neuroinformatics, neuromorphics, neuroprosthetics, neurorobotics, neuroscience, simulation, supercomputing, ⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	pages = {39--42},
	series = {Proceedings of the 2nd {European} {Future} {Technologies} {Conference} and {Exhibition} 2011 ({FET} 11)},
	title = {Introducing the {Human} {Brain} {Project}},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050911006806},
	urldate = {2022-11-14},
	volume = {7},
	year = {2011},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1877050911006806},
	bdsk-url-2 = {https://doi.org/10.1016/j.procs.2011.12.015}}

@article{Markram97,
	abstract = {{\textless}jats:p{\textgreater}Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.{\textless}/jats:p{\textgreater}},
	author = {Markram, Henry and L{\"u}bke, Joachim and Frotscher, Michael and Sakmann, Bert},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.275.5297.213},
	journal = {Science},
	language = {en},
	month = jan,
	number = {5297},
	pages = {213--215},
	title = {Regulation of {Synaptic} {Efficacy} by {Coincidence} of {Postsynaptic} {APs} and {EPSPs}},
	url = {https://doi.org/ftvvd8},
	volume = {275},
	year = {1997},
	bdsk-url-1 = {https://doi.org/ftvvd8},
	bdsk-url-2 = {https://doi.org/10.1126/science.275.5297.213}}

@article{Maro20,
	abstract = {In this paper, we introduce a framework for dynamic gesture recognition with background suppression operating on the output of a moving event-based camera. The system is developed to operate in real-time using only the computational capabilities of a mobile phone. It introduces a new development around the concept of time-surfaces. It also presents a novel event-based methodology to dynamically remove backgrounds that uses the high temporal resolution properties of event-based cameras. To our knowledge, this is the first Android event-based framework for vision-based recognition of {\textbackslash}textit\{dynamic\} gestures running on a smartphone without off-board processing. We assess the performances by considering several scenarios in both indoors and outdoors, for static and dynamic conditions, in uncontrolled lighting conditions. We also introduce a new event-based dataset for gesture recognition with static and dynamic backgrounds (made publicly available). The set of gestures has been selected following a clinical trial to allow human-machine interaction for the visually impaired and older adults. We finally report comparisons with prior work that addressed event-based gesture recognition reporting comparable results, without the use of advanced classification techniques nor power greedy hardware.},
	author = {Maro, Jean-Matthieu and Ieng, Sio-Hoi and Benosman, Ryad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2020.00275},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	keywords = {Background suppression, Dynamic gesture recognition, Neuromorphic, dynamic vision sensor (DVS), event-based, gesture recognition, mobile device, smartphone},
	language = {English},
	note = {Publisher: Frontiers},
	title = {Event-{Based} {Gesture} {Recognition} {With} {Dynamic} {Background} {Suppression} {Using} {Smartphone} {Computational} {Capabilities}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00275/full?report=reader},
	urldate = {2021-01-15},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00275/full?report=reader},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2020.00275}}

@article{Maunsell83,
	abstract = {1. Recordings were made from single units in the middle temporal visual area (MT) of anesthetized, paralyzed macaque monkeys. A computer-driven stimulator was used to make quantitative tests of selectivity for stimulus direction, speed, and orientation. The data were taken from 168 units that were histologically identified as being in MT. 2. The results confirm previous reports of a high degree of direction selectivity in MT. The response above background to stimuli moving in a unit's preferred direction was, an average, 10.9 times that to stimuli moving in the opposite direction. There was a marked tendency for nearby units to have similar preferred directions. 3. Most units were also sharply tuned for the speed of stimulus motion. For some cells the response fell to less than half-maximal at speeds only a factor of two from the optimum; on average, responses were greater than half-maximal only over a 7.7-fold range of speed. The distribution of preferred speeds for different units was unimodal, with a peak near 32 degrees/s; the total range of preferred speeds extended from 2 to 256 degrees/s. Nearby units generally responded best to similar speeds of motion. 4. Most units in MT showed selectivity for stimulus orientation when tested with stationary, flashed bars. However, stationary stimuli generally elicited only brief responses; when averaged over the duration of the stimulus, the responses were much less than those to moving stimuli. The preferred orientation was usually, but not always, perpendicular to the preferred direction of movement. 5. A comparison of the results of the present study with a previous quantitative investigation in the owl monkey shows a striking similarity in response properties in MT of the two species. 6. The presence of both direction and speed selectivity in MT of the macaque suggests that this area is more specialized for the analysis of visual motion than has been previously recognized.},
	author = {Maunsell, J. H. and Van Essen, D. C.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.1983.49.5.1127},
	issn = {0022-3077, 1522-1598},
	journal = {Journal of Neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = may,
	number = {5},
	pages = {1127--1147},
	title = {Functional properties of neurons in middle temporal visual area of the macaque monkey. {I}. {Selectivity} for stimulus direction, speed, and orientation},
	url = {https://www.physiology.org/doi/10.1152/jn.1983.49.5.1127},
	urldate = {2022-12-15},
	volume = {49},
	year = {1983},
	bdsk-url-1 = {https://www.physiology.org/doi/10.1152/jn.1983.49.5.1127},
	bdsk-url-2 = {https://doi.org/10.1152/jn.1983.49.5.1127}}

@book{Mead89,
	abstract = {This volume contains the proceedings of a workshop on Analog Integrated Neural Systems held May 8, 1989, in connection with the International Symposium on Circuits and Systems. The presentations were chosen to encompass the entire range of topics currently under study in this exciting new discipline. Stringent acceptance requirements were placed on contributions: (1) each description was required to include detailed characterization of a working chip, and (2) each design was not to have been published previously. In several cases, the status of the project was not known until a few weeks before the meeting date. As a result, some of the most recent innovative work in the field was presented. Because this discipline is evolving rapidly, each project is very much a work in progress. Authors were asked to devote considerable attention to the shortcomings of their designs, as well as to the notable successes they achieved. In this way, other workers can now avoid stumbling into the same traps, and evolution can proceed more rapidly (and less painfully). The chapters in this volume are presented in the same order as the corresponding presentations at the workshop. The first two chapters are concerned with fmding solutions to complex optimization problems under a predefmed set of constraints. The first chapter reports what is, to the best of our knowledge, the first neural-chip design. In each case, the physics of the underlying electronic medium is used to represent a cost function in a natural way, using only nearest-neighbor connectivity.},
	author = {Mead, Carver and Ismail, Mohammed},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	isbn = {978-0-7923-9040-4},
	keywords = {Computers / CAD-CAM, Computers / Computer Vision \& Pattern Recognition, Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / Circuits / General, Technology \& Engineering / Electronics / General, Technology \& Engineering / Imaging Systems, ⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	note = {Google-Books-ID: 9e29dOiXeiMC},
	publisher = {Springer Science \& Business Media},
	title = {Analog {VLSI} {Implementation} of {Neural} {Systems}},
	year = {1989}}

@article{Meister95,
	author = {Meister, Markus and Lagnado, Leon and Baylor, Denis A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.270.5239.1207},
	journal = {Science},
	language = {en},
	month = nov,
	number = {5239},
	pages = {1207--1210},
	title = {Concerted {Signaling} by {Retinal} {Ganglion} {Cells}},
	url = {https://doi.org/dszfrn},
	volume = {270},
	year = {1995},
	bdsk-url-1 = {https://doi.org/dszfrn},
	bdsk-url-2 = {https://doi.org/10.1126/science.270.5239.1207}}

@article{Mel17,
	abstract = {The elaborate morphology, nonlinear membrane mechanisms and spatiotemporally varying synaptic activation patterns of dendrites complicate the expression, compartmentalization and modulation of synaptic plasticity. To grapple with this complexity, we start with the observation that neurons in different brain areas face markedly different learning problems, and dendrites of different neuron types contribute to the cell's input-output function in markedly different ways. By committing to specific assumptions regarding a neuron's learning problem and its input-output function, specific inferences can be drawn regarding the synaptic plasticity mechanisms and outcomes that we 'ought' to expect for that neuron. Exploiting this assumption-driven approach can help both in interpreting existing experimental data and designing future experiments aimed at understanding the brain's myriad learning processes.},
	author = {Mel, Bartlett W. and Schiller, Jackie and Poirazi, Panayiota},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.conb.2017.03.012},
	issn = {1873-6882},
	journal = {Current Opinion in Neurobiology},
	keywords = {Dendrites, Humans, Learning, Models, Neurological, Neuronal Plasticity, Synapses, ⛔ No INSPIRE recid found},
	language = {eng},
	month = apr,
	pages = {177--186},
	pmid = {28453975},
	shorttitle = {Synaptic plasticity in dendrites},
	title = {Synaptic plasticity in dendrites: complications and coping strategies},
	volume = {43},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1016/j.conb.2017.03.012}}

@article{Merolla14,
	abstract = {Modeling computer chips on real brains
            
              Computers are nowhere near as versatile as our own brains. Merolla
              et al.
              applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.
            
            
              Science
              , this issue p.
              668
            
          , 
            A large-scale computer chip mimics many features of a real brain.
          , 
            Inspired by the brain's structure, we have developed an efficient, scalable, and flexible non--von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1254642},
	issn = {0036-8075, 1095-9203},
	journal = {Science},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	number = {6197},
	pages = {668--673},
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	url = {https://www.science.org/doi/10.1126/science.1254642},
	urldate = {2022-11-13},
	volume = {345},
	year = {2014},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.1254642},
	bdsk-url-2 = {https://doi.org/10.1126/science.1254642}}

@incollection{Middlebrooks15,
	abstract = {The auditory system derives locations of sound sources from spatial cues provided by the interaction of sound with the head and external ears. Those cues are analyzed in specific brainstem pathways and then integrated as cortical representation of locations. The principal cues for horizontal localization are interaural time differences (ITDs) and interaural differences in sound level (ILDs). Vertical and front/back localization rely on spectral-shape cues derived from direction-dependent filtering properties of the external ears. The likely first sites of analysis of these cues are the medial superior olive (MSO) for ITDs, lateral superior olive (LSO) for ILDs, and dorsal cochlear nucleus (DCN) for spectral-shape cues. Localization in distance is much less accurate than that in horizontal and vertical dimensions, and interpretation of the basic cues is influenced by additional factors, including acoustics of the surroundings and familiarity of source spectra and levels. Listeners are quite sensitive to sound motion, but it remains unclear whether that reflects specific motion detection mechanisms or simply detection of changes in static location. Intact auditory cortex is essential for normal sound localization. Cortical representation of sound locations is highly distributed, with no evidence for point-to-point topography. Spatial representation is strictly contralateral in laboratory animals that have been studied, whereas humans show a prominent right-hemisphere dominance.},
	author = {Middlebrooks, John C.},
	booktitle = {Handbook of {Clinical} {Neurology}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/B978-0-444-62630-1.00006-8},
	editor = {Aminoff, Michael J. and Boller, Fran{\c c}ois and Swaab, Dick F.},
	keywords = {HRTF, Spatial hearing, auditory cortex, auditory motion, distance perception, interaural level difference, interaural time difference, precedence effect, superior colliculus, superior olivary complex, ⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	pages = {99--116},
	publisher = {Elsevier},
	series = {The {Human} {Auditory} {System}},
	title = {Chapter 6 - {Sound} localization},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444626301000068},
	urldate = {2022-12-16},
	volume = {129},
	year = {2015},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780444626301000068},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-444-62630-1.00006-8}}

@article{Miller14,
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}This study demonstrates that neuronal groups or ensembles, rather than individual neurons, are emergent functional units of cortical activity. We show that in the presence and absence of visual stimulation, cortical activity is dominated by coactive groups of neurons forming ensembles. These ensembles are flexible and cannot be accounted for by the independent firing properties of neurons in isolation. Intrinsically generated ensembles and stimulus-evoked ensembles are similar, with one main difference: Whereas intrinsic ensembles recur at random time intervals, visually evoked ensembles are time-locked to stimuli. We propose that visual stimuli recruit endogenously generated ensembles to represent visual attributes.{\textless}/jats:p{\textgreater}},
	author = {Miller, Jae-eun Kang and Ayzenshtat, Inbal and Carrillo-Reid, Luis and Yuste, Rafael},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.1406077111},
	journal = {Proceedings of the National Academy of Sciences},
	language = {en},
	month = sep,
	number = {38},
	title = {Visual stimuli recruit intrinsically generated cortical ensembles},
	url = {https://doi.org/f6htkt},
	volume = {111},
	year = {2014},
	bdsk-url-1 = {https://doi.org/f6htkt},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1406077111}}

@article{Montemurro08,
	author = {Montemurro, Marcelo A. and Rasch, Malte J. and Murayama, Yusuke and Logothetis, Nikos K. and Panzeri, Stefano},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.cub.2008.02.023},
	issn = {0960-9822},
	journal = {Current Biology},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	language = {English},
	month = mar,
	note = {Publisher: Elsevier},
	number = {5},
	pages = {375--380},
	pmid = {18328702},
	title = {Phase-of-{Firing} {Coding} of {Natural} {Visual} {Stimuli} in {Primary} {Visual} {Cortex}},
	url = {http://www.cell.com/current-biology/abstract/S0960-9822(08)00168-1},
	urldate = {2022-12-15},
	volume = {18},
	year = {2008},
	bdsk-url-1 = {http://www.cell.com/current-biology/abstract/S0960-9822(08)00168-1},
	bdsk-url-2 = {https://doi.org/10.1016/j.cub.2008.02.023}}

@article{Moser14,
	author = {Moser, Bernhard A. and Natschlager, Thomas},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/tsp.2014.2305642},
	journal = {IEEE Transactions on Signal Processing},
	number = {8},
	pages = {1987--1999},
	title = {On {Stability} of {Distance} {Measures} for {Event} {Sequences} {Induced} by {Level}-{Crossing} {Sampling}},
	url = {https://doi.org/gnpb7w},
	volume = {62},
	year = {2014},
	bdsk-url-1 = {https://doi.org/gnpb7w},
	bdsk-url-2 = {https://doi.org/10.1109/tsp.2014.2305642}}

@article{Muller18,
	abstract = {Advanced recording techniques have enabled the identification of travelling waves of neuronal activity in different areas of the cortex. Sejnowski and colleagues review these findings, consider the mechanisms by which travelling waves are generated and evaluate their possible roles in cortical function.},
	author = {Muller, Lyle and Chavane, Fr{\'e}d{\'e}ric and Reynolds, John and Sejnowski, Terrence J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nrn.2018.20},
	issn = {1471-003X},
	journal = {Nature Reviews Neuroscience},
	month = mar,
	note = {Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Muller18},
	shorttitle = {Cortical travelling waves},
	title = {Cortical travelling waves: {Mechanisms} and computational principles},
	url = {http://www.nature.com/doifinder/10.1038/nrn.2018.20},
	year = {2018},
	bdsk-url-1 = {http://www.nature.com/doifinder/10.1038/nrn.2018.20},
	bdsk-url-2 = {https://doi.org/10.1038/nrn.2018.20}}

@article{Muller14,
	abstract = {Propagating waves occur in many excitable media and were recently found in neural systems from retina to neocortex. While propagating waves are clearly present under anaesthesia, whether they also appear during awake and conscious states remains unclear. One possibility is that these waves are systematically missed in trial-averaged data, due to variability. Here we present a method for detecting propagating waves in noisy multichannel recordings. Applying this method to single-trial voltage-sensitive dye imaging data, we show that the stimulus-evoked population response in primary visual cortex of the awake monkey propagates as a travelling wave, with consistent dynamics across trials. A network model suggests that this reliability is the hallmark of the horizontal fibre network of superficial cortical layers. Propagating waves with similar properties occur independently in secondary visual cortex, but maintain precise phase relations with the waves in primary visual cortex. These results show that, in response to a visual stimulus, propagating waves are systematically evoked in several visual areas, generating a consistent spatiotemporal frame for further neuronal interactions.},
	author = {Muller, Lyle and Reynaud, Alexandre and Chavane, Fr{\'e}d{\'e}ric and Destexhe, Alain},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/ncomms4675},
	issn = {2041-1723},
	journal = {Nature Communications},
	note = {00068 Loaded from an external bibliography file by Manubot. source\_bibliography: manual-references.bib standard\_id: Muller14},
	pages = {3675},
	title = {The stimulus-evoked population response in visual cortex of awake monkey is a propagating wave},
	volume = {5},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1038/ncomms4675}}

@misc{Nadafian20,
	abstract = {The plasticity of the conduction delay between neurons plays a fundamental role in learning. However, the exact underlying mechanisms in the brain for this modulation is still an open problem. Understanding the precise adjustment of synaptic delays could help us in developing effective brain-inspired computational models in providing aligned insights with the experimental evidence. In this paper, we propose an unsupervised biologically plausible learning rule for adjusting the synaptic delays in spiking neural networks. Then, we provided some mathematical proofs to show that our learning rule gives a neuron the ability to learn repeating spatio-temporal patterns. Furthermore, the experimental results of applying an STDP-based spiking neural network equipped with our proposed delay learning rule on Random Dot Kinematogram indicate the efficacy of the proposed delay learning rule in extracting temporal features.},
	author = {Nadafian, Alireza and Ganjtabesh, Mohammad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, ⛔ No INSPIRE recid found},
	month = nov,
	publisher = {arXiv},
	title = {Bio-plausible {Unsupervised} {Delay} {Learning} for {Extracting} {Temporal} {Features} in {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2011.09380},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2011.09380}}

@article{Nadasdy99,
	abstract = {Information in neuronal networks may be represented by the spatiotemporal patterns of spikes. Here we examined the temporal coordination of pyramidal cell spikes in the rat hippocampus during slow-wave sleep. In addition, rats were trained to run in a defined position in space (running wheel) to activate a selected group of pyramidal cells. A template-matching method and a joint probability map method were used for sequence search. Repeating spike sequences in excess of chance occurrence were examined by comparing the number of repeating sequences in the original spike trains and in surrogate trains after Monte Carlo shuffling of the spikes. Four different shuffling procedures were used to control for the population dynamics of hippocampal neurons. Repeating spike sequences in the recorded cell assemblies were present in both the awake and sleeping animal in excess of what might be predicted by random variations. Spike sequences observed during wheel running were ``replayed'' at a faster timescale during single sharp-wave bursts of slow-wave sleep. We hypothesize that the endogenously expressed spike sequences during sleep reflect reactivation of the circuitry modified by previous experience. Reactivation of acquired sequences may serve to consolidate information.},
	author = {N{\'a}dasdy, Zolt{\'a}n and Hirase, Hajime and Czurk{\'o}, Andr{\'a}s and Csicsvari, Jozsef and Buzs{\'a}ki, Gy{\"o}rgy},
	copyright = {Copyright {\copyright} 1999 Society for Neuroscience},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.19-21-09497.1999},
	issn = {0270-6474, 1529-2401},
	journal = {Journal of Neuroscience},
	keywords = {coding, decoding, memory, network, retrieval, sharp waves, sleep, θ},
	language = {en},
	month = nov,
	note = {Publisher: Society for Neuroscience Section: ARTICLE},
	number = {21},
	pages = {9497--9507},
	pmid = {10531452},
	title = {Replay and {Time} {Compression} of {Recurring} {Spike} {Sequences} in the {Hippocampus}},
	url = {https://www.jneurosci.org/content/19/21/9497},
	urldate = {2022-10-17},
	volume = {19},
	year = {1999},
	bdsk-url-1 = {https://www.jneurosci.org/content/19/21/9497},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.19-21-09497.1999}}

@article{Nave06,
	abstract = {Neuregulins comprise a family of epidermal growth factor-like ligands that interact with ErbB receptor tyrosine kinases to control many aspects of neural development. One of the most dramatic effects of neuregulin-1 is on glial cell differentiation. The membrane-bound neuregulin-1 type III isoform is an axonal ligand for glial ErbB receptors that regulates the early Schwann cell lineage, including the generation of precursors. Recent studies have shown that the amount of neuregulin-1 type III expressed on axons also dictates the glial phenotype, with a threshold level triggering Schwann cell myelination. Remarkably, neuregulin-1 type III also regulates Schwann cell membrane growth to adjust myelin sheath thickness to match axon caliber precisely. Whether this signaling system operates in central nervous system myelination remains an open question of major importance for human demyelinating diseases.},
	author = {Nave, Klaus-Armin and Salzer, James L.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.conb.2006.08.008},
	issn = {0959-4388},
	journal = {Current Opinion in Neurobiology},
	keywords = {Animals, Axons, Cell Differentiation, Cell Lineage, Humans, Myelin Sheath, Neuregulin-1, Neuroglia, Protein Isoforms, Receptor, ErbB-2, Receptor, ErbB-3, Signal Transduction, Stem Cells, ⛔ No INSPIRE recid found},
	language = {eng},
	month = oct,
	number = {5},
	pages = {492--500},
	pmid = {16962312},
	title = {Axonal regulation of myelination by neuregulin 1},
	volume = {16},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1016/j.conb.2006.08.008}}

@article{Neckar19,
	author = {Neckar, Alexander and Fok, Sam and Benjamin, Ben V. and Stewart, Terrence C. and Oza, Nick N. and Voelker, Aaron R. and Eliasmith, Chris and Manohar, Rajit and Boahen, Kwabena},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/JPROC.2018.2881432},
	journal = {Proceedings of the IEEE},
	keywords = {⛔ No INSPIRE recid found},
	number = {1},
	pages = {144--164},
	title = {Braindrop: {A} mixed-signal neuromorphic architecture with a dynamical systems-based programming model},
	volume = {107},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/JPROC.2018.2881432}}

@article{Neftci19,
	author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/MSP.2019.2931595},
	issn = {1053-5888, 1558-0792},
	journal = {IEEE Signal Processing Magazine},
	keywords = {⛔ No INSPIRE recid found},
	month = nov,
	number = {6},
	pages = {51--63},
	shorttitle = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}},
	title = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}: {Bringing} the {Power} of {Gradient}-{Based} {Optimization} to {Spiking} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8891809/},
	urldate = {2022-11-14},
	volume = {36},
	year = {2019},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8891809/},
	bdsk-url-2 = {https://doi.org/10.1109/MSP.2019.2931595}}

@article{Nieters17,
	abstract = {Neuromorphic computing provides a promising platform for processing high-dimensional noisy signals on dedicated hardware. Using design elements inspired by neurobiological findings and advances in machine learning methodology, delay-coupled systems have recently been developed in the field of neuromorphic computing. Delayed feedback connections enable such systems to generate a complex representation of injected input in the internal state of single nodes, which in our context refer to hardware components with nonlinear behavior and without any memory. In contrast to classical combinatorial circuits or feed-forward networks, this state is not distributed in space but in time. Hardware implementations with low hardware component counts are therefore particularly easy to design for delay-coupled systems. In this paper, we present an argument for using delay-coupled reservoirs using multiple feedback terms with different delays. We present a theoretical analysis of the resulting system, discuss surprising effects pertaining to the precise choice of delays, and provide a guideline for the optimal design of such systems.},
	author = {Nieters, P. and Leugering, J. and Pipa, G.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1147/JRD.2017.2664698},
	issn = {0018-8646},
	journal = {IBM Journal of Research and Development},
	keywords = {Biological neural networks, Computational modeling, Computer architecture, Learning systems, Neuromorphics, Noise measurement},
	month = mar,
	note = {Conference Name: IBM Journal of Research and Development},
	number = {2/3},
	pages = {8:7--8:9},
	title = {Neuromorphic computation in multi-delay coupled models},
	volume = {61},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1147/JRD.2017.2664698}}

@article{Nowak97,
	author = {Nowak, L.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1093/cercor/7.6.487},
	journal = {Cerebral Cortex},
	month = sep,
	number = {6},
	pages = {487--501},
	title = {Influence of low and high frequency inputs on spike timing in visual cortical neurons},
	url = {https://doi.org/fvjpx7},
	volume = {7},
	year = {1997},
	bdsk-url-1 = {https://doi.org/fvjpx7},
	bdsk-url-2 = {https://doi.org/10.1093/cercor/7.6.487}}

@incollection{Nowak97a,
	author = {Nowak, Lionel G. and Bullier, Jean},
	booktitle = {Extrastriate {Cortex} in {Primates}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	pages = {205--241},
	publisher = {Springer US},
	title = {The {Timing} of {Information} {Transfer} in the {Visual} {System}},
	url = {https://doi.org/gpb33s},
	year = {1997},
	bdsk-url-1 = {https://doi.org/gpb33s}}

@article{Olshausen17,
	abstract = {The adjustable resistive state of memristors makes it possible to implement sparse coding algorithms naturally and efficiently.},
	author = {Olshausen, Bruno A. and Rozell, Christopher J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nnano.2017.112},
	issn = {1748-3395},
	journal = {Nature Nanotechnology},
	keywords = {Computer science, Electrical and electronic engineering, Network models},
	language = {en},
	month = aug,
	note = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Number: 8 Primary\_atype: News \& Views Publisher: Nature Publishing Group Subject\_term: Computer science;Electrical and electronic engineering;Network models Subject\_term\_id: computer-science;electrical-and-electronic-engineering;network-models},
	number = {8},
	pages = {722--723},
	title = {Sparse codes from memristor grids},
	url = {http://www.nature.com/articles/nnano.2017.112},
	urldate = {2021-11-25},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {http://www.nature.com/articles/nnano.2017.112},
	bdsk-url-2 = {https://doi.org/10.1038/nnano.2017.112}}

@article{Pachitariu18,
	author = {Pachitariu, Marius and Stringer, Carsen and Harris, Kenneth D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.3339-17.2018},
	journal = {The Journal of Neuroscience},
	language = {en},
	number = {37},
	pages = {7976--7985},
	title = {Robustness of {Spike} {Deconvolution} for {Neuronal} {Calcium} {Imaging}},
	url = {https://doi.org/gd9mcx},
	volume = {38},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gd9mcx},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.3339-17.2018}}

@article{Pan20,
	abstract = {Experience-dependent myelination is hypothesized to shape neural circuit function and subsequent behavioral output. Using a contextual fear memory task in mice, we demonstrate that fear learning induces oligodendrocyte precursor cells to proliferate and differentiate into myelinating oligodendrocytes in the medial prefrontal cortex. Transgenic animals that cannot form new myelin exhibit deficient remote, but not recent, fear memory recall. Recording population calcium dynamics by fiber photometry, we observe that the neuronal response to conditioned context cues evolves over time in the medial prefrontal cortex, but not in animals that cannot form new myelin. Finally, we demonstrate that pharmacological induction of new myelin formation with clemastine fumarate improves remote memory recall and promotes fear generalization. Thus, bidirectional manipulation of myelin plasticity functionally affects behavior and neurophysiology, which suggests that neural activity during fear learning instructs the formation of new myelin, which in turn supports the consolidation and/or retrieval of remote fear memories.},
	author = {Pan, Simon and Mayoral, Sonia R. and Choi, Hye Sun and Chan, Jonah R. and Kheirbek, Mazen A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41593-019-0582-1},
	issn = {1546-1726},
	journal = {Nature Neuroscience},
	keywords = {Animals, Cell Proliferation, Conditioning, Classical, Fear, Memory, Long-Term, Mice, Mice, Transgenic, Myelin Sheath, Oligodendrocyte Precursor Cells, Oligodendrocyte Transcription Factor 2, Prefrontal Cortex, ⛔ No INSPIRE recid found},
	language = {eng},
	month = apr,
	number = {4},
	pages = {487--499},
	pmcid = {PMC7213814},
	pmid = {32042175},
	title = {Preservation of a remote fear memory requires new myelin formation},
	volume = {23},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s41593-019-0582-1}}

@techreport{Panahi21,
	abstract = {The principled design and discovery of biologically- and physically-informed models of neuronal dynamics has been advancing since the mid-twentieth century. Recent developments in artificial intelligence (AI) have accelerated this progress. This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, AI, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.},
	author = {Panahi, Mahta Ramezanian and Abrevaya, Germ{\'a}n and Gagnon-Audet, Jean-Christophe and Voleti, Vikram and Rish, Irina and Dumas, Guillaume},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	number = {2112.12147},
	title = {Generative {Models} of {Brain} {Dynamics} -- {A} review},
	url = {https://arxiv.org/abs/2112.12147},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2112.12147}}

@article{Pastalkova08,
	abstract = {A longstanding conjecture in neuroscience is that aspects of cognition depend on the brain's ability to self-generate sequential neuronal activity. We found that reliably and continually-changing cell assemblies in the rat hippocampus appeared not only during spatial navigation but also in the absence of changing environmental or body-derived inputs. During the delay period of a memory task each moment in time was characterized by the activity of a unique assembly of neurons. Identical initial conditions triggered a similar assembly sequence, whereas different conditions gave rise, uniquely, to different sequences, thereby predicting behavioral choices, including errors. Such sequences were not formed in control, non-memory, tasks. We hypothesize that neuronal representations, evolved for encoding distance in spatial navigation, also support episodic recall and the planning of action sequences.},
	author = {Pastalkova, Eva and Itskov, Vladimir and Amarasingham, Asohan and Buzs{\'a}ki, Gy{\"o}rgy},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1159775},
	issn = {0036-8075},
	journal = {Science (New York, N.Y.)},
	month = sep,
	number = {5894},
	pages = {1322--1327},
	title = {Internally {Generated} {Cell} {Assembly} {Sequences} in the {Rat} {Hippocampus}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	urldate = {2022-02-23},
	volume = {321},
	year = {2008},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	bdsk-url-2 = {https://doi.org/10.1126/science.1159775}}

@article{Pasturel20,
	abstract = {Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate "how much they are confident that the target will move to the right or left in the next trial" and to adjust the cursor's position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile).},
	author = {Pasturel, Chlo{\'e} and Montagnini, Anna and Perrinet, Laurent U},
	copyright = {All rights reserved},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1007438},
	journal = {PLoS Computational Biology},
	keywords = {motion anticipation},
	month = jan,
	title = {Humans adapt their anticipatory eye movements to the volatility of visual motion properties},
	url = {https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020},
	year = {2020},
	bdsk-url-1 = {https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1007438}}

@incollection{Paugam-Moisy12,
	author = {Paugam-Moisy, H{\'e}l{\`e}ne and Bohte, Sander M.},
	booktitle = {Handbook of natural computing},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	month = sep,
	publisher = {Springer-Verlag},
	title = {Computing with spiking neuron networks},
	year = {2012}}

@article{Pauli18,
	abstract = {Any modeler who has attempted to reproduce a spiking neural network model from its description in a paper has discovered what a painful endeavor this is. Even when all parameters appear to have been specified, which is rare, typically the initial attempt to reproduce the network does not yield results that are recognizably akin to those in the original publication. Causes include inaccurately reported or hidden parameters (e.g., wrong unit or the existence of an initialization distribution), differences in implementation of model dynamics, and ambiguities in the text description of the network experiment. The very fact that adequate reproduction often cannot be achieved until a series of such causes have been tracked down and resolved is in itself disconcerting, as it reveals unreported model dependencies on specific implementation choices that either were not clear to the original authors, or that they chose not to disclose. In either case, such dependencies diminish the credibility of the model's claims about the behavior of the target system. To demonstrate these issues, we provide a worked example of reproducing a seminal study for which, unusually, source code was provided at time of publication. Despite this seemingly optimal starting position, reproducing the results was time consuming and frustrating. Further examination of the correctly reproduced model reveals that it is highly sensitive to implementation choices such as the realization of background noise, the integration timestep, and the thresholding parameter of the analysis algorithm. From this process, we derive a guideline of best practices that would substantially reduce the investment in reproducing neural network studies, whilst simultaneously increasing their scientific quality. We propose that this guideline can be used by authors and reviewers to assess and improve the reproducibility of future network models.},
	author = {Pauli, Robin and Weidel, Philipp and Kunkel, Susanne and Morrison, Abigail},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/gd8zj5},
	issn = {1662-5196},
	journal = {Frontiers in Neuroinformatics},
	note = {00000},
	pages = {46},
	shorttitle = {Reproducing {Polychronization}},
	title = {Reproducing {Polychronization}: {A} {Guide} to {Maximizing} the {Reproducibility} of {Spiking} {Network} {Models}},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00046},
	urldate = {2021-10-21},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://www.frontiersin.org/article/10.3389/fninf.2018.00046},
	bdsk-url-2 = {https://doi.org/10/gd8zj5}}

@article{Pearce09,
	abstract = {The child prodigy Marie-Jean-Pierre Flourens received his medical degree at Montpellier when aged 19. As a young promising physician Flourens was asked to investigate Gall's controversial views on cerebral localization. To test Gall's assertions, Flourens developed ablation as a procedure to explore the workings of the brain. By removing anatomically defined areas of the brain of an animal and watching its behaviour, he thought he might localize certain functions. Flourens did not favour the idea of cerebral localization and concluded that the brain functioned as a whole and thus arose the concept of `cerebral equipotentiality'. This culminated in his 1824 Recherches exp{\'e}rimentales sur les propri{\'e}t{\'e}s et les fonctions du syst{\`e}me nerveux. His techniques were, however, crude and imperfect, and his experiments were mainly on birds. Much criticism and debate ensued. A gifted man, Flourens also advanced the physiology of the vestibular apparatus and described the anaesthetic properties of ether.},
	author = {Pearce, J.M.S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1159/000206858},
	issn = {0014-3022, 1421-9913},
	journal = {European Neurology},
	language = {en},
	number = {5},
	pages = {311--314},
	title = {Marie-{Jean}-{Pierre} {Flourens} (1794--1867) and {Cortical} {Localization}},
	url = {https://www.karger.com/Article/FullText/206858},
	urldate = {2022-10-10},
	volume = {61},
	year = {2009},
	bdsk-url-1 = {https://www.karger.com/Article/FullText/206858},
	bdsk-url-2 = {https://doi.org/10.1159/000206858}}

@article{Pena01,
	abstract = {The auditory system uses delay lines and coincidence detection to measure the interaural time difference (ITD). Both axons and the cochlea could provide such delays. The stereausis theory assumes that differences in wave propagation time along the basilar membrane can provide the necessary delays, if the coincidence detectors receive input from fibers innervating different loci on the left and right basilar membranes. If this hypothesis were true, the left and right inputs to coincidence detectors should differ in their frequency tuning. The owl's nucleus laminaris contains coincidence detector neurons that receive input from the left and right cochlear nuclei. Monaural frequency-tuning curves of nucleus laminaris neurons showed small interaural differences. In addition, their preferred ITDs were not correlated with the interaural frequency mismatches. Instead, the preferred ITD of the neuron agrees with that predicted from the distribution of axonal delays. Thus, there is no need to invoke mechanisms other than neural delays to explain the detection of ITDs by the barn owl's laminaris neurons.},
	author = {Pe{\~n}a, Jos{\'e} Luis and Viete, Svenja and Funabiki, Kazuo and Saberi, Kourosh and Konishi, Masakazu},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.21-23-09455.2001},
	issn = {0270-6474},
	journal = {The Journal of Neuroscience},
	keywords = {biology, delay-learning},
	month = dec,
	number = {23},
	pages = {9455--9459},
	pmcid = {PMC6763915},
	pmid = {11717379},
	title = {Cochlear and {Neural} {Delays} for {Coincidence} {Detection} in {Owls}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6763915/},
	urldate = {2021-01-07},
	volume = {21},
	year = {2001},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6763915/},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.21-23-09455.2001}}

@article{Perez-Cerda15,
	abstract = {P{\'\i}o del R{\'\i}o Hortega (1882-1945) discovered microglia and oligodendrocytes (OLGs), and after Ram{\'o}n y Cajal, was the most prominent figure of the Spanish school of neurology. He began his scientific career with Nicol{\'a}s Ach{\'u}carro from whom he learned the use of metallic impregnation techniques suitable to study non-neuronal cells. Later on, he joined Cajal's laboratory. and Subsequently, he created his own group, where he continued to develop other innovative modifications of silver staining methods that revolutionized the study of glial cells a century ago. He was also interested in neuropathology and became a leading authority on Central Nervous System (CNS) tumors. In parallel to this clinical activity, del R{\'\i}o Hortega rendered the first systematic description of a major polymorphism present in a subtype of macroglial cells that he named as oligodendroglia and later OLGs. He established their ectodermal origin and suggested that they built the myelin sheath of CNS axons, just as Schwann cells did in the periphery. Notably, he also suggested the trophic role of OLGs for neuronal functionality, an idea that has been substantiated in the last few years. Del R{\'\i}o Hortega became internationally recognized and established an important neurohistological school with outstanding pupils from Spain and abroad, which nearly disappeared after his exile due to the Spanish civil war. Yet, the difficulty of metal impregnation methods and their variability in results, delayed for some decades the confirmation of his great insights into oligodendrocyte biology until the development of electron microscopy and immunohistochemistry. This review aims at summarizing the pioneer and essential contributions of del R{\'\i}o Hortega to the current knowledge of oligodendrocyte structure and function, and to provide a hint of the scientific personality of this extraordinary and insufficiently recognized man.},
	author = {P{\'e}rez-Cerd{\'a}, Fernando and S{\'a}nchez-G{\'o}mez, Mar{\'\i}a Victoria and Matute, Carlos},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnana.2015.00092},
	issn = {1662-5129},
	journal = {Frontiers in Neuroanatomy},
	keywords = {Del R{\'\i}o Hortega, Ram{\'o}n y Cajal, myelin sheath, oligodendrocyte precursor cell (OPC), oligodendroglia},
	language = {eng},
	pages = {92},
	pmcid = {PMC4493393},
	pmid = {26217196},
	title = {P{\'\i}o del {R{\'\i}o} {Hortega} and the discovery of the oligodendrocytes},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.3389/fnana.2015.00092}}

@article{Perkel67,
	abstract = {In a growing class of neurophysiological experiments, the train of impulses (``spikes'') produced by a nerve cell is subjected to statistical treatment involving the time intervals between spikes. The statistical techniques available for the analysis of single spike trains are described and related to the underlying mathematical theory, that of stochastic point processes, i.e., of stochastic processes whose realizations may be described as series of point events occurring in time, separated by random intervals. For single stationary spike trains, several orders of complexity of statistical treatment are described; the major distinction is that between statistical measures that depend in an essential way on the serial order of interspike intervals and those that are order-independent. The interrelations among the several types of calculations are shown, and an attempt is made to ameliorate the current nomenclatural confusion in this field. Applications, interpretations, and potential difficulties of the statistical techniques are discussed, with special reference to types of spike trains encountered experimentally. Next, the related types of analysis are described for experiments which involve repeated presentations of a brief, isolated stimulus. Finally, the effects of nonstationarity, e.g. long-term changes in firing rate, on the various statistical measures are discussed. Several commonly observed patterns of spike activity are shown to be differentially sensitive to such changes. A companion paper covers the analysis of simultaneously observed spike trains.},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0006-3495(67)86596-2},
	issn = {0006-3495},
	journal = {Biophysical Journal},
	language = {en},
	month = jul,
	number = {4},
	pages = {391--418},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {I}. {The} {Single} {Spike} {Train}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865962},
	urldate = {2022-10-04},
	volume = {7},
	year = {1967},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0006349567865962},
	bdsk-url-2 = {https://doi.org/10.1016/S0006-3495(67)86596-2}}

@article{Perkel67a,
	abstract = {The statistical analysis of two simultaneously observed trains of neuronal spikes is described, using as a conceptual framework the theory of stochastic point processes. The first statistical question that arises is whether the observed trains are independent; statistical techniques for testing independence are developed around the notion that, under the null hypothesis, the times of spike occurrence in one train represent random instants in time with respect to the other. If the null hypothesis is rejected---if dependence is attributed to the trains---the problem then becomes that of characterizing the nature and source of the observed dependencies. Statistical signs of various classes of dependencies, including direct interaction and shared input, are discussed and illustrated through computer simulations of interacting neurons. The effects of nonstationarities on the statistical measures for simultaneous spike trains are also discussed. For two-train comparisons of irregularly discharging nerve cells, moderate nonstationarities are shown to have little effect on the detection of interactions. Combining repetitive stimulation and simultaneous recording of spike trains from two (or more) neurons yields additional clues as to possible modes of interaction among the monitored neurons; the theory presented is illustrated by an application to experimentally obtained data from auditory neurons. A companion paper covers the analysis of single spike trains.},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0006-3495(67)86597-4},
	issn = {0006-3495},
	journal = {Biophysical Journal},
	language = {en},
	month = jul,
	number = {4},
	pages = {419--440},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {II}. {Simultaneous} {Spike} {Trains}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865974},
	urldate = {2022-10-04},
	volume = {7},
	year = {1967},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0006349567865974},
	bdsk-url-2 = {https://doi.org/10.1016/S0006-3495(67)86597-4}}

@article{Perrinet14,
	abstract = {This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.},
	author = {Perrinet, Laurent U and Adams, Rick A and Friston, Karl J},
	copyright = {All rights reserved},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s00422-014-0620-8},
	issn = {1432-0770},
	journal = {Biological Cybernetics},
	keywords = {Active inference, Bayesian model, Biologically Inspired Computer vision, Generalised coordinates, Oculomotor delays, Smooth pursuit eye movements, Tracking eye movements, Variational free energy, active inference, active-inference, bayesian, bicv-motion, bicv-sparse, delays, eye, eye movements, eye-movements, free energy, free-energy, generalized-coordinates, generalized-filtering, motion detection, oculomotor, perception, perrinetadamsfriston14, smooth-pursuit, tracking-eye-movements, variational-filtering},
	month = dec,
	number = {6},
	pages = {777--801},
	title = {Active inference, eye movements and oculomotor delays},
	url = {https://doi.org/10.1007/s00422-014-0620-8},
	volume = {108},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00422-014-0620-8}}

@article{Perrinet04,
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/TNN.2004.833303},
	journal = {IEEE Transactions on neural networks},
	keywords = {Biomembranes, Central nervous system, Filters, Fires, Image coding, Image reconstruction, Neurons, Retina, Statistics, Wavelet transforms, ⛔ No INSPIRE recid found},
	note = {Publisher: IEEE},
	number = {5},
	pages = {1164--1175},
	title = {Coding static natural images using spiking event times: do neurons cooperate?},
	volume = {15},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/TNN.2004.833303}}

@article{Perrinet02,
	author = {Perrinet, L. and Samuelides, M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/s0925-2312(02)00374-0},
	journal = {Neurocomputing},
	language = {en},
	pages = {133--139},
	title = {Coherence detection in a spiking neuron via {Hebbian} learning},
	url = {https://doi.org/fsk9mk},
	volume = {44-46},
	year = {2002},
	bdsk-url-1 = {https://doi.org/fsk9mk},
	bdsk-url-2 = {https://doi.org/10.1016/s0925-2312(02)00374-0}}

@article{Perrinet04a,
	abstract = {A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (VI) solving this problem in the case where the signal may be described by a robust linear generative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to `Matching Pursuit' and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neurons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this transformation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.},
	author = {Perrinet, Laurent},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.jphysparis.2005.09.012},
	issn = {0928-4257},
	journal = {Journal of Physiology-Paris},
	keywords = {Distributed probabilistic representation, Inverse linear model, Matching pursuit, Neuronal representation, Over-complete dictionaries, Sparse spike coding, Spike-event computation, ⛔ No INSPIRE recid found},
	language = {en},
	month = jul,
	number = {4},
	pages = {530--539},
	series = {Decoding and interfacing the brain: from neuronal assemblies to cyborgs},
	shorttitle = {Feature detection using spikes},
	title = {Feature detection using spikes: {The} greedy approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0928425705000161},
	urldate = {2022-12-13},
	volume = {98},
	year = {2004},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0928425705000161},
	bdsk-url-2 = {https://doi.org/10.1016/j.jphysparis.2005.09.012}}

@article{Perrinet12,
	abstract = {In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to physio-logy and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independent of their texture. Second, we observe that incoherent features are explained away, while coherent information diffuses progressively to the global scale. Most previous models included ad hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features as necessary conditions to solve the aperture problem. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This solution may give insights into the role of prediction underlying a large class of sensory computations.},
	author = {Perrinet, Laurent U. and Masson, G.S. Guillaume S.},
	copyright = {All rights reserved},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/neco_a_00332},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {Bayesian model, aperture, aperture problem, aperture-problem, association field, coding, emergence, khoei12jpp, khoei13jpp, motion detection, motion prediction, perrinet12pred, predictive, predictive coding, predictive-coding, probabilistic, probabilistic representation, problem, representation, thesis, toupate-inpress},
	month = aug,
	number = {10},
	pages = {2726--2750},
	title = {Motion-{Based} {Prediction} {Is} {Sufficient} to {Solve} the {Aperture} {Problem}},
	volume = {24},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_00332}}

@article{Perrinet01,
	author = {Perrinet, L. and Delorme, A. and Samuelides, M. and Thorpe, S.J.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/s0925-2312(01)00460-x},
	journal = {Neurocomputing},
	language = {en},
	pages = {817--822},
	title = {Networks of integrate-and-fire neuron using rank order coding {A}: {How} to implement spike time dependent {Hebbian} plasticity},
	url = {https://doi.org/d5p6b2},
	volume = {38-40},
	year = {2001},
	bdsk-url-1 = {https://doi.org/d5p6b2},
	bdsk-url-2 = {https://doi.org/10.1016/s0925-2312(01)00460-x}}

@article{Perrinet02a,
	abstract = {In order to explore coding strategies in the retina, we use a wavelet-like transform which output is sparse, as is observed in biological retinas [4]. This transform is defined in the context of a one-pass feed-forward spiking neural network, and the output is the list of its neurons' spikes: it is recursively constructed using a greedy matching pursuit scheme which first selects higher contrast energy values.},
	author = {Perrinet, Laurent and Samuelides, Manuel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	month = aug,
	title = {Sparse {Image} {Coding} {Using} an {Asynchronous} {Spiking} {Neural} {Network}},
	year = {2002}}

@article{Perrinet04b,
	abstract = {In order to account for the rapidity of visual processing, we explore visual coding strategies using a one-pass feed-forward spiking neural network. We based our model on the work of Van Rullen and Thorpe Neural Comput. 13 (6) (2001) 1255, which constructs a retinal representation using an orthogonal wavelet transform. This strategy provides a spike code, thanks to a rank order coding scheme which offers an alternative to the classical spike frequency coding scheme. We extended this model to efficient representations in arbitrary linear generative models by implementing lateral interactions on top of this feed-forward model. This method uses a matching pursuit scheme---recursively detecting in the image the best match with the elements of a dictionary and then subtracting it---and which may similarly define a visual spike code. In particular, this transform could be used with large and arbitrary dictionaries, so that we may define an over-complete representation which may define an efficient sparse spike coding scheme in arbitrary multi-layered architectures. We show here extensions of this method of computing with spike events, introducing an adaptive scheme leading to the emergence of V1-like receptive fields and then a model of bottom-up saliency pursuit.},
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neucom.2004.01.010},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Natural images statistics, Parallel asynchronous processing, Sparse coding, Ultra-rapid categorization, Vision, Wavelet Hansform, ⛔ No INSPIRE recid found},
	language = {en},
	month = mar,
	pages = {125--134},
	series = {New {Aspects} in {Neurocomputing}: 10th {European} {Symposium} on {Artificial} {Neural} {Networks} 2002},
	title = {Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231204000670},
	urldate = {2022-12-13},
	volume = {57},
	year = {2004},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231204000670},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2004.01.010}}

@article{Peyrard20,
	abstract = {In the last 15 years, a debate has emerged about the validity of the famous Hodgkin-Huxley model for nerve impulse. Mechanical models have been proposed. This note reviews the experimental properties of the nerve impulse and discusses the proposed alternatives. The experimental data, which rule out some of the alternative suggestions, show that while the Hodgkin-Huxley model may not be complete, it nevertheless includes essential features that should not be overlooked in the attempts made to improve, or supersede, it.},
	author = {Peyrard, Michel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s10867-020-09557-2},
	issn = {0092-0606},
	journal = {Journal of Biological Physics},
	month = dec,
	number = {4},
	pages = {327--341},
	pmcid = {PMC7719126},
	pmid = {33037976},
	title = {How is information transmitted in a nerve?},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7719126/},
	urldate = {2022-11-08},
	volume = {46},
	year = {2020},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7719126/},
	bdsk-url-2 = {https://doi.org/10.1007/s10867-020-09557-2}}

@inproceedings{Pfeil13,
	abstract = {Temporal coding is one approach to representing information in spiking neural networks. An example of its application is the location of sounds by barn owls that requires especially precise temporal coding. Dependent upon the azimuthal angle, the arrival times of sound signals are shifted between both ears. In order to determine these interaural time differences, the phase difference of the signals is measured. We implemented this biologically inspired network on a neuromorphic hardware system and demonstrate spike-timing dependent plasticity on an analog, highly accelerated hardware substrate. Our neuromorphic implementation enables the resolution of time differences of less than 50 ns. On-chip Hebbian learning mechanisms select inputs from a pool of neurons which code for the same sound frequency. Hence, noise caused by different synaptic delays across these inputs is reduced. Furthermore, learning compensates for variations on neuronal and synaptic parameters caused by device mismatch intrinsic to the neuromorphic substrate.},
	author = {Pfeil, Thomas and Scherzer, Anne-Christine and Schemmel, Johannes and Meier, Karlheinz},
	booktitle = {The 2013 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/IJCNN.2013.6706828},
	keywords = {Delays, Emulation, Hardware, Neuromorphics, Neurons, System-on-chip, Vectors},
	month = aug,
	note = {ISSN: 2161-4407},
	pages = {1--5},
	title = {Neuromorphic learning towards nano second precision},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN.2013.6706828}}

@article{Pfeil13a,
	author = {Pfeil, Thomas and Gr{\"u}bl, Andreas and Jeltsch, Sebastian and M{\"u}ller, Eric and M{\"u}ller, Paul and Petrovici, Mihai A. and Schmuker, Michael and Br{\"u}derle, Daniel and Schemmel, Johannes and Meier, Karlheinz},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2013.00011},
	journal = {Frontiers in Neuroscience},
	title = {Six {Networks} on a {Universal} {Neuromorphic} {Computing} {Substrate}},
	url = {https://doi.org/gh4jg3},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://doi.org/gh4jg3},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2013.00011}}

@article{Piccolino97,
	abstract = {Luigi Galvani and his famous experiments on frogs carried out in the second half of the 18th century belong more to legend than to the history of science. Galvani not only laid the foundations of a new science, electrophysiology, but also opened the way for the invention of the electric battery, and thus for the development of the physical investigations of electricity. However, in spite of the widespread celebration of his work, Galvani's scientific endeavours have been largely misrepresented in the history of science. The scholar of Bologna has a stereotyped image as an `occasional' scientist, who started his studies by chance, largely ignored the scientific theories of his time and wandered aimlessly in mental elaborations until the physicist of Pavia, Alessandro Volta, entered the field, correctly interpreted Galvani's results and eventually developed the electric battery. With the present understanding of electrical phenomena in excitable membranes, it is now time to reconsider the real matter raised by Galvani's discoveries and by his hypothesis of an intrinsic `animal electricity', and to make a clearer evaluation of a revolutionary phase of scientific progress.},
	author = {Piccolino, Marco},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0166-2236(97)01101-6},
	issn = {0166-2236},
	journal = {Trends in Neurosciences},
	keywords = {Galvani, Volta, animal electricity, electrophysiology, history of science, nervous signalling},
	language = {en},
	month = oct,
	number = {10},
	pages = {443--448},
	shorttitle = {Luigi {Galvani} and animal electricity},
	title = {Luigi {Galvani} and animal electricity: two centuries after the foundation of electrophysiology},
	url = {https://www.sciencedirect.com/science/article/pii/S0166223697011016},
	urldate = {2022-10-10},
	volume = {20},
	year = {1997},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0166223697011016},
	bdsk-url-2 = {https://doi.org/10.1016/S0166-2236(97)01101-6}}

@article{Pillow08,
	author = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nature07140},
	journal = {Nature},
	language = {en},
	number = {7207},
	pages = {995--999},
	title = {Spatio-temporal correlations and visual signalling in a complete neuronal population},
	url = {https://doi.org/dzvdm3},
	volume = {454},
	year = {2008},
	bdsk-url-1 = {https://doi.org/dzvdm3},
	bdsk-url-2 = {https://doi.org/10.1038/nature07140}}

@article{Pipa08,
	author = {Pipa, Gordon and Wheeler, Diek W. and Singer, Wolf and Nikoli{\'c}, Danko},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s10827-007-0065-3},
	journal = {Journal of Computational Neuroscience},
	language = {en},
	month = jan,
	number = {1},
	pages = {64--88},
	title = {{NeuroXidence}: reliable and efficient analysis of an excess or deficiency of joint-spike events},
	url = {https://doi.org/ddh5js},
	volume = {25},
	year = {2008},
	bdsk-url-1 = {https://doi.org/ddh5js},
	bdsk-url-2 = {https://doi.org/10.1007/s10827-007-0065-3}}

@article{Quaglio18,
	author = {Quaglio, Pietro and Rostami, Vahid and Torre, Emiliano and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/s00422-018-0755-0},
	journal = {Biological Cybernetics},
	language = {en},
	number = {1-2},
	pages = {57--80},
	title = {Methods for identification of spike patterns in massively parallel spike trains},
	url = {https://doi.org/gdgckg},
	volume = {112},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gdgckg},
	bdsk-url-2 = {https://doi.org/10.1007/s00422-018-0755-0}}

@article{Rajendran19,
	abstract = {Machine learning has emerged as the dominant tool for implementing complex cognitive tasks that require supervised, unsupervised, and reinforcement learning. While the resulting machines have demonstrated in some cases even superhuman performance, their energy consumption has often proved to be prohibitive in the absence of costly supercomputers. Most state-of-the-art machine-learning solutions are based on memoryless models of neurons. This is unlike the neurons in the human brain that encode and process information using temporal information in spike events. The different computing principles underlying biological neurons and how they combine together to efficiently process information is believed to be a key factor behind their superior efficiency compared to current machine-learning systems.},
	author = {Rajendran, Bipin and Sebastian, Abu and Schmuker, Michael and Srinivasa, Narayan and Eleftheriou, Evangelos},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/MSP.2019.2933719},
	issn = {1558-0792},
	journal = {IEEE Signal Processing Magazine},
	keywords = {Encoding, Hardware, Mathematical model, Neuromorphics, Neurons, Signal processing algorithms, Synapses},
	month = nov,
	note = {Conference Name: IEEE Signal Processing Magazine},
	number = {6},
	pages = {97--110},
	shorttitle = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}},
	title = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}: {A} {Review} of {Architectural} and {System}-{Level} {Design} {Approaches}},
	volume = {36},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/MSP.2019.2933719}}

@techreport{Rasetto22,
	abstract = {Neuromorphic engineering has led to the necessary process of rethinking of how we process and integrate information, analyze data, and use the resulting insights to improve computation and avoid the current high power and latency of Artificial Intelligence (AI) hardware. Current neuromorphic processors are, however, limited by digital technologies, which cannot reproduce the abilities of biological neural computation in terms of power, latency and area cost. In this paper, we show that the combined use of the dynamic properties of memristors to implement a model of synaptic integration and the determination of the correct level of abstraction of biological neural networks has the potential to open a new range of capabilities for neuromorphic processors. We test this approach using a novel three-terminal LixWO3 electrochemical memristor, by deriving its conductance model and using it to emulate synaptic temporal kernel computation in the context of a pattern recognition task. We show that these devices allow for robust results with no loss in precision while opening the path for an energy efficient approach to build novel bio-inspired processing units in silicon.},
	author = {Rasetto, Marco and Wan, Qingzhou and Akolkar, Himanshu and Shi, Bertram and Xiong, Feng and Benosman, Ryad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	number = {2201.12673},
	title = {The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event} {Processors}: {How} {Memristors} {Dynamic} {Properties} {Could} {Revolutionize} {Machine} {Learning}},
	url = {https://arxiv.org/abs/2201.12673},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2201.12673}}

@article{Renner22,
	abstract = {Vector Symbolic Architectures (VSA) were first proposed as connectionist models for symbolic reasoning, leveraging parallel and in-memory computing in brains and neuromorphic hardware that enable low-power, low-latency applications.
Symbols are defined in VSAs as points/vectors in a high-dimensional neural state-space.
For spiking neuromorphic hardware (and brains), particularly sparse representations are of interest, as they minimize the number of costly spikes. Furthermore, sparse representations can be efficiently stored in simple Hebbian auto-associative memories, which provide error correction in VSAs. 
However, the binding of spatially sparse representations is computationally expensive because it is not local to corresponding pairs of neurons as in VSAs with dense vectors.
Here, we present the first implementation of a sparse VSA on spiking neuromorphic hardware, specifically Intel's neuromorphic research chip Loihi.
To reduce the cost of binding, a delay line and coincidence detection are used, trading off space with time.
We show as proof of principle that our network on Loihi can perform the binding operation of a classical analogical reasoning task and discuss the cost of different sparse binding operations.
The proposed binding mechanism can be used as a building block for VSA-based architectures on neuromorphic hardware.},
	author = {Renner, Alpha and Sandamirskaya, Yulia and Sommer, Friedrich T. and Frady, E. Paxon},
	copyright = {info:eu-repo/semantics/closedAccess},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1145/3546790.3546820},
	journal = {Proceedings of the International Conference on Neuromorphic Systems},
	keywords = {Binding, Coincidence detection, Hyperdimensional Computing, Neuromorphic Hardware, Sparse distributed code, Spiking Neural Networks, Vector Symbolic Architecture (VSA)},
	language = {eng},
	month = jul,
	note = {Conference Name: ICONS 2022: International Conference on Neuromorphic Systems 2022 Meeting Name: ICONS 2022: International Conference on Neuromorphic Systems 2022 Place: Knoxville, TN, USA Publisher: ACM Digital library},
	title = {Sparse {Vector} {Binding} on {Spiking} {Neuromorphic} {Hardware} {Using} {Synaptic} {Delays}},
	url = {https://www.zora.uzh.ch/id/eprint/219676/},
	urldate = {2022-11-10},
	year = {2022},
	bdsk-url-1 = {https://www.zora.uzh.ch/id/eprint/219676/},
	bdsk-url-2 = {https://doi.org/10.1145/3546790.3546820}}

@article{Reynolds28,
	author = {Reynolds, F. E. and Slater, James K.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {0367-1038},
	journal = {Edinburgh Medical Journal},
	month = feb,
	number = {2},
	pages = {49--57},
	pmcid = {PMC5297025},
	pmid = {null},
	title = {A {Study} of the {Structure} and {Function} of the {Interstitial} {Tissue} of the {Central} {Nervous} {System}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5297025/},
	urldate = {2022-11-08},
	volume = {35},
	year = {1928},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5297025/}}

@article{Riehle97,
	author = {Riehle, Alexa and Grun, Sonja and Diesmann, Markus and Aertsen, Ad},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.278.5345.1950},
	journal = {Science (New York, N.Y.)},
	note = {Publisher: American Association for the Advancement of Science},
	number = {5345},
	pages = {1950--1953},
	title = {Spike synchronization and rate modulation differentially involved in motor cortical function},
	volume = {278},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1126/science.278.5345.1950}}

@article{Rinberg06,
	abstract = {The basic psychophysical principle of speed-accuracy tradeoff (SAT) has been used to understand key aspects of neuronal information processing in vision and audition, but the principle of SAT is still debated in olfaction. In this study we present the direct observation of SAT in olfaction. We developed a behavioral paradigm for mice in which both the duration of odorant sampling and the difficulty of the odor discrimination task were controlled by the experimenter. We observed that the accuracy of odor discrimination increases with the duration of imposed odorant sampling, and that the rate of this increase is slower for harder tasks. We also present a unifying picture of two previous, seemingly disparate experiments on timing of odorant sampling in odor discrimination tasks. The presence of SAT in olfaction provides strong evidence for temporal integration in olfaction and puts a constraint on models of olfactory processing.},
	author = {Rinberg, Dmitry and Koulakov, Alexei and Gelperin, Alan},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuron.2006.07.013},
	issn = {0896-6273},
	journal = {Neuron},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	number = {3},
	pages = {351--358},
	title = {Speed-{Accuracy} {Tradeoff} in {Olfaction}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627306005538},
	urldate = {2022-12-16},
	volume = {51},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627306005538},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2006.07.013}}

@article{Roelfsema97,
	author = {Roelfsema, Pieter R. and Engel, Andreas K. and K{\"o}nig, Peter and Singer, Wolf},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/385157a0},
	journal = {Nature},
	language = {en},
	month = jan,
	number = {6612},
	pages = {157--161},
	title = {Visuomotor integration is associated with zero time-lag synchronization among cortical areas},
	url = {https://doi.org/dp8q5v},
	volume = {385},
	year = {1997},
	bdsk-url-1 = {https://doi.org/dp8q5v},
	bdsk-url-2 = {https://doi.org/10.1038/385157a0}}

@article{Roy19,
	abstract = {Guided by brain-like `spiking' computational frameworks, neuromorphic computing---brain-inspired computing for machine intelligence---promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm--hardware codesign.},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	copyright = {2019 Springer Nature Limited},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41586-019-1677-2},
	issn = {1476-4687},
	journal = {Nature},
	language = {en},
	month = nov,
	note = {Number: 7784 Publisher: Nature Publishing Group},
	number = {7784},
	pages = {607--617},
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	url = {http://www.nature.com/articles/s41586-019-1677-2},
	urldate = {2021-03-23},
	volume = {575},
	year = {2019},
	bdsk-url-1 = {http://www.nature.com/articles/s41586-019-1677-2},
	bdsk-url-2 = {https://doi.org/10.1038/s41586-019-1677-2}}

@article{Rubinsky08,
	abstract = {Surgical procedures using hypothermic temperatures have been linked to complications such as seizures, impaired mental development and impaired memory. Although there is some evidence that the profound hypothermia ({\textless}12 ∘C) used in these procedures may be contributing to these neurological impairments, skepticism remains because of lack of evidence from experimental studies isolating the effects of hypothermia on neuronal networks. In order to attain a better understanding of profound hypothermia effects on neurons during surgical procedures, we applied cold to a cultured in-vitro neuronal network. The typical pattern of activity of such cultures is in the form of synchronized bursts, in which most of the recorded neurons fire action potentials in a short time period. In most cases, the bursting activity shows one or more repeating precise spatio-temporal patterns (motifs) that are sustained over long periods of time. In this experimental study, neuronal networks grown on microelectrode arrays (MEA) are subjected to profound hypothermia for an hour and the collective dynamics of the network as a whole are assessed. We show, by using a similarity analysis that compares changes in the time delays between neuronal activation at different burst motifs, that neuronal networks survive total inhibition by profound hypothermia and retain their intrinsic synchronized burst motifs even with substantial generalized neuronal degeneration. By applying multiple sessions of cold, we also show a marked monotonic reduction in the rate of burst firing and in the number of spikes of each neuron after each session.},
	author = {Rubinsky, Liel and Raichman, Nadav and Lavee, Jacob and Frenk, Hanan and Ben-Jacob, Eshel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neunet.2008.06.008},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Low temperature, Microelectrode arrays, Neuronal cultures, Synchronized bursting events, ⛔ No INSPIRE recid found},
	language = {en},
	month = nov,
	number = {9},
	pages = {1232--1237},
	title = {Spatio-temporal motifs `remembered' in neuronal networks following profound hypothermia},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608008001202},
	urldate = {2022-11-19},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608008001202},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2008.06.008}}

@article{Rucci18,
	author = {Rucci, Michele and Ahissar, Ehud and Burr, David},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.tics.2018.07.009},
	journal = {Trends in Cognitive Sciences},
	language = {en},
	month = oct,
	number = {10},
	pages = {883--895},
	title = {Temporal {Coding} of {Visual} {Space}},
	url = {https://doi.org/gfcskd},
	volume = {22},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gfcskd},
	bdsk-url-2 = {https://doi.org/10.1016/j.tics.2018.07.009}}

@article{Rueckauer17,
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2017.00682},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	urldate = {2022-10-25},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2017.00682}}

@article{Russo17,
	abstract = {{\textless}jats:p{\textgreater}Hebb's idea of a cell assembly as the fundamental unit of neural information processing has dominated neuroscience like no other theoretical concept within the past 60 years. A range of different physiological phenomena, from precisely synchronized spiking to broadly simultaneous rate increases, has been subsumed under this term. Yet progress in this area is hampered by the lack of statistical tools that would enable to extract assemblies with arbitrary constellations of time lags, and at multiple temporal scales, partly due to the severe computational burden. Here we present such a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. Applying this methodology to multiple single unit recordings from various cortical areas, we find that there is no universal cortical coding scheme, but that assembly structure and precision significantly depends on the brain area recorded and ongoing task demands.{\textless}/jats:p{\textgreater}},
	author = {Russo, Eleonora and Durstewitz, Daniel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.7554/elife.19428},
	journal = {eLife},
	language = {en},
	month = jan,
	title = {Cell assemblies at multiple time scales with arbitrary lag constellations},
	url = {https://doi.org/f9kxd8},
	volume = {6},
	year = {2017},
	bdsk-url-1 = {https://doi.org/f9kxd8},
	bdsk-url-2 = {https://doi.org/10.7554/elife.19428}}

@article{Safaie20,
	abstract = {{\textless}jats:p{\textgreater}How animals adapt their behavior according to regular time intervals between events is not well understood, especially when intervals last several seconds. One possibility is that animals use disembodied internal neuronal representations of time to decide when to initiate a given action at the end of an interval. However, animals rarely remain immobile during time intervals but tend to perform stereotyped behaviors, raising the possibility that motor routines improve timing accuracy. To test this possibility, we used a task in which rats, freely moving on a motorized treadmill, could obtain a reward if they approached it after a fixed interval. Most animals took advantage of the treadmill length and its moving direction to develop, by trial-and-error, the same motor routine whose execution resulted in the precise timing of their reward approaches. Noticeably, when proficient animals did not follow this routine, their temporal accuracy decreased. Then, na{\"\i}ve animals were trained in modified versions of the task designed to prevent the development of this routine. Compared to rats trained in the first protocol, these animals didn't reach a comparable level of timing accuracy. Altogether, our results indicate that timing accuracy in rats is improved when the environment affords cues that animals can incorporate into motor routines.{\textless}/jats:p{\textgreater}},
	author = {Safaie, Mostafa and Jurado-Parras, Maria-Teresa and Sarno, Stefania and Louis, Jordane and Karoutchi, Corane and Petit, Ludovic F. and Pasquet, Matthieu O. and Eloy, Christophe and Robbe, David},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.1921226117},
	journal = {Proceedings of the National Academy of Sciences},
	language = {en},
	number = {23},
	pages = {13084--13093},
	title = {Turning the body into a clock: {Accurate} timing is facilitated by simple stereotyped interactions with the environment},
	url = {https://doi.org/gnqsmh},
	volume = {117},
	year = {2020},
	bdsk-url-1 = {https://doi.org/gnqsmh},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1921226117}}

@article{Sandamirskaya22,
	abstract = {Neuromorphic hardware enables fast and power-efficient neural network--based artificial intelligence that is well suited to solving robotic tasks. Neuromorphic algorithms can be further developed following neural computing principles and neural network architectures inspired by biological neural systems. In this Viewpoint, we provide an overview of recent insights from neuroscience that could enhance signal processing in artificial neural networks on chip and unlock innovative applications in robotics and autonomous intelligent systems. These insights uncover computing principles, primitives, and algorithms on different levels of abstraction and call for more research into the basis of neural computation and neuronally inspired computing hardware.},
	author = {Sandamirskaya, Yulia and Kaboli, Mohsen and Conradt, Jorg and Celikel, Tansu},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/scirobotics.abl8419},
	journal = {Science Robotics},
	month = jun,
	note = {Publisher: American Association for the Advancement of Science},
	number = {67},
	pages = {eabl8419},
	title = {Neuromorphic computing hardware and neural architectures for robotics},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.abl8419},
	urldate = {2022-11-10},
	volume = {7},
	year = {2022},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/scirobotics.abl8419},
	bdsk-url-2 = {https://doi.org/10.1126/scirobotics.abl8419}}

@inproceedings{Schemmel10,
	abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project "FACETS" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a unified neural system description language, called PyNN, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within FACETS is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
	author = {Schemmel, Johannes and Br{\"u}derle, Daniel and Gr{\"u}bl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ISCAS.2010.5536970},
	keywords = {Biological neural networks, Biological system modeling, Biological tissues, Biology computing, Hardware, Large-scale systems, Neuromorphics, Numerical simulation, Semiconductor device modeling, Usability, ⛔ No INSPIRE recid found},
	month = may,
	note = {ISSN: 2158-1525},
	pages = {1947--1950},
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/ISCAS.2010.5536970}}

@article{Schmitt39,
	abstract = {1. In avoiding certain inherent indeterminacies in classical morphological methods and in obtaining further details regarding the microscopic and ultra-microscopic structure of nerve axon sheaths, the methods of polarization optics and X-ray diffraction are of great value. In the case of the myelin sheaths of vertebrate nerve fibres, for example, the optical and diffraction studies indicate the structure of the living fibre's sheath to be of smectic mixed fluid-crystalline nature. The structure is, therefore, readily altered by chemical treatment to form the artifacts commonly observed in histological preparations. 2. A number of considerations suggest that the specific configuration of the lipoid and protein components of the myelin sheath is as follows. The proteins occur as thin sheets wrapped concentrically about the axon, with two bimolecular layers of lipoids interspersed between adjacent protein layers. While this means that in a radial direction within the cylindrical sheath there are alternate predominantly aqueous and predominantly hyirocarbon phases, the latter cannot be described as being entirely ``non-aqueous'' 3. Polarization optical studies show that, contrary to the general view, invertebrate nerve fibres quite widely possess, aside from connective tissue investments, thin sheaths which are essentially similar in ultrastructure to the well-defined myelin sheaths of vertebrate fibres. The demonstration of this fact involved a reinterpretation of the meaning of Gothlin's metatropic reaction, in which immersion of the fibre in media of high refractive index permits the (intrinsic) birefringence of lipoids present in the normal sheath in an oriented condition to become apparent by the reduction of the masking (form) double refraction of protein. Associated with the invertebrate metatropic axon sheaths are cells similar to the Schwann cells of vertebrate fibres. 4. Quantitative birefringence studies have disclosed that the axon sheaths of a wide variety of fibre types differ chiefly with respect to the relative amounts of oriented protein and lipoid present. This difference is observed not only between typical invertebrate and vertebrate fibres, but also when the fibres of a single vertebrate nerve are compared. For example, the curve obtained when sheath birefringence of frog sciatic fibres is plotted against fibre diameter shows wide variations in the magnitude of double refraction, changing continuously from birefringence due preponderantly to lipoids, in the case of the larger fibres, to that which, in the smallest fibres, results primarily from proteins. The transition from lipoid to protein predominance occurs at a fibre diameter of about 2μ., agreeing well with the division between ``medullated'' and ``non-medullated'' fibres arrived at by histologists. It has been suggested that the low concentration of lipoid in the sheaths of small fibres is related to physical factors opposing the introduction of the lipoids into cylindrical structures of high curvature. 5. Examination of available information with respect to the relation of the velocity of impulse propagation to certain fibre characteristics, such as diameter and sheath ultrastructure, indicates that in a wide variety of fibres conduction velocity is a function of both of these factors. Thus, if fibres from invertebrate and vertebrate sources are classified according to sheath composition and ultrastructure, it is found that, within a group having similar sheaths, fast conduction is favoured by large diameter, while between groups with different sheaths, heavy myelination results in faster propagation. Comparison of fibre velocities with diameter alone, without regard to degree of myelination, is apt to be confusing, a fact which should be borne in mind in attempting to relate conduction velocity to diameter in a nerve, such as the frog sciatic, which contains fibres with very different sheaths. 6. Several types of invertebrate and vertebrate unipolar ganglion cells have been observed to possess investments similar to the axon sheaths and continuous with the latter. The entire surface of these neurons, therefore, is provided with a characteristic lipoid-protein covering, except possibly at the nodes of the myelin sheaths of the vertebrate sensory axons. The limiting envelopes of certain other cells and nuclei have been shown to possess an ultrastructure similar in type to that of the axon sheath. Permeability studies on cells have indicated the importance of lipoids and proteins in determining the properties of the plasma membrane, but it cannot be concluded that the visible envelopes are identical with the membrane which determines the physiological properties, since electrical and chemical studies favour the view that this membrane is extremely thin. The parallelisms observed between nerve sheath ultrastructure and physiological function, however, suggest some relation of these to membrane phenomena, and it is particularly difficult to understand how a multilayered structure, such as the vertebrate axon's myelin sheath, could fail to influence the chemical and electrical accessibility of the axon.},
	author = {Schmitt, Francis O. and Bear, Richard S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1111/j.1469-185X.1939.tb00922.x},
	issn = {1469-185X},
	journal = {Biological Reviews},
	language = {en},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-185X.1939.tb00922.x},
	number = {1},
	pages = {27--50},
	title = {The {Ultrastructure} of the {Nerve} {Axon} {Sheath}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-185X.1939.tb00922.x},
	urldate = {2022-11-08},
	volume = {14},
	year = {1939},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-185X.1939.tb00922.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1469-185X.1939.tb00922.x}}

@article{Schmolesky98,
	abstract = {The onset latencies of single-unit responses evoked by flashing visual stimuli were measured in the parvocellular (P) and magnocellular (M) layers of the dorsal lateral geniculate nucleus (LGNd) and in cortical visual areas V1, V2, V3, V4, middle temporal area (MT), medial superior temporal area (MST), and in the frontal eye field (FEF) in individual anesthetized monkeys. Identical procedures were carried out to assess latencies in each area, often in the same monkey, thereby permitting direct comparisons of timing across areas. This study presents the visual flash-evoked latencies for cells in areas where such data are common (V1 and V2), and are therefore a good standard, and also in areas where such data are sparse (LGNd M and P layers, MT, V4) or entirely lacking (V3, MST, and FEF in anesthetized preparation). Visual-evoked onset latencies were, on average, 17 ms shorter in the LGNd M layers than in the LGNd P layers. Visual responses occurred in V1 before any other cortical area. The next wave of activation occurred concurrently in areas V3, MT, MST, and FEF. Visual response latencies in areas V2 and V4 were progressively later and more broadly distributed. These differences in the time course of activation across the dorsal and ventral streams provide important temporal constraints on theories of visual processing.},
	author = {Schmolesky, M. T. and Wang, Y. and Hanes, D. P. and Thompson, K. G. and Leutgeb, S. and Schall, J. D. and Leventhal, A. G.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.1998.79.6.3272},
	issn = {0022-3077},
	journal = {Journal of Neurophysiology},
	keywords = {Animals, Evoked Potentials, Visual, Macaca, Neurons, Photic Stimulation, Signal Transduction, Time Factors, Vision, Ocular, Visual Cortex},
	language = {eng},
	month = jun,
	number = {6},
	pages = {3272--3278},
	pmid = {9636126},
	title = {Signal timing across the macaque visual system},
	volume = {79},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1152/jn.1998.79.6.3272}}

@article{Schneidman06,
	author = {Schneidman, Elad and Berry, Michael J. and Segev, Ronen and Bialek, William},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/nature04701},
	journal = {Nature},
	language = {en},
	number = {7087},
	pages = {1007--1012},
	title = {Weak pairwise correlations imply strongly correlated network states in a neural population},
	url = {https://doi.org/b9ph32},
	volume = {440},
	year = {2006},
	bdsk-url-1 = {https://doi.org/b9ph32},
	bdsk-url-2 = {https://doi.org/10.1038/nature04701}}

@article{Schrader08,
	abstract = {{\textless}jats:p{\textgreater} The synfire chain model has been proposed as the substrate that underlies computational processes in the brain and has received extensive theoretical study. In this model cortical tissue is composed of a superposition of feedforward subnetworks (chains) each capable of transmitting packets of synchronized spikes with high reliability. Computations are then carried out by interactions of these chains. Experimental evidence for synfire chains has so far been limited to inference from detection of a few repeating spatiotemporal neuronal firing patterns in multiple single-unit recordings. Demonstration that such patterns actually come from synfire activity would require finding a meta organization among many detected patterns, as yet an untried approach. In contrast we present here a new method that directly visualizes the repetitive occurrence of synfire activity even in very large data sets of multiple single-unit recordings. We achieve reliability and sensitivity by appropriately averaging over neuron space (identities) and time. We test the method with data from a large-scale balanced recurrent network simulation containing 50 randomly activated synfire chains. The sensitivity is high enough to detect synfire chain activity in simultaneous single-unit recordings of 100 to 200 neurons from such data, enabling application to experimental data in the near future. {\textless}/jats:p{\textgreater}},
	author = {Schrader, Sven and Gr{\"u}n, Sonja and Diesmann, Markus and Gerstein, George L.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.01245.2007},
	journal = {Journal of Neurophysiology},
	language = {en},
	month = oct,
	number = {4},
	pages = {2165--2176},
	title = {Detecting {Synfire} {Chain} {Activity} {Using} {Massively} {Parallel} {Spike} {Train} {Recording}},
	url = {https://doi.org/cvb22p},
	volume = {100},
	year = {2008},
	bdsk-url-1 = {https://doi.org/cvb22p},
	bdsk-url-2 = {https://doi.org/10.1152/jn.01245.2007}}

@article{Schuman22,
	abstract = {Neuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic computing has focused on hardware development. Here, we review recent results in neuromorphic computing algorithms and applications. We highlight characteristics of neuromorphic computing technologies that make them attractive for the future of computing and we discuss opportunities for future development of algorithms and applications on these systems.},
	author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
	copyright = {2022 Springer Nature America, Inc.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s43588-021-00184-y},
	issn = {2662-8457},
	journal = {Nature Computational Science},
	keywords = {Computational science, Computer science},
	language = {en},
	month = jan,
	note = {Number: 1 Publisher: Nature Publishing Group},
	number = {1},
	pages = {10--19},
	title = {Opportunities for neuromorphic computing algorithms and applications},
	url = {http://www.nature.com/articles/s43588-021-00184-y},
	urldate = {2022-11-10},
	volume = {2},
	year = {2022},
	bdsk-url-1 = {http://www.nature.com/articles/s43588-021-00184-y},
	bdsk-url-2 = {https://doi.org/10.1038/s43588-021-00184-y}}

@article{Schuman17,
	abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
	author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {arXiv:1705.06963 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	month = may,
	note = {arXiv: 1705.06963},
	title = {A {Survey} of {Neuromorphic} {Computing} and {Neural} {Networks} in {Hardware}},
	url = {http://arxiv.org/abs/1705.06963},
	urldate = {2021-03-25},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1705.06963}}

@article{Seidl10,
	abstract = {Understanding binaural perception requires detailed analyses of the neural circuitry responsible for the computation of interaural time differences (ITDs). In the avian brainstem, this circuit consists of internal axonal delay lines innervating an array of coincidence detector neurons that encode external ITDs. Nucleus magnocellularis (NM) neurons project to the dorsal dendritic field of the ipsilateral nucleus laminaris (NL) and to the ventral field of the contralateral NL. Contralateral-projecting axons form a delay line system along a band of NL neurons. Binaural acoustic signals in the form of phase-locked action potentials from NM cells arrive at NL and establish a topographic map of sound source location along the azimuth. These pathways are assumed to represent a circuit similar to the Jeffress model of sound localization, establishing a place code along an isofrequency contour of NL. Three-dimensional measurements of axon lengths reveal major discrepancies with the current model; the temporal offset based on conduction length alone makes encoding of physiological ITDs impossible. However, axon diameter and distances between Nodes of Ranvier also influence signal propagation times along an axon. Our measurements of these parameters reveal that diameter and internode distance can compensate for the temporal offset inferred from axon lengths alone. Together with other recent studies, these unexpected results should inspire new thinking on the cellular biology, evolution, and plasticity of the circuitry underlying low-frequency sound localization in both birds and mammals.},
	author = {Seidl, Armin H. and Rubel, Edwin W. and Harris, David M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.3464-09.2010},
	issn = {1529-2401},
	journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
	keywords = {Acoustic Stimulation, Animals, Animals, Newborn, Auditory Pathways, Auditory Perception, Brain Stem, Chickens, Nerve Net, Sound Localization, Time Factors, itd},
	language = {eng},
	month = jan,
	number = {1},
	pages = {70--80},
	pmcid = {PMC2822993},
	pmid = {20053889},
	title = {Mechanisms for adjusting interaural time differences to achieve binaural coincidence detection},
	volume = {30},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1523/JNEUROSCI.3464-09.2010}}

@article{Serre07,
	author = {Serre, T. and Oliva, A. and Poggio, T.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.0700622104},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	note = {tex.bdsk-url-2: https://doi.org/10.1073/pnas.0700622104 tex.date-added: 2022-05-05 19:08:15 +0200 tex.date-modified: 2022-05-05 19:08:15 +0200},
	number = {15},
	pages = {6424--6429},
	title = {A feedforward architecture accounts for rapid categorization},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.0700622104}}

@article{Simoncelli03,
	author = {Simoncelli, Eero P and Paninski, Liam and Pillow, Jonathan and Schwartz, Odelia},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	language = {en},
	title = {Characterization of {Neural} {Responses} with {Stochastic} {Stimuli}},
	url = {http://pillowlab.princeton.edu/pubs/simoncelli03c-preprint.pdf},
	year = {2003},
	bdsk-url-1 = {http://pillowlab.princeton.edu/pubs/simoncelli03c-preprint.pdf}}

@article{Simons16,
	abstract = {Myelinated nerve fibers have evolved to enable fast and efficient transduction of electrical signals in the nervous system. To act as an electric insulator, the myelin sheath is formed as a multilamellar membrane structure by the spiral wrapping and subsequent compaction of the oligodendroglial plasma membrane around central nervous system (CNS) axons. Current evidence indicates that the myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes are metabolically active and functionally connected to the subjacent axon via cytoplasmic-rich myelinic channels for movement of macromolecules to and from the internodal periaxonal space under the myelin sheath. This review summarizes our current understanding of how myelin is generated and also the role of oligodendrocytes in supporting the long-term integrity of myelinated axons., The myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes in the sheath are connected to the subjacent axon via cytoplasmic-rich myelinic channels, and they actively support the integrity of the neuron.},
	author = {Simons, Mikael and Nave, Klaus-Armin},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1101/cshperspect.a020479},
	issn = {1943-0264},
	journal = {Cold Spring Harbor Perspectives in Biology},
	month = jan,
	number = {1},
	pages = {a020479},
	pmcid = {PMC4691794},
	pmid = {26101081},
	shorttitle = {Oligodendrocytes},
	title = {Oligodendrocytes: {Myelination} and {Axonal} {Support}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691794/},
	urldate = {2022-11-10},
	volume = {8},
	year = {2016},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691794/},
	bdsk-url-2 = {https://doi.org/10.1101/cshperspect.a020479}}

@article{Singer95,
	author = {Singer, W and Gray, C M},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1146/annurev.ne.18.030195.003011},
	journal = {Annual Review of Neuroscience},
	language = {en},
	month = mar,
	number = {1},
	pages = {555--586},
	title = {Visual {Feature} {Integration} and the {Temporal} {Correlation} {Hypothesis}},
	url = {https://doi.org/cgx8jp},
	volume = {18},
	year = {1995},
	bdsk-url-1 = {https://doi.org/cgx8jp},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.ne.18.030195.003011}}

@inproceedings{Sironi18,
	abstract = {Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their framebased counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust eventbased representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation.},
	address = {Salt Lake City, UT, USA},
	author = {Sironi, Amos and Brambilla, Manuele and Bourdis, Nicolas and Lagorce, Xavier and Benosman, Ryad},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/CVPR.2018.00186},
	isbn = {978-1-5386-6420-9},
	language = {en},
	month = jun,
	pages = {1731--1740},
	publisher = {IEEE},
	shorttitle = {{HATS}},
	title = {{HATS}: {Histograms} of {Averaged} {Time} {Surfaces} for {Robust} {Event}-{Based} {Object} {Classification}},
	url = {https://ieeexplore.ieee.org/document/8578284/},
	urldate = {2020-12-23},
	year = {2018},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8578284/},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR.2018.00186}}

@article{Softky93,
	author = {Softky, WR and Koch, C},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.13-01-00334.1993},
	journal = {The Journal of Neuroscience},
	language = {en},
	month = jan,
	number = {1},
	pages = {334--350},
	title = {The highly irregular firing of cortical cells is inconsistent with temporal integration of random {EPSPs}},
	url = {https://doi.org/ggzph6},
	volume = {13},
	year = {1993},
	bdsk-url-1 = {https://doi.org/ggzph6},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.13-01-00334.1993}}

@article{Sotomayor-Gomez21,
	author = {Sotomayor-G{\'o}mez, Boris and Battaglia, Francesco P and Vinck, Martin},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1101/2020.06.03.131573},
	journal = {bioRxiv : the preprint server for biology},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2020--06},
	title = {{SpikeShip}: {A} method for fast, unsupervised discovery of high-dimensional neural spiking patterns},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.03.131573},
	year = {2021},
	bdsk-url-1 = {https://www.biorxiv.org/content/10.1101/2020.06.03.131573},
	bdsk-url-2 = {https://doi.org/10.1101/2020.06.03.131573}}

@article{Spencer18,
	abstract = {Asynchrony among synaptic inputs may prevent a neuron from responding to behaviorally relevant sensory stimuli. For example, "octopus cells" are monaural neurons in the auditory brainstem of mammals that receive input from auditory nerve fibers (ANFs) representing a broad band of sound frequencies. Octopus cells are known to respond with finely timed action potentials at the onset of sounds despite the fact that due to the traveling wave delay in the cochlea, synaptic input from the auditory nerve is temporally diffuse. This paper provides a proof of principle that the octopus cells' dendritic delay may provide compensation for this input asynchrony, and that synaptic weights may be adjusted by a spike-timing dependent plasticity (STDP) learning rule. This paper used a leaky integrate and fire model of an octopus cell modified to include a "rate threshold," a property that is known to create the appropriate onset response in octopus cells. Repeated audio click stimuli were passed to a realistic auditory nerve model which provided the synaptic input to the octopus cell model. A genetic algorithm was used to find the parameters of the STDP learning rule that reproduced the microscopically observed synaptic connectivity. With these selected parameter values it was shown that the STDP learning rule was capable of adjusting the values of a large number of input synaptic weights, creating a configuration that compensated the traveling wave delay of the cochlea.},
	author = {Spencer, Martin J. and Meffin, Hamish and Burkitt, Anthony N. and Grayden, David B.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fncom.2018.00036},
	issn = {1662-5188},
	journal = {Frontiers in Computational Neuroscience},
	keywords = {auditory brainstem, cochlear nucleus, dendritic delay, octopus cells, spike-timing dependent plasticity, ⛔ No INSPIRE recid found},
	language = {eng},
	pages = {36},
	pmcid = {PMC5996126},
	pmid = {29922141},
	title = {Compensation for {Traveling} {Wave} {Delay} {Through} {Selection} of {Dendritic} {Delays} {Using} {Spike}-{Timing}-{Dependent} {Plasticity} in a {Model} of the {Auditory} {Brainstem}},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3389/fncom.2018.00036}}

@inproceedings{State19,
	abstract = {Spiking Neural Networks (SNNs) are a promising computational paradigm, both to understand biological information processing and for low-power, embedded chips. Although SNNs are known to encode information in the precise timing of spikes, conventional artificial learning algorithms do not take this into account directly. In this work, we implement the spike timing by training the synaptic delays in a single layer SNN. We use two different approaches: a classical gradient descent and a direct algebraic method that is based on a complex-valued encoding of the spikes. Both algorithms are equally able to correctly solve simple detection tasks. Our work provides new optimization methods for the data analysis of highly time-dependent data and training methods for neuromorphic chips.},
	address = {Cham},
	author = {State, Laura and Vilimelis Aceituno, Pau},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} -- {ICANN} 2019: {Theoretical} {Neural} {Computation}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/978-3-030-30487-4_54},
	editor = {Tetko, Igor V. and K{\r u}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
	isbn = {978-3-030-30487-4},
	language = {en},
	pages = {713--717},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Training {Delays} in {Spiking} {Neural} {Networks}},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-30487-4_54}}

@article{Steadman20,
	abstract = {The generation of myelin-forming oligodendrocytes persists throughout life and is regulated by neural activity. Here we tested whether experience-driven changes in oligodendrogenesis are important for memory consolidation. We found that water maze learning promotes oligodendrogenesis and de
novo myelination in the cortex and associated white matter tracts. Preventing these learning-induced increases in oligodendrogenesis without affecting existing oligodendrocytes impaired memory consolidation of water maze, as well as contextual fear, memories. These results suggest that de
novo myelination tunes activated circuits, promoting coordinated activity that is important for memory consolidation. Consistent with this, contextual fear learning increased the coupling of hippocampal sharp wave ripples and cortical spindles, and these learning-induced increases in ripple-spindle coupling were blocked when oligodendrogenesis was suppressed. Our results identify a non-neuronal form of plasticity that remodels hippocampal-cortical networks following learning and is required for memory consolidation., 
          
        , Experience-dependent de
novo myelination may fine-tune activated circuits by promoting brain synchrony, important for memory consolidation. Steadman et al. find that blocking this form of adaptive myelination prevents learning-induced increases in coordinated activity and impairs memory consolidation.},
	author = {Steadman, Patrick E. and Xia, Frances and Ahmed, Moriam and Mocle, Andrew J. and Penning, Amber R.A. and Geraghty, Anna C. and Steenland, Hendrik W. and Monje, Michelle and Josselyn, Sheena A. and Frankland, Paul W.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuron.2019.10.013},
	issn = {0896-6273},
	journal = {Neuron},
	keywords = {⛔ No INSPIRE recid found},
	month = jan,
	number = {1},
	pages = {150--164.e6},
	pmcid = {PMC7579726},
	pmid = {31753579},
	title = {Disruption of {Oligodendrogenesis} {Impairs} {Memory} {Consolidation} in {Adult} {Mice}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7579726/},
	urldate = {2022-11-14},
	volume = {105},
	year = {2020},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7579726/},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2019.10.013}}

@article{Stella19,
	author = {Stella, Alessandra and Quaglio, Pietro and Torre, Emiliano and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.biosystems.2019.104022},
	journal = {Biosystems},
	language = {en},
	month = nov,
	pages = {104022},
	title = {3d-{SPADE}: {Significance} evaluation of spatio-temporal patterns of various temporal extents},
	url = {https://doi.org/gpshj2},
	volume = {185},
	year = {2019},
	bdsk-url-1 = {https://doi.org/gpshj2},
	bdsk-url-2 = {https://doi.org/10.1016/j.biosystems.2019.104022}}

@article{Stella22,
	author = {Stella, Alessandra and Bouss, Peter and Palm, G{\"u}nther and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/eneuro.0505-21.2022},
	journal = {eneuro},
	language = {en},
	number = {3},
	pages = {ENEURO.0505--21.2022},
	title = {Comparing {Surrogates} to {Evaluate} {Precisely} {Timed} {Higher}-{Order} {Spike} {Correlations}},
	url = {https://doi.org/gqjvht},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/gqjvht},
	bdsk-url-2 = {https://doi.org/10.1523/eneuro.0505-21.2022}}

@article{Stetson92,
	abstract = {Associations among measures of median, ulnar, and sural nerve conduction and age, skin temperature, sex, and anthropometric factors were evaluated in a population of 105 healthy, asymptomatic adults without occupational exposure to highly repetitive or forceful hand exertions. Height was negatively associated with sensory amplitude in all nerves tested (P {\textless} 0.001), and positively associated with median and ulnar sensory distal latencies (P {\textless} 0.01) and sural latency (P {\textless} 0.001). Index finger circumference was negatively associated with median and ulnar sensory amplitudes (P {\textless} 0.05). Sex, in isolation from highly correlated anthropometric factors such as height, was not found to be a significant predictor of median or ulnar nerve conduction measures. Equations using age, height, and finger circumference for prediction of normal values are presented. Failure to adjust normal nerve conduction values for these factors decreases the diagnostic specificity and sensitivity of the described measures, and may result in misclassification of individuals. {\copyright} 1992 John Wiley \& Sons, Inc.},
	author = {Stetson, Diana S. and Albers, James W. and Silverstein, Barbara A. and Wolfe, Robert A.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1002/mus.880151007},
	issn = {1097-4598},
	journal = {Muscle \& Nerve},
	keywords = {age, anthropometry, height, nerve conduction studies, normal values},
	language = {en},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mus.880151007},
	number = {10},
	pages = {1095--1104},
	title = {Effects of age, sex, and anthropometric factors on nerve conduction measures},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mus.880151007},
	urldate = {2022-11-10},
	volume = {15},
	year = {1992},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mus.880151007},
	bdsk-url-2 = {https://doi.org/10.1002/mus.880151007}}

@article{Stimberg19,
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.7554/eLife.47314},
	issn = {2050-084X},
	journal = {eLife},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	pages = {e47314},
	title = {Brian 2, an intuitive and efficient neural simulator},
	url = {https://elifesciences.org/articles/47314},
	urldate = {2022-11-14},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://elifesciences.org/articles/47314},
	bdsk-url-2 = {https://doi.org/10.7554/eLife.47314}}

@inproceedings{Stoffregen19,
	abstract = {In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called "events"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10\%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90\% accuracy at 4 pixels relative displacement.},
	author = {Stoffregen, Timo and Gallego, Guillermo and Drummond, Tom and Kleeman, Lindsay and Scaramuzza, Davide},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ICCV.2019.00734},
	keywords = {Cameras, Computer vision, Image segmentation, Motion compensation, Motion segmentation, Robot vision systems, Tracking},
	month = oct,
	note = {ISSN: 2380-7504},
	pages = {7243--7252},
	title = {Event-{Based} {Motion} {Segmentation} by {Motion} {Compensation}},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2019.00734}}

@article{Stringer19,
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/s41586-019-1346-5},
	journal = {Nature},
	language = {en},
	number = {7765},
	pages = {361--365},
	title = {High-dimensional geometry of population responses in visual cortex},
	url = {https://doi.org/gf4cfj},
	volume = {571},
	year = {2019},
	bdsk-url-1 = {https://doi.org/gf4cfj},
	bdsk-url-2 = {https://doi.org/10.1038/s41586-019-1346-5}}

@article{Stringer21,
	author = {Stringer, Carsen and Michaelos, Michalis and Tsyboulski, Dmitri and Lindo, Sarah E. and Pachitariu, Marius},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.cell.2021.03.042},
	journal = {Cell},
	language = {en},
	number = {10},
	pages = {2767--2778.e15},
	title = {High-precision coding in visual cortex},
	url = {https://doi.org/gjqbjd},
	volume = {184},
	year = {2021},
	bdsk-url-1 = {https://doi.org/gjqbjd},
	bdsk-url-2 = {https://doi.org/10.1016/j.cell.2021.03.042}}

@misc{Stringer20,
	author = {Stringer, Carsen},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {⛔ No INSPIRE recid found},
	title = {{MouseLand}/rastermap: {A} multi-dimensional embedding algorithm},
	url = {https://github.com/MouseLand/rastermap},
	urldate = {2022-12-19},
	year = {2020},
	bdsk-url-1 = {https://github.com/MouseLand/rastermap}}

@article{Stringer19a,
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.aav7893},
	journal = {Science},
	language = {en},
	number = {6437},
	title = {Spontaneous behaviors drive multidimensional, brainwide activity},
	url = {https://doi.org/gfz6mh},
	volume = {364},
	year = {2019},
	bdsk-url-1 = {https://doi.org/gfz6mh},
	bdsk-url-2 = {https://doi.org/10.1126/science.aav7893}}

@article{Sun16,
	abstract = {Polychronous neuronal group (PNG), a type of cell assembly, is one of the putative mechanisms for neural information representation. According to the reader-centric definition, some readout neurons can become selective to the information represented by polychronous neuronal groups under ongoing activity. Here, in computational models, we show that the frequently activated polychronous neuronal groups can be learned by readout neurons with joint weight-delay spike-timing-dependent plasticity. The identity of neurons in the group and their expected spike timing at millisecond scale can be recovered from the incoming weights and delays of the readout neurons. The detection performance can be further improved by two layers of readout neurons. In this way, the detection of polychronous neuronal groups becomes an intrinsic part of the network, and the readout neurons become differentiated members in the group to indicate whether subsets of the group have been activated according to their spike timing. The readout spikes representing this information can be used to analyze how PNGs interact with each other or propagate to downstream networks for higher-level processing.},
	author = {Sun, Haoqi and Sourina, Olga and Huang, Guang-Bin},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/NECO_a_00879},
	issn = {0899-7667},
	journal = {Neural Computation},
	month = oct,
	note = {tex.eprint: https://direct.mit.edu/neco/article-pdf/28/10/2181/972099/neco{\textbackslash}\_a{\textbackslash}\_00879.pdf},
	number = {10},
	pages = {2181--2212},
	title = {Learning polychronous neuronal groups using joint weight-delay spike-timing-dependent plasticity},
	url = {https://doi.org/10.1162/NECO_a_00879},
	volume = {28},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1162/NECO_a_00879}}

@article{Susi21,
	abstract = {The recent "multi-neuronal spike sequence detector" (MNSD) architecture integrates the weight- and delay-adjustment methods by combining heterosynaptic plasticity with the neurocomputational feature spike latency, representing a new opportunity to understand the mechanisms underlying biological learning. Unfortunately, the range of problems to which this topology can be applied is limited because of the low cardinality of the parallel spike trains that it can process, and the lack of a visualization mechanism to understand its internal operation. We present here the nMNSD structure, which is a generalization of the MNSD to any number of inputs. The mathematical framework of the structure is introduced, together with the "trapezoid method," that is a reduced method to analyze the recognition mechanism operated by the nMNSD in response to a specific input parallel spike train. We apply the nMNSD to a classification problem previously faced with the classical MNSD from the same authors, showing the new possibilities the nMNSD opens, with associated improvement in classification performances. Finally, we benchmark the nMNSD on the classification of static inputs (MNIST database) obtaining state-of-the-art accuracies together with advantageous aspects in terms of time- and energy-efficiency if compared to similar classification methods.},
	author = {Susi, Gianluca and Ant{\'o}n-Toro, Luis F. and Maest{\'u}, Fernando and Pereda, Ernesto and Mirasso, Claudio},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2021.582608},
	issn = {1662-4548},
	journal = {Frontiers in Neuroscience},
	keywords = {MNIST database, MNSD, classification, delay learning, heterosynaptic plasticity, online learning, spike latency},
	language = {eng},
	pages = {582608},
	pmcid = {PMC7933525},
	pmid = {33679293},
	title = {{nMNSD}-{A} {Spiking} {Neuron}-{Based} {Classifier} {That} {Combines} {Weight}-{Adjustment} and {Delay}-{Shift}},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3389/fnins.2021.582608}}

@article{Suthers02,
	abstract = {One of the challenges when considering the motor control of birdsong is to understand how such a wide variety of temporally and spectrally diverse vocalizations are learned and produced. A better understanding of central neural processing, together with direct endoscopic observations and physiological studies of peripheral motor function during singing, has resulted in the formation of new theoretical models of song production. Recent work suggests that it may be more profitable to focus on the temporal relationship between control parameters than to attempt to directly correlate neural processing with details of the acoustic output.},
	author = {Suthers, Roderick A and Margoliash, Daniel},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/s0959-4388(02)00386-0},
	issn = {0959-4388},
	journal = {Current opinion in neurobiology},
	number = {6},
	pages = {684--90},
	title = {Motor control of birdsong.},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/12490259},
	volume = {12},
	year = {2002},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/12490259},
	bdsk-url-2 = {https://doi.org/10.1016/s0959-4388(02)00386-0}}

@article{Tavanaei18,
	author = {Tavanaei, Amirhossein and Masquelier, Timoth{\'e}e and Maida, Anthony},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neunet.2018.05.018},
	journal = {Neural Networks},
	language = {en},
	month = sep,
	pages = {294--303},
	title = {Representation learning using event-based {STDP}},
	url = {https://doi.org/gd6tgv},
	volume = {105},
	year = {2018},
	bdsk-url-1 = {https://doi.org/gd6tgv},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2018.05.018}}

@inproceedings{Thanasoulis21,
	abstract = {Neuromorphic engineering implements large-scale systems that provide a high integration density of power efficient synapse-and-neuron blocks. This represents a promising alternative to the numerical simulations for studying the dynamics of spiking neural networks. A key aspect of these systems is the implementation of communication and routing of pulse events produced by the neural network. In this paper we present a measurement methodology and results of a neural benchmark that tests the configurable delays, multicasting and connectivity implemented by a routing logic for neuromorphic hardware. Pulses are handled according to their timestamp and transmitted with configurable delays and routing to different post-synaptic neurons. The results show the suitability of communication and routing logic for delay-based neural computation and point out effects of time discretization in resolution of pulse timestamps.},
	author = {Thanasoulis, Vasilis and Vogginger, Bernhard and Partzsch, Johannes and Mayr, Christian},
	booktitle = {2021 28th {IEEE} {International} {Conference} on {Electronics}, {Circuits}, and {Systems} ({ICECS})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ICECS53924.2021.9665468},
	keywords = {Benchmark testing, Hardware, Multicast communication, Neuromorphic engineering, Neurons, Numerical simulation, Routing},
	month = nov,
	pages = {1--5},
	shorttitle = {Delay-{Based} {Neural} {Computation}},
	title = {Delay-{Based} {Neural} {Computation}: {Pulse} {Routing} {Architecture} and {Benchmark} {Application} in {FPGA}},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/ICECS53924.2021.9665468}}

@article{Thorpe01,
	author = {Thorpe, Simon J. and Fabre-Thorpe, Mich{\`e}le},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.1058249},
	journal = {Science},
	language = {en},
	month = jan,
	number = {5502},
	pages = {260--263},
	title = {Seeking {Categories} in the {Brain}},
	url = {https://doi.org/bzn42k},
	volume = {291},
	year = {2001},
	bdsk-url-1 = {https://doi.org/bzn42k},
	bdsk-url-2 = {https://doi.org/10.1126/science.1058249}}

@article{Thorpe96,
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/381520a0},
	journal = {Nature},
	language = {en},
	number = {6582},
	pages = {520--522},
	title = {Speed of processing in the human visual system},
	url = {https://doi.org/c4v35x},
	volume = {381},
	year = {1996},
	bdsk-url-1 = {https://doi.org/c4v35x},
	bdsk-url-2 = {https://doi.org/10.1038/381520a0}}

@article{Tolle11,
	author = {Tolle, Kristin M. and Tansley, D. Stewart W. and Hey, Anthony J. G.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/jproc.2011.2155130},
	journal = {Proceedings of the IEEE},
	number = {8},
	pages = {1334--1337},
	title = {The {Fourth} {Paradigm}: {Data}-{Intensive} {Scientific} {Discovery} [{Point} of {View}]},
	url = {https://doi.org/dfggjc},
	volume = {99},
	year = {2011},
	bdsk-url-1 = {https://doi.org/dfggjc},
	bdsk-url-2 = {https://doi.org/10.1109/jproc.2011.2155130}}

@article{Torre16,
	author = {Torre, Emiliano and Canova, Carlos and Denker, Michael and Gerstein, George and Helias, Moritz and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1371/journal.pcbi.1004939},
	editor = {Sporns, Olaf},
	journal = {PLOS Computational Biology},
	language = {en},
	number = {7},
	pages = {e1004939},
	title = {{ASSET}: {Analysis} of {Sequences} of {Synchronous} {Events} in {Massively} {Parallel} {Spike} {Trains}},
	url = {https://doi.org/gnpx4q},
	volume = {12},
	year = {2016},
	bdsk-url-1 = {https://doi.org/gnpx4q},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1004939}}

@article{Torre13,
	author = {Torre, Emiliano and Picado-Mui{\~n}o, David and Denker, Michael and Borgelt, Christian and Gr{\"u}n, Sonja},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fncom.2013.00132},
	journal = {Frontiers in Computational Neuroscience},
	title = {Statistical evaluation of synchronous spike patterns extracted by frequent item set mining},
	url = {https://doi.org/gpshgq},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://doi.org/gpshgq},
	bdsk-url-2 = {https://doi.org/10.3389/fncom.2013.00132}}

@article{Torre16a,
	author = {Torre, E. and Quaglio, P. and Denker, M. and Brochier, T. and Riehle, A. and Grun, S.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.4375-15.2016},
	journal = {Journal of Neuroscience},
	language = {en},
	number = {32},
	pages = {8329--8340},
	title = {Synchronous {Spike} {Patterns} in {Macaque} {Motor} {Cortex} during an {Instructed}-{Delay} {Reach}-to-{Grasp} {Task}},
	url = {https://doi.org/f82b6j},
	volume = {36},
	year = {2016},
	bdsk-url-1 = {https://doi.org/f82b6j},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.4375-15.2016}}

@inproceedings{Tschechne14,
	abstract = {Computational models of visual processing often use frame-based image acquisition techniques to process a temporally changing stimulus. This approach is unlike biological mechanisms that are spike-based and independent of individual frames. The neuromorphic Dynamic Vision Sensor (DVS) [Lichtsteiner et al., 2008] provides a stream of independent visual events that indicate local illumination changes, resembling spiking neurons at a retinal level. We introduce a new approach for the modelling of cortical mechanisms of motion detection along the dorsal pathway using this type of representation. Our model combines filters with spatio-temporal tunings also found in visual cortex to yield spatio-temporal and direction specificity. We probe our model with recordings of test stimuli, articulated motion and ego-motion. We show how our approach robustly estimates optic flow and also demonstrate how this output can be used for classification purposes.},
	address = {Cham},
	author = {Tschechne, Stephan and Sailer, Roman and Neumann, Heiko},
	booktitle = {Artificial {Neural} {Networks} in {Pattern} {Recognition}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/978-3-319-11656-3_16},
	editor = {El Gayar, Neamat and Schwenker, Friedhelm and Suen, Cheng},
	isbn = {978-3-319-11656-3},
	keywords = {Classification, Event-Vision, Neural Model, Optic Flow},
	language = {en},
	pages = {171--182},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Bio-{Inspired} {Optic} {Flow} from {Event}-{Based} {Neuromorphic} {Sensor} {Input}},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-319-11656-3_16}}

@article{Kempen21,
	author = {van Kempen, Jochem and Gieselmann, Marc A. and Boyd, Michael and Steinmetz, Nicholas A. and Moore, Tirin and Engel, Tatiana A. and Thiele, Alexander},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuron.2020.12.013},
	journal = {Neuron},
	language = {en},
	month = mar,
	number = {5},
	pages = {894--904.e8},
	title = {Top-down coordination of local cortical state during selective attention},
	url = {https://doi.org/ghvj3k},
	volume = {109},
	year = {2021},
	bdsk-url-1 = {https://doi.org/ghvj3k},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2020.12.013}}

@article{Rossum01,
	abstract = {The discrimination between two spike trains is a fundamental problem for both experimentalists and the nervous system itself. We introduce a measure for the distance between two spike trains. The distance has a time constant as a parameter. Depending on this parameter, the distance interpolates between a coincidence detector and a rate difference counter. The dependence of the distance on noise is studied with an integrate-and-fire model. For an intermediate range of the time constants, the distance depends linearly on the noise. This property can be used to determine the intrinsic noise of a neuron.},
	author = {van Rossum, M. C.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/089976601300014321},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {Algorithms, Evoked Potentials, Models, Neurological, Neurons, Poisson Distribution},
	language = {eng},
	month = apr,
	number = {4},
	pages = {751--763},
	pmid = {11255567},
	title = {A novel spike distance},
	volume = {13},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1162/089976601300014321}}

@article{Vanni01,
	abstract = {Proper understanding of processes underlying visual perception
 requires information on the activation order of distinct brain areas.
 We measured dynamics of cortical signals with magnetoencephalography
 while human subjects viewed stimuli at four visual quadrants. The
 signals were analyzed with minimum current estimates at the individual
 and group level. Activation emerged 55--70 ms after stimulus onset both
 in the primary posterior visual areas and in the anteromedial part of
 the cuneus. Other cortical areas were active after this initial dual
 activation. Comparison of data between species suggests that the
 anteromedial cuneus either comprises a homologue of the monkey area V6
 or is an area unique to humans. Our results show that visual stimuli
 activate two cortical areas right from the beginning of the cortical
 response. The anteromedial cuneus has the temporal position needed to
 interact with the primary visual cortex V1 and thereby to modify
 information transferred via V1 to extrastriate cortices.},
	author = {Vanni, Simo and Tanskanen, Topi and Sepp{\"a}, Mika and Uutela, Kimmo and Hari, Riitta},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1073/pnas.041600898},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	month = feb,
	number = {5},
	pages = {2776--2780},
	pmcid = {PMC30215},
	pmid = {11226316},
	title = {Coinciding early activation of the human primary visual cortex and anteromedial cuneus},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC30215/},
	urldate = {2022-11-08},
	volume = {98},
	year = {2001},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC30215/},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.041600898}}

@article{VanRullen06,
	author = {VanRullen, Rufin and Reddy, Leila and Koch, Christof},
	copyright = {Copyright {\copyright} 2006 Society for Neuroscience 0270-6474/06/26502-06.00/0},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/jneurosci.4654-05.2006},
	issn = {0270-6474, 1529-2401},
	journal = {Journal of Neuroscience},
	language = {en},
	month = jan,
	note = {00000},
	number = {2},
	pages = {502--507},
	pmid = {16407547},
	title = {The {Continuous} {Wagon} {Wheel} {Illusion} {Is} {Associated} with {Changes} in {Electroencephalogram} {Power} at 13 {Hz}},
	url = {https://www.jneurosci.org/content/26/2/502},
	urldate = {2019-11-05},
	volume = {26},
	year = {2006},
	bdsk-url-1 = {https://www.jneurosci.org/content/26/2/502},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.4654-05.2006}}

@article{Victor96,
	abstract = {1. We recorded single-unit and multi-unit activity in response to transient presentation of texture and grating patterns at 25 sites within the parafoveal representation of V1, V2, and V3 of two awake monkeys trained to perform a fixation task. In grating experiments, stimuli varied in orientation, spatial frequency, or both. In texture experiments, stimuli varied in contrast, check size, texture type, or pairs of these attributes. 2. To examine the nature and precision of temporal coding, we compared individual responses elicited by each set of stimuli in terms of two families of metrics. One family of metrics, D(spike), was sensitive to the absolute spike time (following stimulus onset). The second family of metrics, D(interval), was sensitive to the pattern of interspike intervals. In each family, the metrics depend on a parameter q, which expresses the precision of temporal coding. For q = 0, both metrics collapse into the "spike count" metric D(Count), which is sensitive to the number of impulses but insensitive to their position in time. 3. Each of these metrics, with values of q ranging from 0 to 512/s, was used to calculate the distance between all pairs of spike trains within each dataset. The extent of stimulus-specific clustering manifest in these pairwise distances was quantified by an information measure. Chance clustering was estimated by applying the same procedure to synthetic data sets in which responses were assigned randomly to the input stimuli. 4. Of the 352 data sets, 170 showed evidence of tuning via the spike count (q = 0) metric, 294 showed evidence of tuning via the spike time metric, 272 showed evidence of tuning via the spike interval metric to the stimulus attribute (contrast, check size, orientation, spatial frequency, or texture type) under study. Across the entire dataset, the information not attributable to chance clustering averaged 0.042 bits for the spike count metric, 0.171 bits for the optimal spike time metric, and 0.107 bits for the optimal spike interval metric. 5. The reciprocal of the optimal cost q serves as a measure of the temporal precision of temporal coding. In V1 and V2, with both metrics, temporal precision was highest for contrast (ca. 10-30 ms) and lowest for texture type (ca. 100 ms). This systematic dependence of q on stimulus attribute provides a possible mechanism for the simultaneous representation of multiple stimulus attributes in one spike train. 6. Our findings are inconsistent with Poisson models of spike trains. Synthetic data sets in which firing rate was governed by a time-dependent Poisson process matched to the observed poststimulus time histogram (PSTH) overestimated clustering induced by D(count) and, for low values of q, D(spike)[q] and D(intervals)[q]. Synthetic data sets constructed from a modified Poisson process, which preserved not only the PSTH but also spike count statistics accounted for the clustering induced by D(count) but underestimated the clustering induced by D(spike)[q] and D(interval)[q].},
	author = {Victor, J. D. and Purpura, K. P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1152/jn.1996.76.2.1310},
	issn = {0022-3077, 1522-1598},
	journal = {Journal of Neurophysiology},
	language = {en},
	month = aug,
	number = {2},
	pages = {1310--1326},
	shorttitle = {Nature and precision of temporal coding in visual cortex},
	title = {Nature and precision of temporal coding in visual cortex: a metric-space analysis},
	url = {https://www.physiology.org/doi/10.1152/jn.1996.76.2.1310},
	urldate = {2022-10-17},
	volume = {76},
	year = {1996},
	bdsk-url-1 = {https://www.physiology.org/doi/10.1152/jn.1996.76.2.1310},
	bdsk-url-2 = {https://doi.org/10.1152/jn.1996.76.2.1310}}

@article{Vidyasagar96,
	abstract = {For over three decades, the mechanism of orientation selectivity of visual cortical neurones has been hotly debated. While intracortical inhibition has been implicated as playing a vital role, it has been difficult to observe it clearly. On the basis of recent findings, we propose a model in which the visual cortex brings together a number of different mechanisms for generating orientation-selective responses. Orientation biases in the thalamo-cortical input fibres provide an initial weak selectivity either directly in the excitatory input or by acting via cortical interneurones. This weak selectivity of postsynaptic potentials is then amplified by voltage-sensitive conductances of the cell membrane and excitatory and inhibitory intracortical circuitry, resulting in the sharp tuning seen in the spike discharges of visual cortical cells.},
	author = {Vidyasagar, T. R. and Pei, X. and Volgushev, M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/S0166-2236(96)20027-X},
	issn = {0166-2236},
	journal = {Trends in Neurosciences},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jul,
	number = {7},
	pages = {272--277},
	title = {Multiple mechanisms underlying the orientation selectivity of visual cortical neurones},
	url = {https://www.sciencedirect.com/science/article/pii/S016622369620027X},
	urldate = {2022-12-16},
	volume = {19},
	year = {1996},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016622369620027X},
	bdsk-url-2 = {https://doi.org/10.1016/S0166-2236(96)20027-X}}

@article{Villette15,
	abstract = {The hippocampus is essential for spatiotemporal cognition. Sequences of neuronal activation provide a substrate for this fundamental function. At the behavioral timescale, these sequences have been shown to occur either in the presence of successive external landmarks or through internal mechanisms within an episodic memory task. In both cases, activity is externally constrained by the organization of the task and by the size of the environment explored. Therefore, it remains unknown whether hippocampal activity can self-organize into a default mode in the absence of any external memory demand or spatiotemporal boundary. Here we show that, in the presence of self-motion cues, a population code integrating distance naturally emerges in the hippocampus in the form of recurring sequences. These internal dynamics clamp spontaneous travel since run distance distributes into integer multiples of the span of these sequences. These sequences may thus guide navigation when external landmarks are reduced.},
	author = {Villette, Vincent and Malvache, Arnaud and Tressard, Thomas and Dupuy, Nathalie and Cossart, Rosa},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10/f7whnn},
	issn = {0896-6273},
	journal = {Neuron},
	language = {en},
	month = oct,
	note = {00085},
	number = {2},
	pages = {357--366},
	title = {Internally {Recurring} {Hippocampal} {Sequences} as a {Population} {Template} of {Spatiotemporal} {Information}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627315008417},
	urldate = {2022-01-17},
	volume = {88},
	year = {2015},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627315008417},
	bdsk-url-2 = {https://doi.org/10/f7whnn}}

@article{Vinje00,
	abstract = {Theoretical studies suggest that primary visual cortex (area V1) uses a sparse code to efficiently represent natural scenes. This issue was investigated by recording from V1 neurons in awake behaving macaques during both free viewing of natural scenes and conditions simulating natural vision. Stimulation of the nonclassical receptive field increases the selectivity and sparseness of individual V1 neurons, increases the sparseness of the population response distribution, and strongly decorrelates the responses of neuron pairs. These effects are due to both excitatory and suppressive modulation of the classical receptive field by the nonclassical receptive field and do not depend critically on the spatiotemporal structure of the stimuli. During natural vision, the classical and nonclassical receptive fields function together to form a sparse representation of the visual world. This sparse code may be computationally efficient for both early vision and higher visual processing.},
	author = {Vinje, William E. and Gallant, Jack L.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.287.5456.1273},
	journal = {Science},
	keywords = {⛔ No INSPIRE recid found},
	month = feb,
	note = {Publisher: American Association for the Advancement of Science},
	number = {5456},
	pages = {1273--1276},
	title = {Sparse {Coding} and {Decorrelation} in {Primary} {Visual} {Cortex} {During} {Natural} {Vision}},
	url = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	urldate = {2022-12-15},
	volume = {287},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	bdsk-url-2 = {https://doi.org/10.1126/science.287.5456.1273}}

@article{Von-Helmholz50,
	author = {Von Helmholz, H.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {Archiv f{\"u}r Anatomie, Physiologie und wissenschaftliche Medicin},
	language = {deu},
	pages = {176--364},
	title = {Messungen {\"u}ber den zeitlichen {Verlauf} der {Zuckung} animalischer {Muskeln} und die {Fortpflanzungsgeschwindigkeit} der {Reizung} in den {Nerven}},
	url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2346292},
	urldate = {2022-11-09},
	volume = {17},
	year = {1850},
	bdsk-url-1 = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2346292}}

@article{Wan20,
	abstract = {To define the importance of iron storage in oligodendrocyte development and function, the ferritin heavy subunit (Fth) was specifically deleted in oligodendroglial cells. Blocking Fth synthesis in Sox10 or NG2-positive oligodendrocytes during the first or the third postnatal week significantly reduces oligodendrocyte iron storage and maturation. The brain of Fth KO animals presented an important decrease in the expression of myelin proteins and a substantial reduction in the percentage of myelinated axons. This hypomyelination was accompanied by a decline in the number of myelinating oligodendrocytes and with a reduction in proliferating oligodendrocyte progenitor cells (OPCs). Importantly, deleting Fth in Sox10-positive oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte quantities. We also tested the capacity of Fth-deficient OPCs to remyelinate the adult brain in the cuprizone model of myelin injury and repair. Fth deletion in NG2-positive OPCs significantly reduces the number of mature oligodendrocytes and myelin production throughout the remyelination process. Furthermore, the corpus callosum of Fth KO animals presented a significant decrease in the percentage of remyelinated axons and a substantial reduction in the average myelin thickness. These results indicate that Fth synthesis during the first three postnatal weeks is important for an appropriate oligodendrocyte development, and suggest that Fth iron storage in adult OPCs is also essential for an effective remyelination of the mouse brain., SIGNIFICANCE STATEMENT To define the importance of iron storage in oligodendrocyte function, we have deleted the ferritin heavy chain (Fth) specifically in the oligodendrocyte lineage. Fth ablation in oligodendroglial cells throughout early postnatal development significantly reduces oligodendrocyte maturation and myelination. In contrast, deletion of Fth in oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte numbers. We have also tested the consequences of disrupting Fth iron storage in oligodendrocyte progenitor cells (OPCs) after demyelination. We have found that Fth deletion in NG2-positive OPCs significantly delays the remyelination process in the adult brain. Therefore, Fth iron storage is essential for early oligodendrocyte development as well as for OPC maturation in the demyelinated adult brain.},
	author = {Wan, Rensheng and Cheli, Veronica T. and Santiago-Gonz{\'a}lez, Diara A. and Rosenblum, Shaina L. and Wan, Qiuchen and Paez, Pablo M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1523/JNEUROSCI.1281-20.2020},
	issn = {0270-6474},
	journal = {The Journal of Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	month = sep,
	number = {40},
	pages = {7609--7624},
	pmcid = {PMC7531557},
	pmid = {32868463},
	title = {Impaired {Postnatal} {Myelination} in a {Conditional} {Knockout} {Mouse} for the {Ferritin} {Heavy} {Chain} in {Oligodendroglial} {Cells}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7531557/},
	urldate = {2022-11-14},
	volume = {40},
	year = {2020},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7531557/},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.1281-20.2020}}

@article{Wang19,
	abstract = {Neuroscience research confirms that the synaptic delays are not constant, but can be modulated. This paper proposes a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. The proposed algorithm firstly defines spike train kernels to transform discrete spike trains during the learning phase into continuous analog signals so that common mathematical operations can be performed on them, and then deduces the supervised learning rules of synaptic weights and delays by gradient descent method. The proposed algorithm is successfully applied to various spike train learning tasks, and the effects of parameters of synaptic delays are analyzed in detail. Experimental results show that the network with dynamic delays achieves higher learning accuracy and less learning epochs than the network with static delays. The delay learning algorithm is further validated on a practical example of an image classification problem. The results again show that it can achieve a good classification performance with a proper receptive field. Therefore, the synaptic delay learning is significant for practical applications and theoretical researches of spiking neural networks.},
	author = {Wang, Xiangwen and Lin, Xianghong and Dang, Xiaochao},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	title = {A {Delay} {Learning} {Algorithm} {Based} on {Spike} {Train} {Kernels} for {Spiking} {Neurons}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252},
	urldate = {2022-10-04},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252}}

@inproceedings{Wang14,
	abstract = {We present an FPGA design framework for large-scale spiking neural networks, particularly the ones with a high-density of connections or all-to-all connections. The proposed FPGA design framework is based on a reconfigurable neural layer, which is implemented using a time-multiplexing approach to achieve up to 200,000 virtual neurons with one physical neuron using only a fraction of the hardware resources in commercial-off-the-shelf FPGAs (even entry level ones). Rather than using a mathematical computational model, the physical neuron was efficiently implemented with a conductance-based model, of which the parameters were randomised between neurons to emulate the variance in biological neurons. Besides these building blocks, the proposed time-multiplexed reconfigurable neural layer has an address buffer, which will generate a fixed random weight for each connection on the fly for incoming spikes. This structure effectively reduces the usage of memory. After presenting the architecture of the proposed neural layer, we present a network with 23 proposed neural layers, each containing 64k neurons, yielding 1.5 M neurons and 92 G synapses with a total spike throughput of 1.2T spikes/s, while running in real-time on a Virtex 6 FPGA.},
	author = {Wang, Runchun and Hamilton, Tara Julia and Tapson, Jonathan and van Schaik, Andr{\'e}},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1109/ISCAS.2014.6865169},
	keywords = {Arrays, Biological neural networks, Biological system modeling, Digital signal processing, Field programmable gate arrays, Generators, Neurons, ⛔ No INSPIRE recid found},
	month = jun,
	note = {ISSN: 2158-1525},
	pages = {457--460},
	title = {An {FPGA} design framework for large-scale spiking neural networks},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/ISCAS.2014.6865169}}

@article{Wang15,
	abstract = {We present a neuromorphic implementation of multiple synaptic plasticity learning rules, which include both Spike Timing Dependent Plasticity (STDP) and Spike Timing Dependent Delay Plasticity (STDDP). We present a fully digital implementation as well as a mixed-signal implementation, both of which use a novel dynamic-assignment time-multiplexing approach and support up to 226 (64M) synaptic plasticity elements. Rather than implementing dedicated synapses for particular types of synaptic plasticity, we implemented a more generic synaptic plasticity adaptor array that is separate from the neurons in the neural network. Each adaptor performs synaptic plasticity according to the arrival times of the pre- and post-synaptic spikes assigned to it, and sends out a weighted or delayed pre-synaptic spike to the post-synaptic neuron in the neural network. This strategy provides great flexibility for building complex large-scale neural networks, as a neural network can be configured for multiple synaptic plasticity rules without changing its structure. We validate the proposed neuromorphic implementations with measurement results and illustrate that the circuits are capable of performing both STDP and STDDP. We argue that it is practical to scale the work presented here up to 236 (64G) synaptic adaptors on a current high-end FPGA platform.},
	author = {Wang, Runchun M. and Hamilton, Tara J. and Tapson, Jonathan C. and van Schaik, Andr{\'e}},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnins.2015.00180},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	title = {A neuromorphic implementation of multiple spike-timing synaptic plasticity rules for large-scale neural networks},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00180},
	urldate = {2022-10-06},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00180},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2015.00180}}

@article{Warner22,
	abstract = {We introduce a novel, probabilistic binary latent variable model to detect noisy or approximate repeats of patterns in sparse binary data. The model is based on the ''Noisy-OR model'' [5], used previously for disease and topic modelling. The model's capability is demonstrated by extracting structure in recordings from retinal neurons, but it can be widely applied to discover and model latent structure in noisy binary data. In the context of spiking neural data, the task is to ``explain'' spikes of individual neurons in terms of groups of neurons, ''Cell Assemblies'' (CAs), that often fire together, due to mutual interactions or other causes. The model infers sparse activity in a set of binary latent variables, each describing the activity of a cell assembly. When the latent variable of a cell assembly is active, it reduces the probabilities of neurons belonging to this assembly to be inactive. The conditional probability kernels of the latent components are learned from the data in an expectation maximization scheme, involving inference of latent states and parameter adjustments to the model. We thoroughly validate the model on synthesized spike trains constructed to statistically resemble recorded retinal responses to white noise stimulus and natural movie stimulus in data. We also apply our model to spiking responses recorded in retinal ganglion cells (RGCs) during stimulation with a movie and discuss the found structure.},
	author = {Warner, Christopher and Ruda, Kiersten and Sommer, Friedrich T.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	journal = {arXiv:2201.11108 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	language = {en},
	month = jan,
	note = {arXiv: 2201.11108},
	title = {A probabilistic latent variable model for detecting structure in binary data},
	url = {http://arxiv.org/abs/2201.11108},
	urldate = {2022-02-16},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2201.11108}}

@article{Wehr96,
	author = {Wehr, Michael and Laurent, Gilles},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1038/384162a0},
	journal = {Nature},
	language = {en},
	month = nov,
	number = {6605},
	pages = {162--166},
	title = {Odour encoding by temporal sequences of firing in oscillating neural assemblies},
	url = {https://doi.org/b3t32c},
	volume = {384},
	year = {1996},
	bdsk-url-1 = {https://doi.org/b3t32c},
	bdsk-url-2 = {https://doi.org/10.1038/384162a0}}

@article{Weyl16,
	author = {Weyl, Hermann},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1007/bf01475864},
	journal = {Mathematische Annalen},
	language = {de},
	month = sep,
	number = {3},
	pages = {313--352},
	title = {Ueber die {Gleichverteilung} von {Zahlen} mod. {Eins}},
	url = {https://doi.org/crprvc},
	volume = {77},
	year = {1916},
	bdsk-url-1 = {https://doi.org/crprvc},
	bdsk-url-2 = {https://doi.org/10.1007/bf01475864}}

@article{Wild93,
	abstract = {The descending, efferent projections of nucleus robustus archistriatalis were investigated in male zebra finches and greenfinches with injections of either biotinylated dextran amine or cholera toxin B-chain conjugated to horseradish peroxidase. The results show that in addition to the well-known projections to the tracheosyringeal motor nucleus and the dorsomedial nucleus of the intercollicular complex, there are other projections of comparable density to the ipsilateral nucleus ambiguus and nucleus retroambigualis. Within nucleus ambiguus, robustus axons terminate in close proximity to laryngeal motoneurons which were retrogradely labelled in the same bird by injections of cholera B-chain into the laryngeal muscles; and within nucleus retroambigualis robustus axons terminate in relation to bulbospinal neurons previously shown to project to regions of spinal cord containing motoneurons innervating abdominal expiratory muscles (J.M. Wild, Brain Res. 606:119-124, 1993). These projections of nucleus robustus thus seem well placed to coordinate syringeal, laryngeal, and expiratory muscle activity during vocalization. Other relatively sparse, but distinct, projections of nucleus robustus were found to nucleus dorsolateralis anterior thalami, pars medialis, to a narrow region between the superior olivary nucleus and the spinal lemniscus, and to the rostral ventrolateral medulla. Neurons in these last two locations were retrogradely labelled bilaterally following injections of cholera B-chain into nucleus retroambigualis of one side. Together with sparse contralateral projections of nucleus robustus to all brainstem targets receiving ipsilateral projections, potential pathways are thus identified by which the respiratory-vocal activity controlled by one side of the lower medulla can be influenced by the nucleus robustus of either side, thereby possibly bringing about bilateral coordination of respiratory-vocal output.},
	author = {Wild, J M},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1002/cne.903380207},
	issn = {0021-9967},
	journal = {The Journal of comparative neurology},
	number = {2},
	pages = {225--41},
	title = {Descending projections of the songbird nucleus robustus archistriatalis.},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/8308169},
	volume = {338},
	year = {1993},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/8308169},
	bdsk-url-2 = {https://doi.org/10.1002/cne.903380207}}

@techreport{Williams20,
	abstract = {Sparse sequences of neural spikes are posited to underlie aspects of working memory, motor production, and learning. Discovering these sequences in an unsupervised manner is a longstanding problem in statistical neuroscience. Promising recent work utilized a convolutive nonnegative matrix factorization model to tackle this challenge. However, this model requires spike times to be discretized, utilizes a sub-optimal least-squares criterion, and does not provide uncertainty estimates for model predictions or estimated parameters. We address each of these shortcomings by developing a point process model that characterizes fine-scale sequences at the level of individual spikes and represents sequence occurrences as a small number of marked events in continuous time. This ultra-sparse representation of sequence events opens new possibilities for spike train modeling. For example, we introduce learnable time warping parameters to model sequences of varying duration, which have been experimentally observed in neural circuits. We demonstrate these advantages on experimental recordings from songbird higher vocal center and rodent hippocampus.},
	author = {Williams, Alex H. and Degleris, Anthony and Wang, Yixin and Linderman, Scott W.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	month = oct,
	number = {2010.04875},
	title = {Point process models for sequence detection in high-dimensional neural spike trains},
	url = {https://arxiv.org/abs/2010.04875},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.04875}}

@article{Xue21,
	abstract = {Neurodegenerative disorders are characterized by typical neuronal degeneration and axonal loss in the central nervous system (CNS). Demyelination occurs when myelin or oligodendrocytes experience damage. Pathological changes in demyelination contribute to neurodegenerative diseases and worsen clinical symptoms during disease progression. Glaucoma is a neurodegenerative disease characterized by progressive degeneration of retinal ganglion cells (RGCs) and the optic nerve. Since it is not yet well understood, we hypothesized that demyelination could play a significant role in glaucoma. Therefore, this study started with the morphological and functional manifestations of demyelination in the CNS. Then, we discussed the main mechanisms of demyelination in terms of oxidative stress, mitochondrial damage, and immuno-inflammatory responses. Finally, we summarized the existing research on the relationship between optic nerve demyelination and glaucoma, aiming to inspire effective treatment plans for glaucoma in the future.},
	author = {Xue, Jingfei and Zhu, Yingting and Liu, Zhe and Lin, Jicheng and Li, Yangjiani and Li, Yiqing and Zhuo, Yehong},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.3389/fnagi.2021.701322},
	issn = {1663-4365},
	journal = {Frontiers in Aging Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	month = nov,
	pages = {701322},
	pmcid = {PMC8593209},
	pmid = {34795572},
	shorttitle = {Demyelination of the {Optic} {Nerve}},
	title = {Demyelination of the {Optic} {Nerve}: {An} {Underlying} {Factor} in {Glaucoma}?},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8593209/},
	urldate = {2022-11-13},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8593209/},
	bdsk-url-2 = {https://doi.org/10.3389/fnagi.2021.701322}}

@techreport{Yin22,
	abstract = {Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. Recently, SNNs with backpropagation through time (BPTT) have achieved a higher accuracy result on image recognition tasks compared to other SNN training algorithms. Despite the success on the algorithm perspective, prior works neglect the evaluation of the hardware energy overheads of BPTT, due to the lack of a hardware evaluation platform for SNN training algorithm design. Moreover, although SNNs have been long seen as an energy-efficient counterpart of ANNs, a quantitative comparison between the training cost of SNNs and ANNs is missing. To address the above-mentioned issues, in this work, we introduce SATA (Sparsity-Aware Training Accelerator), a BPTT-based training accelerator for SNNs. The proposed SATA provides a simple and re-configurable accelerator architecture for the general-purpose hardware evaluation platform, which makes it easier to analyze the training energy for SNN training algorithms. Based on SATA, we show quantitative analyses on the energy efficiency of SNN training and make a comparison between the training cost of SNNs and ANNs. The results show that SNNs consume \$1.27{\textbackslash}times\$ more total energy with considering sparsity (spikes, gradient of firing function, and gradient of membrane potential) when compared to ANNs. We find that such high training energy cost is from time-repetitive convolution operations and data movements during backpropagation. Moreover, to guide the future SNN training algorithm design, we provide several observations on energy efficiency with respect to different SNN-specific training parameters.},
	author = {Yin, Ruokai and Moitra, Abhishek and Bhattacharjee, Abhiroop and Kim, Youngeun and Panda, Priyadarshini},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	institution = {arXiv},
	number = {2204.05422},
	title = {{SATA}: {Sparsity}-{Aware} {Training} {Accelerator} for {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2204.05422},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2204.05422}}

@article{Young38,
	abstract = {1. Stimulation of single giant nerve fibres in the stellar nerves of the squid (Loligo pealii) shows them to be motor axons which produce contraction of the circular fibres of the mantle muscles.2. When a stellar nerve is stimulated with condenser discharges a maximal response is obtained at threshold voltage. No increase of response is obtained by further increase in the strength of stimulation except for an occasional slight increase at about ten times threshold voltage probably due to repetitive firing. It therefore appears that the stimulus produces a single impulse in the giant fibre, and that this is capable of exciting contraction in all the muscle fibres which it reaches. This confirms the conclusion reached on histological grounds that in spite of their syncytial nature each of the giant nerve fibres is a single functional unit.3. Since there are about ten giant fibres on each side the mantle is divided into 20 neuromotor units, each nerve fibre innervating an enormous number of muscle fibres. The existence of these units can also very readily be demonstrated by the fact that threshold electrical stimulation at any point within the territory innervated by each single giant fibre sets up a contraction of the muscle fibres of all parts of the territory with which the stimulated area is in connexion through the nerve.4. Stimulation of the smaller fibres in a stellar nerve after destruction of the giant fibre also causes contraction of the circular muscles of the mantle. The amount of this contraction increases progressively with increased voltage, presumably on account of the stimulation of more and more nerve fibres. The maximum tension developed in this way is always very much less than that produced by stimulation of the giant fibres.5. The mantle is therefore provided with a double mechanism of expiratory contraction, maximal contractions being produced by single impulses in the giant fibres and graded contractions by those in the smaller fibres of the nerve. Presumably the former contractions are those involved in rapid movement, the latter in respiration.6. There are also radial muscles, running through the thickness of the mantle, whose contractions effect the inspiration by making the cavity larger.},
	author = {Young, J. Z.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1242/jeb.15.2.170},
	issn = {0022-0949},
	journal = {Journal of Experimental Biology},
	keywords = {⛔ No INSPIRE recid found},
	month = apr,
	number = {2},
	pages = {170--185},
	title = {The {Functioning} of the {Giant} {Nerve} {Fibres} of the {Squid}},
	url = {https://doi.org/10.1242/jeb.15.2.170},
	urldate = {2022-11-13},
	volume = {15},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1242/jeb.15.2.170}}

@misc{Yu22,
	abstract = {Spiking Neural Networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing the event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a spatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Concretely, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance vias varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.},
	author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	month = oct,
	note = {arXiv:2210.05241 [cs, q-bio, stat]},
	publisher = {arXiv},
	shorttitle = {{STSC}-{SNN}},
	title = {{STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with {Temporal} {Convolution} and {Attention} for {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.05241},
	urldate = {2022-10-25},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2210.05241}}

@article{Yu96,
	abstract = {Songs of birds comprise hierarchical sets of vocal gestures. In zebra finches, songs include notes and syllables (groups of notes) delivered in fixed sequences. During singing, premotor neurons in the forebrain nucleus HVc exhibited reliable changes in activity rates whose patterns were uniquely associated with syllable identity. Neurons in the forebrain nucleus robustus archistriatalis, which receives input from the HVc, exhibited precisely timed and structured bursts of activity that were uniquely associated with note identity. Hence, units of vocal behavior are represented hierarchically in the avian forebrain. The representation of temporal sequences at each level of the hierarchy may be established by means of a decoding process involving interactions of higher level input with intrinsic local circuitry. Behavior is apparently represented by precise temporal patterning of spike trains at lower levels of the hierarchy.},
	author = {Yu, A C and Margoliash, D},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1126/science.273.5283.1871},
	issn = {0036-8075},
	journal = {Science (New York, N.Y.)},
	month = sep,
	number = {5283},
	pages = {1871--5},
	title = {Temporal hierarchical control of singing in birds.},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/8791594},
	volume = {273},
	year = {1996},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/8791594},
	bdsk-url-2 = {https://doi.org/10.1126/science.273.5283.1871}}

@article{Zenke21,
	abstract = {Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.},
	author = {Zenke, Friedemann and Vogels, Tim P.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1162/neco_a_01367},
	issn = {0899-7667},
	journal = {Neural Computation},
	month = mar,
	number = {4},
	pages = {899--925},
	title = {The {Remarkable} {Robustness} of {Surrogate} {Gradient} {Learning} for {Instilling} {Complex} {Function} in {Spiking} {Neural} {Networks}},
	urldate = {2021-12-02},
	volume = {33},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_01367}}

@article{Zenke21a,
	abstract = {Recent research resolves the challenging problem of building biophysically plausible spiking neural models that are also capable of complex information processing. This advance creates new opportunities in neuroscience and neuromorphic engineering, which we discussed at an online focus meeting.},
	author = {Zenke, Friedemann and Boht{\'e}, Sander M. and Clopath, Claudia and Com{\c s}a, Iulia M. and G{\"o}ltz, Julian and Maass, Wolfgang and Masquelier, Timoth{\'e}e and Naud, Richard and Neftci, Emre O. and Petrovici, Mihai A. and Scherr, Franz and Goodman, Dan F. M.},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neuron.2021.01.009},
	issn = {0896-6273},
	journal = {Neuron},
	language = {en},
	month = feb,
	number = {4},
	pages = {571--575},
	title = {Visualizing a joint future of neuroscience and neuromorphic engineering},
	url = {https://www.sciencedirect.com/science/article/pii/S089662732100009X},
	urldate = {2021-02-24},
	volume = {109},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S089662732100009X},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2021.01.009}}

@article{Zhang20,
	author = {Zhang, Malu and Wu, Jibin and Belatreche, Ammar and Pan, Zihan and Xie, Xiurui and Chua, Yansong and Li, Guoqi and Qu, Hong and Li, Haizhou},
	date-added = {2023-04-07 16:39:21 +0200},
	date-modified = {2023-04-07 16:39:21 +0200},
	doi = {10.1016/j.neucom.2020.03.079},
	journal = {Neurocomputing},
	language = {en},
	month = oct,
	pages = {103--118},
	title = {Supervised learning in spiking neural networks with synaptic delay-weight plasticity},
	url = {https://doi.org/ghsm45},
	volume = {409},
	year = {2020},
	bdsk-url-1 = {https://doi.org/ghsm45},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2020.03.079}}

@article{Mackevicius2019,
	abstract = {Identifying low-dimensional features that describe large-scale neural recordings is a major challenge in neuroscience. Repeated temporal patterns (sequences) are thought to be a salient feature of neural dynamics, but are not succinctly captured by traditional dimensionality reduction techniques. Here, we describe a software toolbox---called seqNMF---with new methods for extracting informative, non-redundant, sequences from high-dimensional neural data, testing the significance of these extracted patterns, and assessing the prevalence of sequential structure in data. We test these methods on simulated data under multiple noise conditions, and on several real neural and behavioral data sets. In hippocampal data, seqNMF identifies neural sequences that match those calculated manually by reference to behavioral events. In songbird data, seqNMF discovers neural sequences in untutored birds that lack stereotyped songs. Thus, by identifying temporal structure directly from neural data, seqNMF enables dissection of complex neural circuits without relying on temporal references from stimuli or behavioral outputs.},
	author = {Mackevicius, Emily L and Bahle, Andrew H and Williams, Alex H and Gu, Shijie and Denisenko, Natalia I and Goldman, Mark S and Fee, Michale S},
	date = {2019-02-05},
	doi = {10.7554/eLife.38471},
	editor = {Colgin, Laura and Behrens, Timothy E},
	issn = {2050-084X},
	journaltitle = {eLife},
	pages = {e38471},
	publisher = {{eLife Sciences Publications, Ltd}},
	title = {Unsupervised Discovery of Temporal Sequences in High-Dimensional Datasets, with Applications to Neuroscience},
	url = {https://doi.org/10.7554/eLife.38471},
	urldate = {2023-01-24},
	volume = {8},
	bdsk-url-1 = {https://doi.org/10.7554/eLife.38471}}

@unpublished{Williams2020,
	abstract = {Sparse sequences of neural spikes are posited to underlie aspects of working memory, motor production, and learning. Discovering these sequences in an unsupervised manner is a longstanding problem in statistical neuroscience. Promising recent work utilized a convolutive nonnegative matrix factorization model to tackle this challenge. However, this model requires spike times to be discretized, utilizes a sub-optimal least-squares criterion, and does not provide uncertainty estimates for model predictions or estimated parameters. We address each of these shortcomings by developing a point process model that characterizes fine-scale sequences at the level of individual spikes and represents sequence occurrences as a small number of marked events in continuous time. This ultra-sparse representation of sequence events opens new possibilities for spike train modeling. For example, we introduce learnable time warping parameters to model sequences of varying duration, which have been experimentally observed in neural circuits. We demonstrate these advantages on experimental recordings from songbird higher vocal center and rodent hippocampus.},
	author = {Williams, Alex H. and Degleris, Anthony and Wang, Yixin and Linderman, Scott W.},
	date = {2020-10-09},
	eprint = {2010.04875},
	eprintclass = {cs, q-bio, stat},
	eprinttype = {arxiv},
	ids = {Williams2020a},
	title = {Point Process Models for Sequence Detection in High-Dimensional Neural Spike Trains},
	url = {http://arxiv.org/abs/2010.04875},
	urldate = {2022-02-04},
	bdsk-url-1 = {http://arxiv.org/abs/2010.04875}}

@article{sanz_leon_virtual_2013,
	abstract = {We present The Virtual Brain (TVB), a neuroinformatics platform for full brain network simulations using biologically realistic connectivity. This simulation environment enables the model-based inference of neurophysiological mechanisms across different brain scales that underlie the generation of macroscopic neuroimaging signals including functional MRI (fMRI), EEG and MEG. Researchers from different backgrounds can benefit from an integrative software platform including a supporting framework for data management (generation, organization, storage, integration and sharing) and a simulation core written in Python. TVB allows the reproduction and evaluation of personalized configurations of the brain by using individual subject data. This personalization facilitates an exploration of the consequences of pathological changes in the system, permitting to investigate potential ways to counteract such unfavorable processes. The architecture of TVB supports interaction with MATLAB packages, for example, the well known Brain Connectivity Toolbox. TVB can be used in a client-server configuration, such that it can be remotely accessed through the Internet thanks to its web-based HTML5, JS, and WebGL graphical user interface. TVB is also accessible as a standalone cross-platform Python library and application, and users can interact with the scientific core through the scripting interface IDLE, enabling easy modeling, development and debugging of the scientific kernel. This second interface makes TVB extensible by combining it with other libraries and modules developed by the Python scientific community. In this article, we describe the theoretical background and foundations that led to the development of TVB, the architecture and features of its major software components as well as potential neuroscience applications.},
	author = {Sanz Leon, Paula and Knock, Stuart and Woodman, M. and Domide, Lia and Mersmann, Jochen and McIntosh, Anthony and Jirsa, Viktor},
	doi = {10.3389/fninf.2013.00010},
	issn = {1662-5196},
	journal = {Frontiers in Neuroinformatics},
	keywords = {⛔ No INSPIRE recid found},
	shorttitle = {The {Virtual} {Brain}},
	title = {The {Virtual} {Brain}: a simulator of primate brain network dynamics},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2013.00010},
	urldate = {2022-09-28},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fninf.2013.00010},
	bdsk-url-2 = {https://doi.org/10.3389/fninf.2013.00010}}

@article{lecun_gradient-based_1998,
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	doi = {10.1109/5.726791},
	issn = {00189219},
	journal = {Proceedings of the IEEE},
	keywords = {⛔ No INSPIRE recid found},
	month = nov,
	note = {tex.bdsk-url-2: https://doi.org/10.1109/5.726791 tex.date-added: 2022-05-05 19:08:15 +0200 tex.date-modified: 2022-05-05 19:08:15 +0200},
	number = {11},
	pages = {2278--2324},
	title = {Gradient-based learning applied to document recognition},
	url = {http://ieeexplore.ieee.org/document/726791/},
	urldate = {2021-05-18},
	volume = {86},
	year = {1998},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/726791/},
	bdsk-url-2 = {https://doi.org/10.1109/5.726791}}

@inproceedings{howard_searching_2019,
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	booktitle = {Proceedings of the {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No INSPIRE recid found},
	month = oct,
	title = {Searching for {MobileNetV3}},
	year = {2019}}

@article{simonyan_very_2015,
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	author = {Simonyan, Karen and Zisserman, Andrew},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ⛔ No DOI found},
	month = apr,
	note = {158 citations (INSPIRE 2022/10/1) 158 citations w/o self (INSPIRE 2022/10/1) arXiv:1409.1556 [cs.CV]},
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	urldate = {2021-05-24},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1409.1556}}

@techreport{grimaldi_learning_2022,
	abstract = {The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. Ho...},
	author = {Grimaldi, Antoine and Perrinet, Laurent U},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	title = {Learning heterogeneous delays in a layer of spiking neurons for fast motion detection},
	url = {https://www.researchsquare.com/article/rs-2120999/v1},
	urldate = {2022-11-06},
	year = {2022},
	bdsk-url-1 = {https://www.researchsquare.com/article/rs-2120999/v1}}

@misc{yu_stsc-snn_2022,
	abstract = {Spiking Neural Networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing the event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a spatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Concretely, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance vias varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.},
	author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, ⛔ No INSPIRE recid found},
	month = oct,
	note = {arXiv:2210.05241 [cs, q-bio, stat]},
	publisher = {arXiv},
	shorttitle = {{STSC}-{SNN}},
	title = {{STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with {Temporal} {Convolution} and {Attention} for {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.05241},
	urldate = {2022-10-25},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2210.05241}}

@article{vinje_sparse_2000,
	abstract = {Theoretical studies suggest that primary visual cortex (area V1) uses a sparse code to efficiently represent natural scenes. This issue was investigated by recording from V1 neurons in awake behaving macaques during both free viewing of natural scenes and conditions simulating natural vision. Stimulation of the nonclassical receptive field increases the selectivity and sparseness of individual V1 neurons, increases the sparseness of the population response distribution, and strongly decorrelates the responses of neuron pairs. These effects are due to both excitatory and suppressive modulation of the classical receptive field by the nonclassical receptive field and do not depend critically on the spatiotemporal structure of the stimuli. During natural vision, the classical and nonclassical receptive fields function together to form a sparse representation of the visual world. This sparse code may be computationally efficient for both early vision and higher visual processing.},
	author = {Vinje, William E. and Gallant, Jack L.},
	doi = {10.1126/science.287.5456.1273},
	issn = {0036-8075, 1095-9203},
	journal = {Science},
	keywords = {\#nosource, bicv-sparse, motion-clouds, motion\_clouds, natural-scenes, natural\_scenes, sanz12jnp, vacher14, ⛔ No INSPIRE recid found},
	language = {en},
	month = feb,
	note = {tex.ids= Vinje2000},
	number = {5456},
	pages = {1273--1276},
	pmid = {10678835},
	title = {Sparse {Coding} and {Decorrelation} in {Primary} {Visual} {Cortex} {During} {Natural} {Vision}},
	url = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	urldate = {2022-10-03},
	volume = {287},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	bdsk-url-2 = {https://doi.org/10.1126/science.287.5456.1273}}

@article{goodman_spike-timing-based_2010,
	abstract = {Spike timing is precise in the auditory system and it has been argued that it conveys information about auditory stimuli, in particular about the location of a sound source. However, beyond simple time differences, the way in which neurons might extract this information is unclear and the potential computational advantages are unknown. The computational difficulty of this task for an animal is to locate the source of an unexpected sound from two monaural signals that are highly dependent on the unknown source signal. In neuron models consisting of spectro-temporal filtering and spiking nonlinearity, we found that the binaural structure induced by spatialized sounds is mapped to synchrony patterns that depend on source location rather than on source signal. Location-specific synchrony patterns would then result in the activation of location-specific assemblies of postsynaptic neurons. We designed a spiking neuron model which exploited this principle to locate a variety of sound sources in a virtual acoustic environment using measured human head-related transfer functions. The model was able to accurately estimate the location of previously unknown sounds in both azimuth and elevation (including front/back discrimination) in a known acoustic environment. We found that multiple representations of different acoustic environments could coexist as sets of overlapping neural assemblies which could be associated with spatial locations by Hebbian learning. The model demonstrates the computational relevance of relative spike timing to extract spatial information about sources independently of the source signal.},
	author = {Goodman, Dan F. M. and Brette, Romain},
	doi = {10.1371/journal.pcbi.1000993},
	issn = {1553-7358},
	journal = {PLoS Comput Biol},
	keywords = {\#nosource, spike, spikes, synchrony, ⛔ No INSPIRE recid found},
	month = nov,
	number = {11},
	pmid = {21085681},
	title = {Spike-timing-based computation in sound localization.},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2978676/},
	volume = {6},
	year = {2010},
	bdsk-url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2978676/},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1000993}}

@article{guise_bayesian_2014,
	author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
	doi = {10/f6chbq},
	journal = {Neural Computation},
	keywords = {⛔ No INSPIRE recid found},
	number = {9},
	pages = {2052--2073},
	title = {A {Bayesian} model of polychronicity},
	volume = {26},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10/f6chbq}}

@article{boutin_effect_2020,
	abstract = {Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.},
	author = {Boutin, Victor and Franciosini, Angelo and Ruffier, Franck and Perrinet, Laurent U},
	copyright = {All rights reserved},
	doi = {10/fnqm},
	journal = {Neural Computation},
	keywords = {deep-learning, sparse coding, ⛔ No INSPIRE recid found},
	month = feb,
	number = {11},
	pages = {2279--2309},
	title = {Effect of top-down connections in {Hierarchical} {Sparse} {Coding}},
	url = {https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/},
	volume = {32},
	year = {2020},
	bdsk-url-1 = {https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/},
	bdsk-url-2 = {https://doi.org/10/fnqm}}

@incollection{paugam-moisy_computing_2012,
	author = {Paugam-Moisy, H{\'e}l{\`e}ne and Bohte, Sander M.},
	booktitle = {Handbook of natural computing},
	keywords = {⛔ No INSPIRE recid found},
	month = sep,
	publisher = {Springer-Verlag},
	title = {Computing with spiking neuron networks},
	year = {2012}}

@article{zenke_remarkable_2021,
	abstract = {Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.},
	author = {Zenke, Friedemann and Vogels, Tim P.},
	doi = {10.1162/neco_a_01367},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {⛔ No INSPIRE recid found},
	month = mar,
	number = {4},
	pages = {899--925},
	title = {The {Remarkable} {Robustness} of {Surrogate} {Gradient} {Learning} for {Instilling} {Complex} {Function} in {Spiking} {Neural} {Networks}},
	urldate = {2021-12-02},
	volume = {33},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_01367}}

@article{olshausen_emergence_1996,
	abstract = {The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
	author = {Olshausen, Bruno A. and Field, David J.},
	doi = {10.1038/381607a0},
	issn = {0028-0836},
	journal = {Nature},
	keywords = {\#nosource, Algorithms, Learning, Models, Neurological, Neurons, Neurons: physiology, Ocular, Ocular: physiology, Vision, Visual Cortex, Visual Cortex: cytology, Visual Cortex: physiology, anr-trax, bicv-sparse, perrinetadamsfriston14, sparse\_coding, sparse\_hebbian\_learning, sparse\_spike\_coding, ⛔ No INSPIRE recid found},
	number = {6583},
	pages = {607--609},
	title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images.},
	volume = {381},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1038/381607a0}}

@article{perrinet_motion-based_2012,
	abstract = {In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to physio-logy and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independent of their texture. Second, we observe that incoherent features are explained away, while coherent information diffuses progressively to the global scale. Most previous models included ad hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features as necessary conditions to solve the aperture problem. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This solution may give insights into the role of prediction underlying a large class of sensory computations.},
	author = {Perrinet, Laurent U. and Masson, G.S. Guillaume S.},
	copyright = {All rights reserved},
	doi = {10.1162/neco_a_00332},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {\#nosource, Bayesian model, aperture, aperture problem, aperture-problem, association field, coding, emergence, khoei12jpp, khoei13jpp, motion detection, motion prediction, perrinet12pred, predictive, predictive coding, predictive-coding, probabilistic, probabilistic representation, problem, representation, thesis, toupate-inpress, ⛔ No INSPIRE recid found},
	month = aug,
	number = {10},
	pages = {2726--2750},
	title = {Motion-{Based} {Prediction} {Is} {Sufficient} to {Solve} the {Aperture} {Problem}},
	volume = {24},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_00332}}

@article{vacher_bayesian_2018,
	abstract = {A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a gen-erative model intended to probe visual motion perception. It is first derived in a set of axiomatic steps constrained by biological plausibility. We then extend previous con-tributions by detailing three equivalent formulations of the Gaussian dynamic texture model. First, the composite dynamic textures are constructed by the random aggrega-tion of warped patterns, which can be viewed as 3D Gaussian fields. Second, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly, texture synthesis using time-discretized auto-regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are finally essential for the construction of a Bayesian inference framework. We use the model to probe speed perception in humans psychophysically using zoom-like changes in stimulus spatial frequency content. The likelihood is contained within the genera-tive model and we chose a slow speed prior consistent with previous literature. We then validated the fitting process of the model using synthesized data. The human data replicates previous findings that relative perceived speed is positively biased by spatial frequency increments. The effect cannot be fully accounted for by previous models, but the current prior acting on the spatio-temporal likelihoods has proved necessary in accounting for the perceptual bias.},
	author = {Vacher, Jonathan and Meso, Andrew Isaac and Perrinet, Laurent U and Peyr{\'e}, Gabriel},
	copyright = {All rights reserved},
	doi = {10.1162/neco_a_01142},
	journal = {Neural Computation},
	keywords = {\#nosource, Bayesian model, Psychophysics, motion detection, motion-clouds, ⛔ No INSPIRE recid found},
	month = nov,
	title = {Bayesian modeling of motion perception using dynamical stochastic textures},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1162/neco_a_01142}}

@article{dardelet_event-by-event_2021,
	abstract = {Contour velocity estimation and tracking from a fully event-based perspective.},
	author = {Dardelet, Laurent and Benosman, Ryad and Ieng, Sio-Hoi},
	doi = {10.36227/techrxiv.17013824.v1},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = nov,
	title = {An {Event}-by-{Event} {Feature} {Detection} and {Tracking} {Invariant} to {Motion} {Direction} and {Velocity}},
	urldate = {2022-09-28},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.36227/techrxiv.17013824.v1}}

@article{grimaldi_robust_2022,
	abstract = {We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization.},
	author = {Grimaldi, Antoine and Boutin, Victor and Ieng, Sio-Hoi and Benosman, Ryad and Perrinet, Laurent U},
	doi = {10.36227/techrxiv.18003077.v1},
	journal = {TechRxiv preprint},
	keywords = {efficient coding, event-based vision, homeostasis, neuromorphic hardware, online classification, ⛔ No INSPIRE recid found},
	month = jan,
	title = {A robust event-driven approach to always-on object recognition},
	urldate = {2022-01-13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.36227/techrxiv.18003077.v1}}

@article{maro_event-based_2020,
	abstract = {In this paper, we introduce a framework for dynamic gesture recognition with background suppression operating on the output of a moving event-based camera. The system is developed to operate in real-time using only the computational capabilities of a mobile phone. It introduces a new development around the concept of time-surfaces. It also presents a novel event-based methodology to dynamically remove backgrounds that uses the high temporal resolution properties of event-based cameras. To our knowledge, this is the first Android event-based framework for vision-based recognition of dynamic gestures running on a smartphone without off-board processing. We assess the performances by considering several scenarios in both indoors and outdoors, for static and dynamic conditions, in uncontrolled lighting conditions. We also introduce a new event-based dataset for gesture recognition with static and dynamic backgrounds (made publicly available). The set of gestures has been selected following a clinical trial to allow human-machine interaction for the visually impaired and older adults. We finally report comparisons with prior work that addressed event-based gesture recognition reporting comparable results, without the use of advanced classification techniques nor power greedy hardware.},
	author = {Maro, Jean-Matthieu and Ieng, Sio-Hoi and Benosman, Ryad},
	doi = {10.3389/fnins.2020.00275},
	issn = {1662-453X},
	journal = {Frontiers in neuroscience},
	keywords = {Background Suppression, Dynamic Gesture Recognition, Dynamic Vision Sensor (Dvs), Event-based, Gesture Recognition, Mobile Device, Neuromorphic, Smartphone, ⛔ No INSPIRE recid found},
	language = {eng},
	month = jan,
	pages = {275},
	title = {Event-{Based} {Gesture} {Recognition} {With} {Dynamic} {Background} {Suppression} {Using} {Smartphone} {Computational} {Capabilities}},
	url = {https://europepmc.org/articles/PMC7160298},
	urldate = {2022-09-28},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://europepmc.org/articles/PMC7160298},
	bdsk-url-2 = {https://doi.org/10.3389/fnins.2020.00275}}

@article{deangelis_functional_1999,
	author = {DeAngelis, Gregory C and Ghose, Geoffrey M and Ohzawa, Izumi and Freeman, Ralph D},
	doi = {10.1523/JNEUROSCI.19-10-04046.1999},
	journal = {Journal of Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	number = {10},
	pages = {4046--4064},
	title = {Functional micro-organization of primary visual cortex: receptive field analysis of nearby neurons},
	volume = {19},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1523/JNEUROSCI.19-10-04046.1999}}

@article{kremkow_push-pull_2016,
	abstract = {Neurons in the primary visual cortex are known for responding vigorously but with high variability to classical stimuli such as drifting bars or gratings. By contrast, natural scenes are encoded more efficiently by sparse and temporal precise spiking responses. We used a conductance-based model of the visual system in higher mammals to investigate how two specific features of the thalamo-cortical pathway, namely push-pull receptive field organization and synaptic depression, can contribute to this contextual reshaping of V1 responses. By comparing cortical dynamics evoked respectively by natural vs. artificial stimuli in a comprehensive parametric space analysis, we demonstrate that the reliability and sparseness of the spiking responses during natural vision is not a mere consequence of the increased bandwidth in the sensory input spectrum. Rather, it results from the combined impacts of synaptic depression and push-pull inhibition, the later acting for natural scenes as a form of ``effective'' feed-forward inhibition as demonstrated in other sensory systems. Thus, the combination of feedforward-like inhibition with fast thalamo-cortical synaptic depression by simple cells receiving a direct structured input from thalamus composes a generic computational mechanism for generating a sparse and reliable encoding of natural sensory events.},
	author = {Kremkow, Jens and Perrinet, Laurent U. and Monier, Cyril and Alonso, Jose-Manuel and Aertsen, Ad and Fr{\'e}gnac, Yves and Masson, Guillaume S.},
	copyright = {All rights reserved},
	doi = {10.3389/fncir.2016.00037},
	issn = {1662-5110},
	journal = {Frontiers in Neural Circuits},
	keywords = {\#nosource, Excitation/inhibition, RetinaClouds, Sensory coding, Visual Cortex, area V1, area-v1, natural visual stimuli, push-pull receptive field, statistics of natural images, ⛔ No INSPIRE recid found},
	language = {English},
	month = may,
	title = {Push-{Pull} {Receptive} {Field} {Organization} and {Synaptic} {Depression}: {Mechanisms} for {Reliably} {Encoding} {Naturalistic} {Stimuli} in {V1}},
	url = {https://doi.org/10.3389/fncir.2016.00037},
	volume = {10},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.3389/fncir.2016.00037}}

@article{priebe_tuning_2006,
	abstract = {We recorded the responses of direction-selective simple and complex cells in the primary visual cortex (V1) of anesthetized, paralyzed macaque monkeys. When studied with sine-wave gratings, almost all simple cells in V1 had responses that were separable for spatial and temporal frequency: the preferred temporal frequency did not change and preferred speed decreased as a function of the spatial frequency of the grating. As in previous recordings from the middle temporal visual area (MT), approximately one-quarter of V1 complex cells had separable responses to spatial and temporal frequency, and one-quarter were ``speed tuned'' in the sense that preferred speed did not change as a function of spatial frequency. Half fell between these two extremes. Reducing the contrast of the gratings caused the population of V1 complex cells to become more separable in their tuning for spatial and temporal frequency. Contrast dependence is explained by the contrast gain of the neurons, which was relatively higher for gratings that were either both of high or both of low temporal and spatial frequency. For stimuli that comprised two spatially superimposed sine-wave gratings, the preferred speeds and tuning bandwidths of V1 neurons could be predicted from the sum of the responses to the component gratings presented alone, unlike neurons in MT that showed nonlinear interactions. We conclude that spatiotemporal modulation of contrast gain creates speed tuning from separable inputs in V1 complex cells. Speed tuning in MT could be primarily inherited from V1, but processing that occurs after V1 and possibly within MT computes selective combinations of speed-tuned signals of special relevance for downstream perceptual and motor mechanisms.},
	author = {Priebe, Nicholas J. and Lisberger, Stephen G. and Movshon, J. Anthony},
	doi = {10.1523/JNEUROSCI.3936-05.2006},
	issn = {0270-6474},
	journal = {The Journal of Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	month = mar,
	number = {11},
	pages = {2941--2950},
	title = {Tuning for {Spatiotemporal} {Frequency} and {Speed} in {Directionally} {Selective} {Neurons} of {Macaque} {Striate} {Cortex}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2532672/},
	urldate = {2022-09-29},
	volume = {26},
	year = {2006},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2532672/},
	bdsk-url-2 = {https://doi.org/10.1523/JNEUROSCI.3936-05.2006}}

@article{simoncini_more_2012,
	abstract = {Moving objects generate motion information at different scales, which are processed in the visual system with a bank of spatiotemporal frequency channels. It is not known how the brain pools this information to reconstruct object speed and whether this pooling is generic or adaptive; that is, dependent on the behavioral task. We used rich textured motion stimuli of varying bandwidths to decipher how the human visual motion system computes object speed in different behavioral contexts. We found that, although a simple visuomotor behavior such as short-latency ocular following responses takes advantage of the full distribution of motion signals, perceptual speed discrimination is impaired for stimuli with large bandwidths. Such opposite dependencies can be explained by an adaptive gain control mechanism in which the divisive normalization pool is adjusted to meet the different constraints of perception and action.},
	author = {Simoncini, Claudio and Perrinet, Laurent U. and Montagnini, Anna and Mamassian, Pascal and Masson, Guillaume S. G.S. Guillaume S.},
	copyright = {All rights reserved},
	doi = {10.1038/nn.3229},
	issn = {1546-1726},
	journal = {Nature Neuroscience},
	keywords = {\#nosource, activeₑyeₘovements, bicv-sparse, eye-movements, eyeₘovements, free energy, freemove, gain$_{\textrm{c}}$ontrol, perrinetadamsfriston14, pursuit, sanz12jnp, smooth-pursuit-eye-movements, spem, vacher14, ⛔ No INSPIRE recid found},
	month = sep,
	number = {11},
	pages = {1596--1603},
	title = {More is not always better: {Adaptive} gain control explains dissociation between perception and action.},
	volume = {15},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1038/nn.3229}}

@inproceedings{mansour_pour_speed_2018,
	abstract = {It is still not fully understood how visual system integrates motion energy across different spatial and temporal frequencies to build a coherent percept of the global motion under the complex, noisy naturalistic conditions. We addressed this question by manipulating local speed variability distribution (i. e. speed bandwidth) using a well-controlled class of broadband random-texture stimuli called Motion Clouds (MCs) with continuous naturalistic spatiotemporal frequency spectra (Sanz-Leon et al., 2012, ; Simoncini et al., 2012). In a first 2AFC experiment on speed discrimination, participants had to compare the speed of a broad speed bandwidth MC (range: 0.05-8 $^{\textrm{∘}}$/s) moving at 1 of 5 possible mean speeds (ranging from 5 to 13  $^{\textrm{∘}}$/s) to that of another MC with a small speed bandwidth (SD: 0.05  $^{\textrm{∘}}$/s), always moving at a mean speed of 10 $^{\textrm{∘}}$/s . We found that MCs with larger speed bandwidth (between 0.05-0.5 $^{\textrm{∘}}$/s) were perceived moving faster. Within this range, speed uncertainty results in over-estimating stimulus velocity. However, beyond a critical bandwidth (SD: 0.5  $^{\textrm{∘}}$/s), perception of a coherent speed was lost. In a second 2AFC experiment on direction discrimination, participants had to estimate the motion direction of moving MCs with different speed bandwidths. We found that for large band MCs participant could no longer discriminate motion direction. These results suggest that when increasing speed bandwidth from small to large range, the observer experiences different perceptual regimes. We then decided to run a Maximum Likelihood Difference Scaling (Knoblauch \& Maloney, 2008) experiment with our speed bandwidth stimuli to investigate these different possible perceptual regimes. We identified three regimes within this space that correspond to motion coherency, motion transparency and motion incoherency. These results allow to further characterize the shape of the interactions kernel observed between different speed tuned channels and different spatiotemporal scales (Gekas et al ., 2017) that underlies global velocity estimation.},
	author = {Mansour Pour, Kiana and Gekas, Nikos and Mamassian, Pascal and Perrinet, Laurent U and Montagnini, Anna and Masson, Guillaume S},
	booktitle = {Journal of {Vision}, {Vol}.18, 345, proceedings of {VSS}},
	copyright = {All rights reserved},
	doi = {10.1167/18.10.345},
	keywords = {\#nosource, motion detection, ⛔ No INSPIRE recid found},
	title = {Speed uncertainty and motion perception with naturalistic random textures},
	url = {https://laurentperrinet.github.io/publication/mansour-18-vss},
	year = {2018},
	bdsk-url-1 = {https://laurentperrinet.github.io/publication/mansour-18-vss},
	bdsk-url-2 = {https://doi.org/10.1167/18.10.345}}

@article{baudot_animation_2013,
	abstract = {Synaptic noise is thought to be a limiting factor for computational efficiency in the brain. In visual cortex (V1), ongoing activity is present in vivo, and spiking responses to simple stimuli are highly unreliable across trials. Stimulus statistics used to plot receptive fields, however, are quite different from those experienced during natural visuomotor exploration. We recorded V1 neurons intracellularly in the anaesthetized and paralyzed cat and compared their spiking and synaptic responses to full field natural images animated by simulated eye-movements to those evoked by simpler (grating) or higher dimensionality statistics (dense noise). In most cells, natural scene animation was the only condition where high temporal precision (in the 10--20 ms range) was maintained during sparse and reliable activity. At the subthreshold level, irregular but highly reproducible membrane potential dynamics were observed, even during long (several 100 ms) ``spike-less'' periods. We showed that both the spatial structure of natural scenes and the temporal dynamics of eye-movements increase the signal-to-noise ratio by a non-linear amplification of the signal combined with a reduction of the subthreshold contextual noise. These data support the view that the sparsening and the time precision of the neural code in V1 may depend primarily on three factors: (1) broadband input spectrum: the bandwidth must be rich enough for recruiting optimally the diversity of spatial and time constants during recurrent processing; (2) tight temporal interplay of excitation and inhibition: conductance measurements demonstrate that natural scene statistics narrow selectively the duration of the spiking opportunity window during which the balance between excitation and inhibition changes transiently and reversibly; (3) signal energy in the lower frequency band: a minimal level of power is needed below 10 Hz to reach consistently the spiking threshold, a situation rarely reached with visual dense noise.},
	author = {Baudot, Pierre and Levy, Manuel and Marre, Olivier and Monier, Cyril and Pananceau, Marc and Fr{\'e}gnac, Yves},
	doi = {10.3389/fncir.2013.00206},
	issn = {1662-5110},
	journal = {Frontiers in Neural Circuits},
	keywords = {⛔ No INSPIRE recid found},
	title = {Animation of natural scene by virtual eye-movements evokes high precision and low noise in {V1} neurons},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2013.00206},
	urldate = {2022-09-29},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fncir.2013.00206},
	bdsk-url-2 = {https://doi.org/10.3389/fncir.2013.00206}}

@article{masquelier_unsupervised_2007,
	abstract = {Spike timing dependent plasticity (STDP) is a learning rule that modifies synaptic strength as a function of the relative timing of pre- and postsynaptic spikes. When a neuron is repeatedly presented with similar inputs, STDP is known to have the effect of concentrating high synaptic weights on afferents that systematically fire early, while postsynaptic spike latencies decrease. Here we use this learning rule in an asynchronous feedforward spiking neural network that mimics the ventral visual pathway and shows that when the network is presented with natural images, selectivity to intermediate-complexity visual features emerges. Those features, which correspond to prototypical patterns that are both salient and consistently present in the images, are highly informative and enable robust object recognition, as demonstrated on various classification tasks. Taken together, these results show that temporal codes may be a key to understanding the phenomenal processing speed achieved by the visual system and that STDP can lead to fast and selective responses.},
	author = {Masquelier, Timoth{\'e}e and Thorpe, Simon J.},
	doi = {10/cjkr36},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	language = {en},
	month = feb,
	note = {00314},
	number = {2},
	pages = {e31},
	title = {Unsupervised {Learning} of {Visual} {Features} through {Spike} {Timing} {Dependent} {Plasticity}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030031},
	urldate = {2018-09-10},
	volume = {3},
	year = {2007},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030031},
	bdsk-url-2 = {https://doi.org/10/cjkr36}}

@article{benosman_asynchronous_2012,
	abstract = {This paper introduces a process to compute optical flow using an asynchronous event-based retina at high speed and low computational load. A new generation of artificial vision sensors has now started to rely on biologically inspired designs for light acquisition. Biological retinas, and their artificial counterparts, are totally asynchronous and data driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework for processing visual data using asynchronous event-based acquisition, providing a method for the evaluation of optical flow. The paper shows that current limitations of optical flow computation can be overcome by using event-based visual acquisition, where high data sparseness and high temporal resolution permit the computation of optical flow with micro-second accuracy and at very low computational cost.},
	author = {Benosman, Ryad},
	doi = {10.1016/j.neunet.2011.11.001},
	journal = {Neural Networks},
	keywords = {Asynchronous acquisition, Event-based vision, Frameless vision, Optical flow, Spikes, Temporal dynamics, ⛔ No INSPIRE recid found},
	language = {english},
	pages = {6},
	title = {Asynchronous frameless event-based optical flow},
	url = {https://doi.org/10/b55t75},
	volume = {27},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10/b55t75},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2011.11.001}}

@article{delorme_spikenet_1999,
	abstract = {SpikeNET is a simulator for modeling large networks of asynchronously spiking neurons. It uses simple integrate-and-fire neurons which undergo step-like changes in membrane potential when synaptic inputs arrive. If a threshold is exceeded, the potential is reset and the neuron added to a list to be propagated on the next time step. Using such spike lists greatly reduces the computations associated with large networks, and simplifies implementations using parallel hardware since inter-processor communication can be limited to sending lists of the neurons which just fired. We have used it to model complex multi-layer architectures based on the primate visual system that involve millions of neurons and billions of synaptic connections. Such models are not only biological but also efficient, robust and very fast, qualities which they share with the human visual system.},
	author = {Delorme, Arnaud and Gautrais, Jacques and van Rullen, Rufin and Thorpe, Simon},
	doi = {10.1016/S0925-2312(99)00095-8},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Biological visual systems, Categorization, Modeling software, Natural scenes, ⛔ No INSPIRE recid found},
	language = {en},
	month = jun,
	pages = {989--996},
	shorttitle = {{SpikeNET}},
	title = {{SpikeNET}: {A} simulator for modeling large networks of integrate and fire neurons},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231299000958},
	urldate = {2022-09-28},
	volume = {26-27},
	year = {1999},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231299000958},
	bdsk-url-2 = {https://doi.org/10.1016/S0925-2312(99)00095-8}}

@article{bohte_error-backpropagation_2002,
	abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutr{\'e}, Han},
	doi = {10.1016/S0925-2312(01)00658-0},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Error-backpropagation, Spiking neurons, Temporal coding, ⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	number = {1},
	pages = {17--37},
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	urldate = {2022-09-28},
	volume = {48},
	year = {2002},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	bdsk-url-2 = {https://doi.org/10.1016/S0925-2312(01)00658-0}}

@article{brunel_phase_2000,
	author = {Brunel, Nicolas},
	doi = {10.1016/s0925-2312(00)00179-x},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Integrate-and-fire neuron, Neural network, Oscillations, Synchrony, ⛔ No INSPIRE recid found},
	language = {en},
	month = jun,
	note = {00009},
	pages = {307--312},
	title = {Phase diagrams of sparsely connected networks of excitatory and inhibitory spiking neurons},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S092523120000179X},
	urldate = {2019-01-14},
	volume = {32-33},
	year = {2000},
	bdsk-url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S092523120000179X},
	bdsk-url-2 = {https://doi.org/10.1016/s0925-2312(00)00179-x}}

@article{izhikevich_polychronization_2006,
	author = {Izhikevich, Eugene M},
	doi = {10.1162/089976606775093882},
	journal = {Neural computation},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	number = {2},
	pages = {245--282},
	title = {Polychronization: computation with spikes},
	volume = {18},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1162/089976606775093882}}

@article{schrimpf_brain-score_2020,
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score -- a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain's mechanisms for core object recognition -- and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ¿= 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain's network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	doi = {10.1101/407007},
	journal = {bioRxiv : the preprint server for biology},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: Cold Spring Harbor Laboratory tex.elocation-id: 407007 tex.eprint: https://www.biorxiv.org/content/early/2020/01/02/407007.full.pdf},
	title = {Brain-score: {Which} artificial neural network for object recognition is most brain-like?},
	url = {https://www.biorxiv.org/content/early/2020/01/02/407007},
	year = {2020},
	bdsk-url-1 = {https://www.biorxiv.org/content/early/2020/01/02/407007},
	bdsk-url-2 = {https://doi.org/10.1101/407007}}

@book{mandelbrot_fractal_1982,
	abstract = {Rev. ed. of: Fractals. c1977; Includes indexes; Bibliography: p. [425]-443},
	author = {Mandelbrot, Benoit B.},
	collaborator = {{Internet Archive}},
	isbn = {978-0-7167-1186-5},
	keywords = {⛔ No INSPIRE recid found},
	language = {eng},
	publisher = {San Francisco : W.H. Freeman},
	title = {The fractal geometry of nature},
	url = {http://archive.org/details/fractalgeometryo00beno},
	urldate = {2022-09-27},
	year = {1982},
	bdsk-url-1 = {http://archive.org/details/fractalgeometryo00beno}}

@article{le_bec_horizontal_2022,
	abstract = {This study demonstrates the functional importance of the Surround context relayed laterally in V1 by the horizontal connectivity, in controlling the latency and the gain of the cortical response to the feedforward visual drive. We report here four main findings: 1) a centripetal apparent motion sequence results in a shortening of the spiking latency of V1 cells, when the orientation of the local inducer and the global motion axis are both co-aligned with the RF orientation preference; 2) this contextual effects grows with visual flow speed, peaking at 150--250$\,^{\circ}$/s when it matches the propagation speed of horizontal connectivity (0.15--0.25 mm/ms); 3) For this speed range, the axial sensitivity of V1 cells is tilted by 90$\,^{\circ}$ to become co-aligned with the orientation preference axis; 4) the strength of modulation by the surround context correlates with the spatiotemporal coherence of the apparent motion flow. Our results suggest an internally-generated binding process, linking local (orientation /position) and global (motion/direction) features as early as V1. This long-range diffusion process constitutes a plausible substrate in V1 of the human psychophysical bias in speed estimation for collinear motion. Since it is demonstrated in the anesthetized cat, this novel form of contextual control of the cortical gain and phase is a built-in property in V1, whose expression does not require behavioral attention and top-down control from higher cortical areas. We propose that horizontal connectivity participates in the propagation of an internal ``prediction'' wave, shaped by visual experience, which links contour co-alignment and global axial motion at an apparent speed in the range of saccade-like eye movements.},
	author = {Le Bec, Benoit and Troncoso, Xoana G. and Desbois, Christophe and Passarelli, Yannick and Baudot, Pierre and Monier, Cyril and Pananceau, Marc and Fr{\'e}gnac, Yves},
	doi = {10.1371/journal.pone.0268351},
	editor = {Charpier, St{\'e}phane},
	issn = {1932-6203},
	journal = {PLOS ONE},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jul,
	number = {7},
	pages = {e0268351},
	shorttitle = {Horizontal connectivity in {V1}},
	title = {Horizontal connectivity in {V1}: {Prediction} of coherence in contour and motion integration},
	url = {https://dx.plos.org/10.1371/journal.pone.0268351},
	urldate = {2022-09-26},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://dx.plos.org/10.1371/journal.pone.0268351},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0268351}}

@article{macdonald_neuromorphic_2022,
	abstract = {Dexterous manipulation in robotic hands relies on an accurate sense of artificial touch. Here we investigate neuromorphic tactile sensation with an event-based optical tactile sensor combined with spiking neural networks for edge orientation detection. The sensor incorporates an event-based vision system (mini-eDVS) into a low-form factor artificial fingertip (the NeuroTac). The processing of tactile information is performed through a Spiking Neural Network with unsupervised Spike-Timing-Dependent Plasticity (STDP) learning, and the resultant output is classified with a 3-nearest neighbours classifier. Edge orientations were classified in 10-degree increments while tapping vertically downward and sliding horizontally across the edge. In both cases, we demonstrate that the sensor is able to reliably detect edge orientation, and could lead to accurate, bio-inspired, tactile processing in robotics and prosthetics applications.},
	author = {Macdonald, Fraser L. A. and Lepora, Nathan F. and Conradt, J{\"o}rg and Ward-Cherrier, Benjamin},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	doi = {10.3390/s22186998},
	issn = {1424-8220},
	journal = {Sensors},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	note = {Number: 18 Publisher: Multidisciplinary Digital Publishing Institute},
	number = {18},
	pages = {6998},
	title = {Neuromorphic {Tactile} {Edge} {Orientation} {Classification} in an {Unsupervised} {Spiking} {Neural} {Network}},
	url = {https://www.mdpi.com/1424-8220/22/18/6998},
	urldate = {2022-09-26},
	volume = {22},
	year = {2022},
	bdsk-url-1 = {https://www.mdpi.com/1424-8220/22/18/6998},
	bdsk-url-2 = {https://doi.org/10.3390/s22186998}}

@article{berens_fast_2012,
	abstract = {Orientation tuning has been a classic model for understanding single-neuron computation in the neocortex. However, little is known about how orientation can be read out from the activity of neural populations, in particular in alert animals. Our study is a first step toward that goal. We recorded from up to 20 well isolated single neurons in the primary visual cortex of alert macaques simultaneously and applied a simple, neurally plausible decoder to read out the population code. We focus on two questions: First, what are the time course and the timescale at which orientation can be read out from the population response? Second, how complex does the decoding mechanism in a downstream neuron have to be to reliably discriminate between visual stimuli with different orientations? We show that the neural ensembles in primary visual cortex of awake macaques represent orientation in a way that facilitates a fast and simple readout mechanism: With an average latency of 30--80 ms, the population code can be read out instantaneously with a short integration time of only tens of milliseconds, and neither stimulus contrast nor correlations need to be taken into account to compute the optimal synaptic weight pattern. Our study shows that---similar to the case of single-neuron computation---the representation of orientation in the spike patterns of neural populations can serve as an exemplary case for understanding the computations performed by neural ensembles underlying visual processing during behavior.},
	author = {Berens, Philipp and Ecker, Alexander S. and Cotton, R. James and Ma, Wei Ji and Bethge, Matthias and Tolias, Andreas S.},
	copyright = {Copyright {\copyright} 2012 the authors 0270-6474/12/3210618-09\$15.00/0. This article is freely available online through the J Neurosci Open Choice option.},
	doi = {10.1523/jneurosci.1335-12.2012},
	issn = {0270-6474, 1529-2401},
	journal = {Journal of Neuroscience},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	language = {en},
	month = aug,
	note = {00000 tex.ids= Berens12a publisher: Society for Neuroscience section: Articles},
	number = {31},
	pages = {10618--10626},
	pmid = {22855811},
	title = {A {Fast} and {Simple} {Population} {Code} for {Orientation} in {Primate} {V1}},
	url = {https://www.jneurosci.org/content/32/31/10618},
	urldate = {2020-11-09},
	volume = {32},
	year = {2012},
	bdsk-url-1 = {https://www.jneurosci.org/content/32/31/10618},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.1335-12.2012}}

@techreport{bernert_fully_2017,
	abstract = {Spike sorting is a crucial step of neural data processing widely used in neuroscience and neuroprosthetics. However, current methods remain not fully automatic and require heavy computations making them not embeddable in implantable devices. To overcome these limitations, we propose a novel method based on an artificial spiking neural network designed to process neural data online and completely automatically. An input layer continuously encodes the data stream into artificial spike trains, which are then processed by two further layers to output artificial trains of spikes reproducing the real spiking activity present in the input signal. The proposed method can be adapted to process several channels simultaneously in the case of tetrode recordings. It outperforms two existing algorithms at low SNR and has the advantage to be compatible with neuromorphic computing and the perspective of being embedded in very low-power analog systems for future implantable devices serving neurorehabilitation applications.},
	author = {Bernert, Marie and Yvert, Blaise},
	copyright = {{\copyright} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	doi = {10.1101/236224},
	institution = {bioRxiv},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = dec,
	note = {tex.ids= Bernert2017a section: New Results type: article},
	pages = {236224},
	title = {Fully unsupervised online spike sorting based on an artificial spiking neural network},
	url = {https://www.biorxiv.org/content/10.1101/236224v1},
	urldate = {2022-04-08},
	year = {2017},
	bdsk-url-1 = {https://www.biorxiv.org/content/10.1101/236224v1},
	bdsk-url-2 = {https://doi.org/10.1101/236224}}

@article{bernert_attention-based_2018,
	abstract = {Bio-inspired computing using artificial spiking neural networks promises performances outperforming currently available computational approaches. Yet, the number of applications of such networks remains limited due to the absence of generic training procedures for complex pattern recognition, which require the design of dedicated architectures for each situation. We developed a spike-timing-dependent plasticity (STDP) spiking neural network (SSN) to address spike-sorting, a central pattern recognition problem in neuroscience. This network is designed to process an extracellular neural signal in an online and unsupervised fashion. The signal stream is continuously fed to the network and processed through several layers to output spike trains matching the truth after a short learning period requiring only few data. The network features an attention mechanism to handle the scarcity of action potential occurrences in the signal, and a threshold adaptation mechanism to handle patterns with different sizes. This method outperforms two existing spike-sorting algorithms at low signal-to-noise ratio (SNR) and can be adapted to process several channels simultaneously in the case of tetrode recordings. Such attention-based STDP network applied to spike-sorting opens perspectives to embed neuromorphic processing of neural data in future brain implants.},
	author = {Bernert, Marie and Yvert, Blaise},
	doi = {10.1142/s0129065718500594},
	issn = {0129-0657},
	journal = {International Journal of Neural Systems},
	keywords = {⛔ No INSPIRE recid found},
	month = dec,
	note = {00016 tex.ids= Bernert2019, Bernert2019a publisher: World Scientific Publishing Co.},
	number = {08},
	pages = {1850059},
	title = {An {Attention}-{Based} {Spiking} {Neural} {Network} for {Unsupervised} {Spike}-{Sorting}},
	url = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	urldate = {2021-01-26},
	volume = {29},
	year = {2018},
	bdsk-url-1 = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	bdsk-url-2 = {https://doi.org/10.1142/s0129065718500594}}

@article{haag_fly_2004,
	abstract = {The computational structure of an optimal motion detector was proposed to depend on the signal-to-noise ratio (SNR) of the stimulus: At low SNR, the optimal motion detector should be a correlation or "Reichardt" type, whereas at high SNR, the detector would employ a gradient scheme [Potters, M. \& Bialek, W. (1994) J. Physiol. (Paris) 4, 1755-1775]. Although a large body of experiments supports the Reichardt detector as the processing scheme leading to direction selectivity in fly motion vision, in most of these studies the SNR was rather low. We therefore reinvestigated the question over a much larger SNR range. Using 2-photon microscopy, we found that local dendritic [Ca(2+)] modulations, which are characteristic of Reichardt detectors, occur in response to drifting gratings over a wide range of luminance levels and contrasts. We also explored, as another fingerprint of Reichardt detectors, the dependence of the velocity optimum on the pattern wavelength. Again, we found Reichardt-typical behavior throughout the whole luminance and contrast range tested. Our results, therefore, provide strong evidence that only a single elementary processing scheme is used in fly motion vision.},
	author = {Haag, J. and Denk, W. and Borst, A.},
	doi = {10.1073/pnas.0407368101},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Algorithms, Animals, Calcium Signaling, Diptera, Electrophysiology, Female, Models, Neurological, Motion, Motion Perception, Optics and Photonics, Photic Stimulation, Vision, Ocular, biology, delay-learning, insects, ⛔ No INSPIRE recid found},
	language = {eng},
	month = nov,
	number = {46},
	pages = {16333--16338},
	pmcid = {PMC526200},
	pmid = {15534201},
	title = {Fly motion vision is based on {Reichardt} detectors regardless of the signal-to-noise ratio},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.0407368101}}

@article{paredes-valles_unsupervised_2020,
	abstract = {The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.},
	author = {Paredes-Vall{\'e}s, Federico and Scheper, Kirk Y. W. and de Croon, Guido C. H. E.},
	doi = {10.1109/tpami.2019.2903179},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Biological information theory, Biological system modeling, Biomedical optical imaging, Event-based vision, Neurons, Optical sensors, Vision sensors, Visualization, feature extraction, motion detection, neural nets, neuromorphic computing, unsupervised learning, ⛔ No INSPIRE recid found},
	month = aug,
	note = {00047 Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	number = {8},
	pages = {2051--2064},
	shorttitle = {Unsupervised {Learning} of a {Hierarchical} {Spiking} {Neural} {Network} for {Optical} {Flow} {Estimation}},
	title = {Unsupervised {Learning} of a {Hierarchical} {Spiking} {Neural} {Network} for {Optical} {Flow} {Estimation}: {From} {Events} to {Global} {Motion} {Perception}},
	volume = {42},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/tpami.2019.2903179}}

@article{benosman_event-based_2014,
	abstract = {This paper introduces a new methodology to compute dense visual flow using the precise timings of spikes from an asynchronous event-based retina. Biological retinas, and their artificial counterparts, are totally asynchronous and data-driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework to estimate visual flow from the local properties of events' spatiotemporal space. We will show that precise visual flow orientation and amplitude can be estimated using a local differential approach on the surface defined by coactive events. Experimental results are presented; they show the method adequacy with high data sparseness and temporal resolution of event-based acquisition that allows the computation of motion flow with microsecond accuracy and at very low computational cost.},
	author = {Benosman, Ryad and Clercq, Charles and Lagorce, Xavier and {Sio-Hoi Ieng} and Bartolozzi, Chiara},
	doi = {10.1109/tnnls.2013.2273537},
	issn = {2162-237X, 2162-2388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = feb,
	number = {2},
	pages = {407--417},
	title = {Event-{Based} {Visual} {Flow}},
	url = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	urldate = {2022-02-01},
	volume = {25},
	year = {2014},
	bdsk-url-1 = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	bdsk-url-2 = {https://doi.org/10.1109/tnnls.2013.2273537}}

@inproceedings{lee_real-time_2014,
	abstract = {Fast and efficient motion estimation is essential for a number of applications including the gesture-based user interface (UI) for portable devices like smart phones. In this paper, we propose a highly efficient method that can estimate four degree of freedom (DOF) motional components of a moving object based on an event-based vision sensor, the dynamic vision sensor (DVS). The proposed method finds informative events occurred at edges and estimates their velocities for global motion analysis. We will also describe a novel method to correct the aperture problem in the motion estimation.},
	address = {Paris, France},
	author = {Lee, Jun Haeng and Lee, Kyoobin and Ryu, Hyunsurk and Park, Paul K. J. and Shin, Chang-Woo and Woo, Jooyeon and Kim, Jun-Seok},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	doi = {10.1109/ICIP.2014.7025040},
	isbn = {978-1-4799-5751-4},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	pages = {204--208},
	publisher = {IEEE},
	title = {Real-time motion estimation based on event-based vision sensor},
	url = {http://ieeexplore.ieee.org/document/7025040/},
	urldate = {2022-07-19},
	year = {2014},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/7025040/},
	bdsk-url-2 = {https://doi.org/10.1109/ICIP.2014.7025040}}

@article{frye_elementary_2015,
	author = {Frye, Mark},
	doi = {10.1016/j.cub.2015.01.013},
	issn = {09609822},
	journal = {Current Biology},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = mar,
	number = {6},
	pages = {R215--R217},
	title = {Elementary motion detectors},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215000159},
	urldate = {2022-03-21},
	volume = {25},
	year = {2015},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215000159},
	bdsk-url-2 = {https://doi.org/10.1016/j.cub.2015.01.013}}

@article{nawrot_eye_2003,
	abstract = {It has been unclear whether the perception of depth from motion parallax is an entirely visual process or whether it requires extra-retinal information such as head movements, vestibular activation, or eye movements. Using a motion aftereffect and static test stimulus technique to eliminate visual cues to depth, this psychophysical study demonstrates that the visual system employs a slow eye movement signal, optokinetic response (OKR) in particular, for the unambiguous perception of depth from motion parallax. A vestibular signal, or vestibularly driven eye movement signal is insufficient for unambiguous depth from motion parallax. Removal of the OKR eye movement signal gives rise to ambiguous perceived depth in motion parallax conditions. Neurophysiological studies suggest a possible neural mechanism in medial temporal and medial superior temporal cortical neurons that are selective to depth, motion, and direction of eye movement.},
	author = {Nawrot, Mark},
	doi = {10.1016/S0042-6989(03)00144-5},
	issn = {0042-6989},
	journal = {Vision Research},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jun,
	number = {14},
	pages = {1553--1562},
	title = {Eye movements provide the extra-retinal signal required for the perception of depth from motion parallax},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698903001445},
	urldate = {2022-09-15},
	volume = {43},
	year = {2003},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0042698903001445},
	bdsk-url-2 = {https://doi.org/10.1016/S0042-6989(03)00144-5}}

@article{masquelier_competitive_2009,
	author = {Masquelier, Timoth{\'e}e and Guyonneau, Rudy and Thorpe, Simon J.},
	doi = {10.1162/neco.2008.06-08-804},
	issn = {0899-7667, 1530-888X},
	journal = {Neural Computation},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	language = {en},
	month = may,
	note = {00203},
	number = {5},
	pages = {1259--1276},
	title = {Competitive {STDP}-{Based} {Spike} {Pattern} {Learning}},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.06-08-804},
	urldate = {2018-09-10},
	volume = {21},
	year = {2009},
	bdsk-url-1 = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.06-08-804},
	bdsk-url-2 = {https://doi.org/10.1162/neco.2008.06-08-804}}

@article{barlow_unsupervised_1989,
	abstract = {What use can the brain make of the massive flow of sensory information that occurs without any associated rewards or punishments? This question is reviewed in the light of connectionist models of unsupervised learning and some older ideas, namely the cognitive maps and working models of Tolman and Craik, and the idea that redundancy is important for understanding perception (Attneave 1954), the physiology of sensory pathways (Barlow 1959), and pattern recognition (Watanabe 1960). It is argued that (1) The redundancy of sensory messages provides the knowledge incorporated in the maps or models. (2) Some of this knowledge can be obtained by observations of mean, variance, and covariance of sensory messages, and perhaps also by a method called ``minimum entropy coding.'' (3) Such knowledge may be incorporated in a model of ``what usually happens'' with which incoming messages are automatically compared, enabling unexpected discrepancies to be immediately identified. (4) Knowledge of the sort incorporated into such a filter is a necessary prerequisite of ordinary learning, and a representation whose elements are independent makes it possible to form associations with logical functions of the elements, not just with the elements themselves.},
	author = {Barlow, H.B.},
	doi = {10.1162/neco.1989.1.3.295},
	issn = {0899-7667},
	journal = {Neural Computation},
	keywords = {⛔ No INSPIRE recid found},
	month = sep,
	number = {3},
	pages = {295--311},
	title = {Unsupervised {Learning}},
	url = {https://doi.org/10.1162/neco.1989.1.3.295},
	urldate = {2022-09-15},
	volume = {1},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1162/neco.1989.1.3.295}}

@article{yoonessi_contribution_2011,
	abstract = {Relative image motion resulting from active movement of the observer could potentially serve as a powerful perceptual cue, both for segmentation of object boundaries and for depth perception. To examine the perceptual role of motion parallax from shearing motion, we measured human performance in three psychophysical tasks: segmentation, depth ordering, and depth magnitude estimation. Stimuli consisted of random dot textures that were synchronized to head movement with sine- or square-wave modulation patterns. Segmentation was assessed with a 2AFC orientation judgment of a motion-defined boundary. In the depth-ordering task, observers reported which modulation half-cycle appeared in front of the other. Perceived depth magnitude was matched to that of a 3D rendered image with multiple static cues. The results indicate that head movement might not be important for segmentation, even though it is crucial for obtaining depth from motion parallax---thus, concomitant depth perception does not appear to facilitate segmentation. Our findings suggest that segmentation works best for abrupt, sharply defined motion boundaries, whereas smooth gradients are more powerful for obtaining depth from motion parallax. Thus, motion parallax may contribute in a different manner to segmentation and to depth perception and suggests that their underlying mechanisms might be distinct.},
	author = {Yoonessi, Ahmad and Baker, Jr., Curtis L.},
	doi = {10.1167/11.9.13},
	issn = {1534-7362},
	journal = {Journal of Vision},
	keywords = {⛔ No INSPIRE recid found},
	month = aug,
	number = {9},
	pages = {13},
	title = {Contribution of motion parallax to segmentation and depth perception},
	url = {https://doi.org/10.1167/11.9.13},
	urldate = {2022-09-15},
	volume = {11},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1167/11.9.13}}

@article{perrinet_emergence_2004,
	abstract = {As an alternative to classical representations in machine learning algorithms, we explore coding strategies using events as is observed for spiking neurons in the central nervous system. Focusing on visual processing, we have previously shown that we may define a sparse spike coding scheme by implementing accordingly lateral interactions (Neurocomputing 57 (2004) 125). This class of algorithms is both compatible with biological constraints and also to neurophysiological observations and yields a performant algorithm of computing by events. We explore here learning mechanisms to unsupervisely derive an optimal overcomplete set of filters based on previous work of (Vision Res. 37 (1998) 3311) and show its biological relevance. {\copyright} 2004 Elsevier B.V. All rights reserved.},
	author = {Perrinet, Laurent},
	copyright = {All rights reserved},
	doi = {10.1016/j.neucom.2004.01.133},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Sparse spike coding, Unsupervised learning, Vision, area-v1, receptive field, receptive\_field, sparse coding, sparse\_coding, ⛔ No INSPIRE recid found},
	number = {C},
	pages = {821--826},
	title = {Emergence of filters from natural scenes in a sparse spike coding scheme},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231204001389},
	volume = {58-60},
	year = {2004},
	bdsk-url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0925231204001389},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2004.01.133}}

@article{pastalkova_internally_2008,
	abstract = {A longstanding conjecture in neuroscience is that aspects of cognition depend on the brain's ability to self-generate sequential neuronal activity. We found that reliably and continually-changing cell assemblies in the rat hippocampus appeared not only during spatial navigation but also in the absence of changing environmental or body-derived inputs. During the delay period of a memory task each moment in time was characterized by the activity of a unique assembly of neurons. Identical initial conditions triggered a similar assembly sequence, whereas different conditions gave rise, uniquely, to different sequences, thereby predicting behavioral choices, including errors. Such sequences were not formed in control, non-memory, tasks. We hypothesize that neuronal representations, evolved for encoding distance in spatial navigation, also support episodic recall and the planning of action sequences.},
	author = {Pastalkova, Eva and Itskov, Vladimir and Amarasingham, Asohan and Buzs{\'a}ki, Gy{\"o}rgy},
	doi = {10.1126/science.1159775},
	issn = {0036-8075},
	journal = {Science (New York, N.Y.)},
	keywords = {⛔ No INSPIRE recid found},
	month = sep,
	number = {5894},
	pages = {1322--1327},
	title = {Internally {Generated} {Cell} {Assembly} {Sequences} in the {Rat} {Hippocampus}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	urldate = {2022-02-23},
	volume = {321},
	year = {2008},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	bdsk-url-2 = {https://doi.org/10.1126/science.1159775}}

@article{malvache_awake_2016,
	abstract = {The chained activation of neuronal assemblies is thought to support major cognitive processes, including memory. In the hippocampus, this is observed during population bursts often associated with sharp-wave ripples, in the form of an ordered reactivation of neurons. However, the organization and lifetime of these assemblies remain unknown. We used calcium imaging to map patterns of synchronous neuronal activation in the CA1 region of awake mice during runs on a treadmill. The patterns were composed of the recurring activation of anatomically intermingled, but functionally orthogonal, assemblies. These assemblies reactivated discrete temporal segments of neuronal sequences observed during runs and could be stable across consecutive days. A binding of these assemblies into longer chains revealed temporally ordered replay. These modules may represent the default building blocks for encoding or retrieving experience.},
	author = {Malvache, Arnaud and Reichinnek, Susanne and Villette, Vincent and Haimerl, Caroline and Cossart, Rosa},
	doi = {10.1126/science.aaf3319},
	issn = {1095-9203},
	journal = {Science (New York, N.Y.)},
	keywords = {Animals, Brain Mapping, CA1 Region, Hippocampal, Calcium Signaling, Exercise Test, Male, Mice, Nerve Net, Neurons, Running, Wakefulness, ⛔ No INSPIRE recid found},
	language = {eng},
	month = sep,
	number = {6305},
	pages = {1280--1283},
	title = {Awake hippocampal reactivations project onto orthogonal neuronal assemblies},
	volume = {353},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1126/science.aaf3319}}

@article{haimerl_internal_2019,
	abstract = {The hippocampus plays a critical role in episodic memory: the sequential representation of visited places and experienced events. This function is mirrored by hippocampal activity that self organizes into sequences of neuronal activation that integrate spatiotemporal information. What are the underlying mechanisms of such integration is still unknown. Single cell activity was recently shown to combine time and distance information; however, it remains unknown whether a degree of tuning between space and time can be defined at the network level. Here, combining daily calcium imaging of CA1 sequence dynamics in running head-fixed mice and network modeling, we show that CA1 network activity tends to represent a specific combination of space and time at any given moment, and that the degree of tuning can shift within a continuum from 1 day to the next. Our computational model shows that this shift in tuning can happen under the control of the external drive power. We propose that extrinsic global inputs shape the nature of spatiotemporal integration in the hippocampus at the population level depending on the task at hand, a hypothesis which may guide future experimental studies.},
	author = {Haimerl, Caroline and Angulo-Garcia, David and Villette, Vincent and Reichinnek, Susanne and Torcini, Alessandro and Cossart, Rosa and Malvache, Arnaud},
	doi = {10.1073/pnas.1718518116},
	issn = {0027-8424, 1091-6490},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {attractor network, hippocampus, neural model, space representation, time representation, ⛔ No INSPIRE recid found},
	language = {en},
	month = apr,
	number = {15},
	pages = {7477--7482},
	title = {Internal representation of hippocampal neuronal population spans a time-distance continuum},
	url = {https://www.pnas.org/content/116/15/7477},
	urldate = {2022-01-17},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/content/116/15/7477},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1718518116}}

@article{luczak_sequential_2007,
	abstract = {Even in the absence of sensory stimulation, the neocortex shows complex spontaneous activity patterns, often consisting of alternating ``DOWN'' states of generalized neural silence and ``UP'' states of massive, persistent network activity. To investigate how this spontaneous activity propagates through neuronal assemblies in vivo, we simultaneously recorded populations of 50--200 cortical neurons in layer V of anesthetized and awake rats. Each neuron displayed a virtually unique spike pattern during UP states, with diversity seen amongst both putative pyramidal cells and interneurons, reflecting a complex but stereotypically organized sequential spread of activation through local cortical networks. Spike timing was most precise during the first ≈100 ms after UP state onset, and decayed as UP states progressed. A subset of UP states propagated as traveling waves, but waves passing a given point in either direction initiated similar local sequences, suggesting local networks as the substrate of sequential firing patterns. A search for repeating motifs indicated that their occurrence and structure was predictable from neurons' individual latencies to UP state onset. We suggest that these stereotyped patterns arise from the interplay of intrinsic cellular conductances and local circuit properties.},
	author = {Luczak, Artur and Barth{\'o}, Peter and Marguet, Stephan L. and Buzs{\'a}ki, Gy{\"o}rgy and Harris, Kenneth D.},
	doi = {10.1073/pnas.0605643104},
	issn = {0027-8424, 1091-6490},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {microcircuits, neuronal assembly, repeating sequences, slow oscillations, syntire chains, ⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	number = {1},
	pages = {347--352},
	title = {Sequential structure of neocortical spontaneous activity in vivo},
	url = {https://www.pnas.org/content/104/1/347},
	urldate = {2022-02-23},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://www.pnas.org/content/104/1/347},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0605643104}}

@article{ikegaya_synfire_2004,
	author = {Ikegaya, Yuji and Aaron, Gloster and Cossart, Rosa and Aronov, Dmitriy and Lampl, Ilan and Ferster, David and Yuste, Rafael},
	doi = {10.1126/science.1093173},
	journal = {Science},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	month = apr,
	number = {5670},
	pages = {559--564},
	shorttitle = {Synfire {Chains} and {Cortical} {Songs}},
	title = {Synfire {Chains} and {Cortical} {Songs}: {Temporal} {Modules} of {Cortical} {Activity}},
	url = {http://www.science.org/doi/10.1126/science.1093173},
	urldate = {2021-11-29},
	volume = {304},
	year = {2004},
	bdsk-url-1 = {http://www.science.org/doi/10.1126/science.1093173},
	bdsk-url-2 = {https://doi.org/10.1126/science.1093173}}

@article{engbert_integrated_2011,
	abstract = {When we fixate a stationary target, our eyes generate miniature (or fixational) eye movements involuntarily. These fixational eye movements are classified as slow components (physiological drift, tremor) and microsaccades, which represent rapid, small-amplitude movements. Here we propose an integrated mathematical model for the generation of slow fixational eye movements and microsaccades. The model is based on the concept of self-avoiding random walks in a potential, a process driven by a self-generated activation field. The self-avoiding walk generates persistent movements on a short timescale, whereas, on a longer timescale, the potential produces antipersistent motions that keep the eye close to an intended fixation position. We introduce microsaccades as fast movements triggered by critical activation values. As a consequence, both slow movements and microsaccades follow the same law of motion; i.e., movements are driven by the self-generated activation field. Thus, the model contributes a unified explanation of why it has been a long-standing problem to separate slow movements and microsaccades with respect to their motion-generating principles. We conclude that the concept of a self-avoiding random walk captures fundamental properties of fixational eye movements and provides a coherent theoretical framework for two physiologically distinct movement types.},
	author = {Engbert, Ralf and Mergenthaler, Konstantin and Sinn, Petra and Pikovsky, Arkady},
	doi = {10.1073/pnas.1102730108},
	issn = {0027-8424, 1091-6490},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = sep,
	number = {39},
	pages = {E765--E770},
	title = {An integrated model of fixational eye movements and microsaccades},
	url = {https://www.pnas.org/content/108/39/E765},
	urldate = {2021-02-18},
	volume = {108},
	year = {2011},
	bdsk-url-1 = {https://www.pnas.org/content/108/39/E765},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1102730108}}

@article{villette_internally_2015,
	abstract = {The hippocampus is essential for spatiotemporal cognition. Sequences of neuronal activation provide a substrate for this fundamental function. At the behavioral timescale, these sequences have been shown to occur either in the presence of successive external landmarks or through internal mechanisms within an episodic memory task. In both cases, activity is externally constrained by the organization of the task and by the size of the environment explored. Therefore, it remains unknown whether hippocampal activity can self-organize into a default mode in the absence of any external memory demand or spatiotemporal boundary. Here we show that, in the presence of self-motion cues, a population code integrating distance naturally emerges in the hippocampus in the form of recurring sequences. These internal dynamics clamp spontaneous travel since run distance distributes into integer multiples of the span of these sequences. These sequences may thus guide navigation when external landmarks are reduced.},
	author = {Villette, Vincent and Malvache, Arnaud and Tressard, Thomas and Dupuy, Nathalie and Cossart, Rosa},
	doi = {10.1016/j.neuron.2015.09.052},
	issn = {0896-6273},
	journal = {Neuron},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	number = {2},
	pages = {357--366},
	title = {Internally {Recurring} {Hippocampal} {Sequences} as a {Population} {Template} of {Spatiotemporal} {Information}},
	urldate = {2022-01-17},
	volume = {88},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1016/j.neuron.2015.09.052}}

@article{brette_exact_2007,
	abstract = {Neural networks can be simulated exactly using event-driven strategies, in which the algorithm advances directly from one spike to the next spike. It applies to neuron models for which we have (1) an explicit expression for the evolution of the state variables between spikes and (2) an explicit test on the state variables that predicts whether and when a spike will be emitted. In a previous work, we proposed a method that allows exact simulation of an integrate-and-fire model with exponential conductances, with the constraint of a single synaptic time constant. In this note, we propose a method, based on polynomial root finding, that applies to integrate-and-fire models with exponential currents, with possibly many different synaptic time constants. Models can include biexponential synaptic currents and spike-triggered adaptation currents.},
	author = {Brette, Romain},
	doi = {10.1162/neco.2007.19.10.2604},
	issn = {0899-7667, 1530-888X},
	journal = {Neural Computation},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = oct,
	number = {10},
	pages = {2604--2609},
	title = {Exact {Simulation} of {Integrate}-and-{Fire} {Models} with {Exponential} {Currents}},
	url = {https://direct.mit.edu/neco/article/19/10/2604-2609/7220},
	urldate = {2022-09-15},
	volume = {19},
	year = {2007},
	bdsk-url-1 = {https://direct.mit.edu/neco/article/19/10/2604-2609/7220},
	bdsk-url-2 = {https://doi.org/10.1162/neco.2007.19.10.2604}}

@article{bohte_evidence_2004,
	author = {Bohte, Sander M},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	journal = {Natural Computing},
	keywords = {⛔ No INSPIRE recid found},
	number = {2},
	pages = {195--206},
	title = {The evidence for neural information processing with precise spike-times: {A} survey},
	volume = {3},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1023/B:NACO.0000027755.02868.60}}

@article{boutin_sparse_2020,
	abstract = {Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1 \& V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.},
	author = {Boutin, Victor and Franciosini, Angelo and Chavane, Fr{\'e}d{\'e}ric Y and Ruffier, Franck and Perrinet, Laurent U},
	copyright = {All rights reserved},
	doi = {10.1371/journal.pcbi.1008629},
	journal = {PLoS Computational Biology},
	keywords = {deep-learning, sparse coding, ⛔ No INSPIRE recid found},
	month = may,
	title = {Sparse {Deep} {Predictive} {Coding} captures contour integration capabilities of the early visual system},
	url = {https://doi.org/10.1371/journal.pcbi.1008629},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pcbi.1008629}}

@article{boutin_pooling_2022,
	abstract = {Neurons in the primary visual cortex are selective to orientation with various degrees of selectivity to the spatial phase, from high selectivity in simple cells to low selectivity in complex cells. Various computational models have suggested a possible link between the presence of phase invariant cells and the existence of orientation maps in higher mammals' V1. These models, however, do not explain the emergence of complex cells in animals that do not show orientation maps. In this study, we build a theoretical model based on a convolutional network called Sparse Deep Predictive Coding (SDPC) and show that a single computational mechanism, pooling, allows the SDPC model to account for the emergence in V1 of complex cells with or without that of orientation maps, as observed in distinct species of mammals. In particular, we observed that pooling in the feature space is directly related to the orientation map formation while pooling in the retinotopic space is responsible for the emergence of a complex cells population. Introducing different forms of pooling in a predictive model of early visual processing as implemented in SDPC can therefore be viewed as a theoretical framework that explains the diversity of structural and functional phenomena observed in V1.},
	author = {Boutin, Victor and Franciosini, Angelo and Chavane, Fr{\'e}d{\'e}ric and Perrinet, Laurent U.},
	doi = {10.1371/journal.pcbi.1010270},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	keywords = {Coding mechanisms, Convolution, Neural networks, Neuronal tuning, Neurons, Neurophysiology, Visual cortex, Visual system, ⛔ No INSPIRE recid found},
	language = {en},
	number = {7},
	pages = {e1010270},
	title = {Pooling strategies in {V1} can account for the functional and structural diversity across species},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010270},
	urldate = {2022-09-14},
	volume = {18},
	year = {2022},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010270},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1010270}}

@article{carr_circuit_1990,
	author = {Carr, CE and Konishi, M},
	doi = {10.1523/JNEUROSCI.10-10-03227.1990},
	journal = {Journal of Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	number = {10},
	pages = {3227--3246},
	title = {A circuit for detection of interaural time differences in the brain stem of the barn owl},
	volume = {10},
	year = {1990},
	bdsk-url-1 = {https://doi.org/10.1523/JNEUROSCI.10-10-03227.1990}}

@article{dandekar_neural_2012,
	abstract = {Studying neural activity during natural viewing conditions is not often attempted. Isolating the neural response of a single saccade is necessary to study neural activity during natural viewing; however, the close temporal spacing of saccades that occurs during natural viewing makes it difficult to determine the response to a single saccade. Herein, a general linear model (GLM) approach is applied to estimate the EEG neural saccadic response for different segments of the saccadic main sequence separately. It is determined that, in visual search conditions, neural responses estimated by conventional event-related averaging are significantly and systematically distorted relative to GLM estimates due to the close temporal spacing of saccades during visual search. Before the GLM is applied, analyses are applied that demonstrate that saccades during visual search with intersaccadic spacings as low as 100--150 ms do not exhibit significant refractory effects. Therefore, saccades displaying different intersaccadic spacings during visual search can be modeled using the same regressor in a GLM. With the use of the GLM approach, neural responses were separately estimated for five different ranges of saccade amplitudes during visual search. Occipital responses time locked to the onsets of saccades during visual search were found to account for, on average, 79 percent of the variance of EEG activity in a window 90--200 ms after the onsets of saccades for all five saccade amplitude ranges that spanned a range of 0.2--6.0 degrees. A GLM approach was also used to examine the lateralized ocular artifacts associated with saccades. Possible extensions of the methods presented here to account for the superposition of microsaccades in event-related EEG studies conducted in nominal fixation conditions are discussed.},
	author = {Dandekar, Sangita and Privitera, Claudio and Carney, Thom and Klein, Stanley A.},
	doi = {10.1152/jn.00237.2011},
	issn = {0022-3077},
	journal = {Journal of Neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	month = mar,
	number = {6},
	pages = {1776--1790},
	title = {Neural saccadic response estimation during natural viewing},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3311669/},
	urldate = {2022-09-14},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3311669/},
	bdsk-url-2 = {https://doi.org/10.1152/jn.00237.2011}}

@article{davis_spontaneous_2021,
	author = {Davis, Zachary W and Benigno, Gabriel B and Fletterman, Charlee and Desbordes, Theo and Steward, Christopher and Sejnowski, Terrence J and H Reynolds, John and Muller, Lyle},
	doi = {10.1038/s41467-021-26175-1},
	journal = {Nature Communications},
	keywords = {⛔ No INSPIRE recid found},
	number = {1},
	pages = {1--16},
	title = {Spontaneous traveling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-021-26175-1}}

@article{ghosh_spatiotemporal_2019,
	author = {Ghosh, Rohan and Gupta, Anupam and Silva, Andrei Nakagawa and Soares, Alcimar and Thakor, Nitish V.},
	journal = {CoRR},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	title = {Spatiotemporal filtering for event-based action recognition},
	url = {http://arxiv.org/abs/1903.07067},
	volume = {abs/1903.07067},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1903.07067}}

@article{gollisch_rapid_2008,
	author = {Gollisch, Tim and Meister, Markus},
	doi = {10.1126/science.1149639},
	journal = {Science (New York, N.Y.)},
	keywords = {⛔ No INSPIRE recid found},
	number = {5866},
	pages = {1108--1111},
	title = {Rapid neural coding in the retina with relative spike latencies},
	volume = {319},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1126/science.1149639}}

@article{gutig_tempotron_2006,
	abstract = {The timing of action potentials in sensory neurons contains substantial information about the eliciting stimuli. Although the computational advantages of spike timing--based neuronal codes have long been recognized, it is unclear whether, and if so how, neurons can learn to read out such representations. We propose a new, biologically plausible supervised synaptic learning rule that enables neurons to efficiently learn a broad range of decision rules, even when information is embedded in the spatiotemporal structure of spike patterns rather than in mean firing rates. The number of categorizations of random spatiotemporal patterns that a neuron can implement is several times larger than the number of its synapses. The underlying nonlinear temporal computation allows neurons to access information beyond single-neuron statistics and to discriminate between inputs on the basis of multineuronal spike statistics. Our work demonstrates the high capacity of neural systems to learn to decode information embedded in distributed patterns of spike synchrony.},
	author = {G{\"u}tig, Robert and Sompolinsky, Haim},
	doi = {10.1038/nn1643},
	issn = {1546-1726},
	journal = {Nature Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	language = {english},
	month = mar,
	number = {3},
	pages = {420--428},
	shorttitle = {The tempotron},
	title = {The tempotron: {A} neuron that learns spike {Timing}--{Based} decisions},
	url = {http://www.nature.com/articles/nn1643/},
	urldate = {2022-01-31},
	volume = {9},
	year = {2006},
	bdsk-url-1 = {http://www.nature.com/articles/nn1643/},
	bdsk-url-2 = {https://doi.org/10.1038/nn1643}}

@article{hanuschkin_general_2010,
	abstract = {Traditionally, event-driven simulations have been limited to the very restricted class of neuronal models for which the timing of future spikes can be expressed in closed form. Recently, the class of models that is amenable to event-driven simulation has been extended by the development of techniques to accurately calculate firing times for some integrate-and-fire neuron models that do not enable the prediction of future spikes in closed form. The motivation of this development is the general perception that time-driven simulations are imprecise. Here, we demonstrate that a globally time-driven scheme can calculate firing times that cannot be discriminated from those calculated by an event-driven implementation of the same model; moreover, the time-driven scheme incurs lower computational costs. The key insight is that time-driven methods are based on identifying a threshold crossing in the recent past, which can be implemented by a much simpler algorithm than the techniques for predicting future threshold crossings that are necessary for event-driven approaches. As run time is dominated by the cost of the operations performed at each incoming spike, which includes spike prediction in the case of event-driven simulation and retrospective detection in the case of time-driven simulation, the simple time-driven algorithm outperforms the event-driven approaches. Additionally, our method is generally applicable to all commonly used integrate-and-fire neuronal models; we show that a non-linear model employing a standard adaptive solver can reproduce a reference spike train with a high degree of precision.},
	author = {Hanuschkin, Alexander and Kunkel, Susanne and Helias, Moritz and Morrison, Abigail and Diesmann, Markus},
	doi = {10.3389/fninf.2010.00113},
	issn = {1662-5196},
	journal = {Frontiers in Neuroinformatics},
	keywords = {⛔ No INSPIRE recid found},
	month = oct,
	pages = {113},
	title = {A {General} and {Efficient} {Method} for {Incorporating} {Precise} {Spike} {Times} in {Globally} {Time}-{Driven} {Simulations}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2965048/},
	urldate = {2022-09-14},
	volume = {4},
	year = {2010},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2965048/},
	bdsk-url-2 = {https://doi.org/10.3389/fninf.2010.00113}}

@article{hogendoorn_predictive_2019,
	author = {Hogendoorn, Hinze and Burkitt, Anthony N.},
	doi = {10.1523/eneuro.0412-18.2019},
	issn = {2373-2822},
	journal = {eneuro},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = mar,
	number = {2},
	pages = {ENEURO.0412--18.2019},
	shorttitle = {Predictive {Coding} with {Neural} {Transmission} {Delays}},
	title = {Predictive {Coding} with {Neural} {Transmission} {Delays}: {A} {Real}-{Time} {Temporal} {Alignment} {Hypothesis}},
	url = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	urldate = {2019-11-12},
	volume = {6},
	year = {2019},
	bdsk-url-1 = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	bdsk-url-2 = {https://doi.org/10.1523/eneuro.0412-18.2019}}

@article{khoei_flash-lag_2017,
	abstract = {Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object's motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects' position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation at the present time of spatial information in the visual pathways.},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	copyright = {Licence Creative Commons Attribution - Pas d'utilisation commerciale - Partage dans les m{\^e}mes conditions 4.0 International (CC-BY-NC-SA)},
	doi = {10.1371/journal.pcbi.1005068},
	issn = {1553-7358},
	journal = {PLOS Computational Biology},
	keywords = {Coding mechanisms, Extrapolation, Motion, Psychophysics, Sensory perception, Velocity, Vision, Visual system, ⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	number = {1},
	pages = {e1005068},
	title = {The {Flash}-{Lag} {Effect} as a {Motion}-{Based} {Predictive} {Shift}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	urldate = {2022-08-31},
	volume = {13},
	year = {2017},
	bdsk-url-1 = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pcbi.1005068}}

@article{koenderink_representation_1987,
	abstract = {It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring.},
	author = {Koenderink, J. J. and van Doorn, A. J.},
	doi = {10.1007/BF00318371},
	issn = {1432-0770},
	journal = {Biological Cybernetics},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = mar,
	number = {6},
	pages = {367--375},
	title = {Representation of local geometry in the visual system},
	url = {https://doi.org/10.1007/BF00318371},
	urldate = {2022-08-31},
	volume = {55},
	year = {1987},
	bdsk-url-1 = {https://doi.org/10.1007/BF00318371}}

@article{lagorce_hots_2017,
	abstract = {This paper describes novel event-based spatiotemporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving near 100\% accuracy on each. We introduce a new 7 class moving face recognition task, achieving 79\% accuracy.},
	author = {Lagorce, Xavier and Orchard, Garrick and Galluppi, Francesco and Shi, Bertram E. and Benosman, Ryad B.},
	doi = {10.1109/TPAMI.2016.2574707},
	issn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Neuromorphic sensing, event-based vision, feature extraction, ⛔ No INSPIRE recid found},
	number = {7},
	pages = {1346--1359},
	title = {{HOTS}: {A} {Hierarchy} of {Event}-{Based} {Time}-{Surfaces} for {Pattern} {Recognition}},
	volume = {39},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TPAMI.2016.2574707}}

@article{leon_motion_2012,
	author = {Leon, Paula Sanz and Vanzetta, Ivo and Masson, Guillaume S and Perrinet, Laurent U},
	doi = {10.1152/jn.00737.2011},
	journal = {Journal of Neurophysiology},
	keywords = {⛔ No INSPIRE recid found},
	number = {11},
	pages = {3217--3226},
	title = {Motion {Clouds}: {Model}-based stimulus synthesis of natural-like random textures for the study of motion perception},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1152/jn.00737.2011}}

@article{lin_supervised_2021,
	abstract = {As a new brain-inspired computational model of artificial neural networks, spiking neural networks transmit and process information via precisely timed spike trains. Constructing efficient learning methods is a significant research field in spiking neural networks. In this paper, we present a supervised learning algorithm for multilayer feedforward spiking neural networks; all neurons can fire multiple spikes in all layers. The feedforward network consists of spiking neurons governed by biologically plausible long-term memory spike response model, in which the effect of earlier spikes on the refractoriness is not neglected to incorporate adaptation effects. The gradient descent method is employed to derive synaptic weight updating rule for learning spike trains. The proposed algorithm is tested and verified on spatiotemporal pattern learning problems, including a set of spike train learning tasks and nonlinear pattern classification problems on four UCI datasets. Simulation results indicate that the proposed algorithm can improve learning accuracy in comparison with other supervised learning algorithms.},
	author = {Lin, Xianghong and Zhang, Mengwei and Wang, Xiangwen},
	doi = {10.1155/2021/8592824},
	issn = {1687-5265},
	journal = {Computational Intelligence and Neuroscience},
	keywords = {⛔ No INSPIRE recid found},
	month = nov,
	pages = {8592824},
	title = {Supervised {Learning} {Algorithm} for {Multilayer} {Spiking} {Neural} {Networks} with {Long}-{Term} {Memory} {Spike} {Response} {Model}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8635912/},
	urldate = {2022-09-14},
	volume = {2021},
	year = {2021},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8635912/},
	bdsk-url-2 = {https://doi.org/10.1155/2021/8592824}}

@article{luo_supervised_2022,
	abstract = {The brain-inspired spiking neural networks (SNNs) hold the advantages of lower power consumption and powerful computing capability. However, the lack of effective learning algorithms has obstructed the theoretical advance and applications of SNNs. The majority of the existing learning algorithms for SNNs are based on the synaptic weight adjustment. However, neuroscience findings confirm that synaptic delays can also be modulated to play an important role in the learning process. Here, we propose a gradient descent-based learning algorithm for synaptic delays to enhance the sequential learning performance of single spiking neuron. Moreover, we extend the proposed method to multilayer SNNs with spike temporal-based error backpropagation. In the proposed multilayer learning algorithm, information is encoded in the relative timing of individual neuronal spikes, and learning is performed based on the exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. Experimental results on both synthetic and realistic datasets show significant improvements in learning efficiency and accuracy over the existing spike temporal-based learning algorithms. We also evaluate the proposed learning method in an SNN-based multimodal computational model for audiovisual pattern recognition, and it achieves better performance compared with its counterparts.},
	author = {Luo, Xiaoling and Qu, Hong and Wang, Yuchen and Yi, Zhang and Zhang, Jilun and Zhang, Malu},
	doi = {10.1109/TNNLS.2022.3164930},
	issn = {2162-2388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Backpropagation, Biological system modeling, Delays, Heuristic algorithms, Membrane potentials, Neurons, Nonhomogeneous media, spike neural networks, spike neurons, supervised learning, synaptic delay plasticity., ⛔ No INSPIRE recid found},
	pages = {1--13},
	title = {Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks} {With} {Spike} {Temporal} {Error} {Backpropagation}},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TNNLS.2022.3164930}}

@article{nessler_bayesian_2013,
	author = {Nessler, Bernhard and Pfeiffer, Michael and Buesing, Lars and Maass, Wolfgang},
	doi = {10.1371/journal.pcbi.1003037},
	journal = {PLoS computational biology},
	keywords = {⛔ No INSPIRE recid found},
	number = {4},
	pages = {e1003037},
	title = {Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity},
	volume = {9},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pcbi.1003037}}

@article{perrinet_active_2014,
	abstract = {This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.},
	author = {Perrinet, Laurent U and Adams, Rick A and Friston, Karl J},
	copyright = {All rights reserved},
	doi = {10.1007/s00422-014-0620-8},
	issn = {1432-0770},
	journal = {Biological Cybernetics},
	keywords = {\#nosource, Active inference, Bayesian model, Biologically Inspired Computer vision, Generalised coordinates, Oculomotor delays, Smooth pursuit eye movements, Tracking eye movements, Variational free energy, active inference, active-inference, bayesian, bicv-motion, bicv-sparse, delays, eye, eye movements, eye-movements, free energy, free-energy, generalized-coordinates, generalized-filtering, motion detection, oculomotor, perception, perrinetadamsfriston14, smooth-pursuit, tracking-eye-movements, variational-filtering, ⛔ No INSPIRE recid found},
	month = dec,
	number = {6},
	pages = {777--801},
	title = {Active inference, eye movements and oculomotor delays},
	url = {https://doi.org/10.1007/s00422-014-0620-8},
	volume = {108},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00422-014-0620-8}}

@article{perrinet_edge_2015,
	author = {Perrinet, Laurent U and Bednar, James A},
	doi = {10.1038/srep11400},
	journal = {Scientific reports},
	keywords = {\#nosource, Biologically Inspired Computer vision, anr-trax, association field, assofield, bicv-sparse, perrinetbednar15, sanz12jnp, sparse coding, vacher14, ⛔ No INSPIRE recid found},
	pages = {11400},
	title = {Edge co-occurrences can account for rapid categorization of natural versus animal images},
	volume = {5},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1038/srep11400}}

@article{poletti_head-eye_2015,
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Humans explore static visual scenes by alternating rapid eye movements (saccades) with periods of slow and incessant eye drifts [1--3]. These drifts are commonly believed to be the consequence of physiological limits in maintaining steady gaze, resulting in Brownian-like trajectories [4--7], which are almost independent in the two eyes [8--10]. However, because of the technical difficulty of recording minute eye movements, most knowledge on ocular drift comes from artificial laboratory conditions, in which the head of the observer is strictly immobilized. Little is known about eye drift during natural head-free fixation, when microscopic head movements are also continually present [11--13]. We have recently observed that the power spectrum of the visual input to the retina during ocular drift is largely unaffected by fixational head movements [14]. Here we elucidate the mechanism responsible for this invariance. We show that, contrary to common assumption, ocular drift does not move the eyes randomly, but compensates for microscopic head movements, thereby yielding highly correlated movements in the two eyes. This compensatory behavior is extremely fast, persists with one eye patched, and results in image motion trajectories that are only partially correlated on the two retinas. These findings challenge established views of how humans acquire visual information. They show that ocular drift is precisely controlled, as long speculated [15], and imply the existence of neural mechanisms that integrate minute multimodal signals.{\textless}/p{\textgreater}},
	author = {Poletti, Martina and Aytekin, Murat and Rucci, Michele},
	doi = {10.1016/j.cub.2015.11.004},
	issn = {0960-9822},
	journal = {Current Biology},
	keywords = {⛔ No INSPIRE recid found},
	language = {English},
	month = dec,
	number = {24},
	pages = {3253--3259},
	title = {Head-{Eye} {Coordination} at a {Microscopic} {Scale}},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(15)01365-2},
	urldate = {2022-09-13},
	volume = {25},
	year = {2015},
	bdsk-url-1 = {https://www.cell.com/current-biology/abstract/S0960-9822(15)01365-2},
	bdsk-url-2 = {https://doi.org/10.1016/j.cub.2015.11.004}}

@article{van_der_stigchel_eye_2006,
	abstract = {In the last two decades, research has shown that eye movement trajectories can be modified by situational determinants. These modifications can inform us about the mechanisms that control eye movements and they can yield information about the oculomotor, memory and attention system that is not easily obtained via other sources. Eye movement trajectories can deviate either towards or away from elements in the visual field. We review the conditions in which these deviations are found and the mechanisms underlying trajectory deviations. It is argued that deviations towards an element are caused by the unresolved competition in the oculomotor system between elements in a visual scene. Deviations away from an element are mainly observed in situations in which top-down preparation can influence the target selection process, but the exact cause of such deviations remains unclear.},
	author = {Van der Stigchel, Stefan and Meeter, Martijn and Theeuwes, Jan},
	doi = {10.1016/j.neubiorev.2005.12.001},
	issn = {0149-7634},
	journal = {Neuroscience \& Biobehavioral Reviews},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = jan,
	number = {5},
	pages = {666--679},
	title = {Eye movement trajectories and what they tell us},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763405001740},
	urldate = {2022-09-13},
	volume = {30},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0149763405001740},
	bdsk-url-2 = {https://doi.org/10.1016/j.neubiorev.2005.12.001}}

@incollection{perrinet_sparse_2015,
	address = {Weinheim, Germany},
	author = {Perrinet, Laurent U},
	booktitle = {Biologically {Inspired} {Computer} {Vision}},
	editor = {Keil, Matthias and Crist{\'o}bal, Gabriel and Perrinet, Laurent U},
	keywords = {\#nosource, Biologically Inspired Computer vision, anr-trax, bicv-sparse, sanz12jnp, sparse coding, vacher14, ⛔ No INSPIRE recid found},
	month = aug,
	pages = {319--346},
	publisher = {Wiley-VCH Verlag GmbH \& Co. KGaA},
	title = {Sparse {Models} for {Computer} {Vision}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9783527680863.ch14/summary},
	year = {2015},
	bdsk-url-1 = {http://onlinelibrary.wiley.com/doi/10.1002/9783527680863.ch14/summary}}

@article{rogers_motion_1979,
	abstract = {The perspective transformations of the retinal image, produced by either the movement of an observer or the movement of objects in the visual world, were found to produce a reliable, consistent, and unambiguous impression of relative depth in the absence of all other cues to depth and distance. The stimulus displays consisted of computer-generated random-dot patterns that could be transformed by each movement of the observer or the display oscilloscope to simulate the relative movement information produced by a three-dimensional surface. Using a stereoscopic matching task, the second experiment showed that the perceived depth from parallax transformations is in close agreement with the degree of relative image displacement, as well as producing a compelling impression of three-dimensionality not unlike that found with random-dot stereograms.},
	author = {Rogers, Brian and Graham, Maureen},
	doi = {10.1068/p080125},
	issn = {0301-0066},
	journal = {Perception},
	language = {en},
	month = apr,
	note = {Publisher: SAGE Publications Ltd STM},
	number = {2},
	pages = {125--134},
	title = {Motion {Parallax} as an {Independent} {Cue} for {Depth} {Perception}},
	url = {https://doi.org/10.1068/p080125},
	urldate = {2022-09-15},
	volume = {8},
	year = {1979},
	bdsk-url-1 = {https://doi.org/10.1068/p080125}}

@article{wang_delay_2019,
	abstract = {Neuroscience research confirms that the synaptic delays are not constant, but can be modulated. This paper proposes a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. The proposed algorithm firstly defines spike train kernels to transform discrete spike trains during the learning phase into continuous analog signals so that common mathematical operations can be performed on them, and then deduces the supervised learning rules of synaptic weights and delays by gradient descent method. The proposed algorithm is successfully applied to various spike train learning tasks, and the effects of parameters of synaptic delays are analyzed in detail. Experimental results show that the network with dynamic delays achieves higher learning accuracy and less learning epochs than the network with static delays. The delay learning algorithm is further validated on a practical example of an image classification problem. The results again show that it can achieve a good classification performance with a proper receptive field. Therefore, the synaptic delay learning is significant for practical applications and theoretical researches of spiking neural networks.},
	author = {Wang, Xiangwen and Lin, Xianghong and Dang, Xiaochao},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	title = {A {Delay} {Learning} {Algorithm} {Based} on {Spike} {Train} {Kernels} for {Spiking} {Neurons}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252},
	urldate = {2022-09-14},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252}}

@article{tatler_eye_2011,
	abstract = {Models of gaze allocation in complex scenes are derived mainly from studies of static picture viewing. The dominant framework to emerge has been image salience, where properties of the stimulus play a crucial role in guiding the eyes. However, salience-based schemes are poor at accounting for many aspects of picture viewing and can fail dramatically in the context of natural task performance. These failures have led to the development of new models of gaze allocation in scene viewing that address a number of these issues. However, models based on the picture-viewing paradigm are unlikely to generalize to a broader range of experimental contexts, because the stimulus context is limited, and the dynamic, task-driven nature of vision is not represented. We argue that there is a need to move away from this class of model and find the principles that govern gaze allocation in a broader range of settings. We outline the major limitations of salience-based selection schemes and highlight what we have learned from studies of gaze allocation in natural vision. Clear principles of selection are found across many instances of natural vision and these are not the principles that might be expected from picture-viewing studies. We discuss the emerging theoretical framework for gaze allocation on the basis of reward maximization and uncertainty reduction.},
	author = {Tatler, Benjamin W. and Hayhoe, Mary M. and Land, Michael F. and Ballard, Dana H.},
	doi = {10.1167/11.5.5},
	issn = {1534-7362},
	journal = {Journal of Vision},
	keywords = {⛔ No INSPIRE recid found},
	month = may,
	number = {5},
	pages = {5},
	shorttitle = {Eye guidance in natural vision},
	title = {Eye guidance in natural vision: {Reinterpreting} salience},
	url = {https://doi.org/10.1167/11.5.5},
	urldate = {2022-09-14},
	volume = {11},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1167/11.5.5}}

@article{zhang_supervised_2020,
	author = {Zhang, Malu and Wu, Jibin and Belatreche, Ammar and Pan, Zihan and Xie, Xiurui and Chua, Yansong and Li, Guoqi and Qu, Hong and Li, Haizhou},
	doi = {10.1016/j.neucom.2020.03.079},
	journal = {Neurocomputing},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: Elsevier},
	pages = {103--118},
	title = {Supervised learning in spiking neural networks with synaptic delay-weight plasticity},
	volume = {409},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.neucom.2020.03.079}}

@article{perrinet_coding_2004,
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	doi = {10.1109/TNN.2004.833303},
	journal = {IEEE Transactions on neural networks},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: IEEE},
	number = {5},
	pages = {1164--1175},
	title = {Coding static natural images using spiking event times: do neurons cooperate?},
	volume = {15},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/TNN.2004.833303}}

@article{riehle_spike_1997,
	author = {Riehle, Alexa and Grun, Sonja and Diesmann, Markus and Aertsen, Ad},
	doi = {10.1126/science.278.5345.1950},
	journal = {Science (New York, N.Y.)},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: American Association for the Advancement of Science},
	number = {5345},
	pages = {1950--1953},
	title = {Spike synchronization and rate modulation differentially involved in motor cortical function},
	volume = {278},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1126/science.278.5345.1950}}

@article{rasetto_challenges_2022,
	abstract = {Neuromorphic engineering has led to the necessary process of rethinking of how we process and integrate information, analyze data, and use the resulting insights to improve computation and avoid the current high power and latency of Artificial Intelligence (AI) hardware. Current neuromorphic processors are, however, limited by digital technologies, which cannot reproduce the abilities of biological neural computation in terms of power, latency and area cost. In this paper, we show that the combined use of the dynamic properties of memristors to implement a model of synaptic integration and the determination of the correct level of abstraction of biological neural networks has the potential to open a new range of capabilities for neuromorphic processors. We test this approach using a novel three-terminal LixWO3 electrochemical memristor, by deriving its conductance model and using it to emulate synaptic temporal kernel computation in the context of a pattern recognition task. We show that these devices allow for robust results with no loss in precision while opening the path for an energy efficient approach to build novel bio-inspired processing units in silicon.},
	author = {Rasetto, Marco and Wan, Qingzhou and Akolkar, Himanshu and Shi, Bertram and Xiong, Feng and Benosman, Ryad},
	journal = {arXiv:2201.12673 [cs]},
	keywords = {Computer Science - Emerging Technologies, ⛔ No DOI found, ⛔ No INSPIRE recid found},
	language = {english},
	month = jan,
	note = {arXiv: 2201.12673 [cs]},
	shorttitle = {The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event} {Processors}},
	title = {The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event} {Processors}: {How} {Memristors} {Dynamic} {Properties} {Could} {Revolutionize} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2201.12673},
	urldate = {2022-02-02},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2201.12673}}

@article{chavane_revisiting_2022,
	abstract = {Horizontal connections in the primary visual cortex of carnivores, ungulates and primates organize on a near-regular lattice. Given the similar length scale for the regularity found in cortical orientation maps, the currently accepted theoretical standpoint is that these maps are underpinned by a like-to-like connectivity rule: horizontal axons connect preferentially to neurons with similar preferred orientation. However, there is reason to doubt the rule's explanatory power, since a growing number of quantitative studies show that the like-to-like connectivity preference and bias mostly observed at short-range scale, are highly variable on a neuron-to-neuron level and depend on the origin of the presynaptic neuron. Despite the wide availability of published data, the accepted model of visual processing has never been revised. Here, we review three lines of independent evidence supporting a much-needed revision of the like-to-like connectivity rule, ranging from anatomy to population functional measures, computational models and to theoretical approaches. We advocate an alternative, distance-dependent connectivity rule that is consistent with new structural and functional evidence: from like-to-like bias at short horizontal distance to like-to-all at long horizontal distance. This generic rule accounts for the observed high heterogeneity in interactions between the orientation and retinotopic domains, that we argue is necessary to process non-trivial stimuli in a task-dependent manner.},
	author = {Chavane, Fr{\'e}d{\'e}ric and Perrinet, Laurent Udo and Rankin, James},
	copyright = {All rights reserved},
	doi = {10.1007/s00429-022-02455-4},
	issn = {1863-2661},
	journal = {Brain Structure and Function},
	keywords = {⛔ No INSPIRE recid found},
	language = {en},
	month = feb,
	shorttitle = {Revisiting horizontal connectivity rules in {V1}},
	title = {Revisiting horizontal connectivity rules in {V1}: from like-to-like towards like-to-all},
	url = {https://doi.org/10.1007/s00429-022-02455-4},
	urldate = {2022-02-06},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s00429-022-02455-4}}

@article{deweese_binary_2002,
	author = {DeWeese, Michael and Zador, Anthony},
	journal = {Advances in neural information processing systems},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	title = {Binary coding in auditory cortex},
	volume = {15},
	year = {2002}}

@article{dan_efficient_1996,
	author = {Dan, Yang and Atick, Joseph J and Reid, R C},
	doi = {10.1523/jneurosci.16-10-03351.1996},
	journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	month = may,
	number = {10},
	pages = {3351--3362},
	title = {Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory},
	volume = {16},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1523/jneurosci.16-10-03351.1996}}

@article{abeles_role_1982,
	author = {Abeles, Moshe},
	journal = {Israel journal of medical sciences},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
	number = {1},
	pages = {83--92},
	title = {Role of the cortical neuron: integrator or coincidence detector?},
	volume = {18},
	year = {1982}}

@article{roelfsema_early_2016,
	author = {Roelfsema, Pieter R and de Lange, Floris P},
	doi = {10.1146/annurev-vision-111815-114443},
	journal = {Annual review of vision science},
	keywords = {⛔ No INSPIRE recid found},
	note = {Publisher: Annual Reviews},
	pages = {131--151},
	title = {Early visual cortex as a multiscale cognitive blackboard},
	volume = {2},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1146/annurev-vision-111815-114443}}

@article{Russo2017,
	author = {Russo, Eleonora and Durstewitz, Daniel},
	doi = {10.7554/elife.19428},
	journal = {eLife},
	month = jan,
	title = {Cell assemblies at multiple time scales with arbitrary lag constellations},
	url = {https://doi.org/f9kxd8},
	volume = {6},
	year = {2017},
	bdsk-url-1 = {https://doi.org/f9kxd8},
	bdsk-url-2 = {https://doi.org/10.7554/elife.19428}}

@article{Berens2012,
	author = {Berens, P. and Ecker, A. S. and Cotton, R. J. and Ma, W. J. and Bethge, M. and Tolias, A. S.},
	doi = {10.1523/jneurosci.1335-12.2012},
	journal = {Journal of Neuroscience},
	number = {31},
	pages = {10618--10626},
	title = {A {Fast} and {Simple} {Population} {Code} for {Orientation} in {Primate} {V1}},
	url = {https://doi.org/f365rn},
	volume = {32},
	year = {2012},
	bdsk-url-1 = {https://doi.org/f365rn},
	bdsk-url-2 = {https://doi.org/10.1523/jneurosci.1335-12.2012}}

@article{Stella2019,
	author = {Stella, Alessandra and Quaglio, Pietro and Torre, Emiliano and Gr{\"u}n, Sonja},
	doi = {10.1016/j.biosystems.2019.104022},
	journal = {Biosystems},
	pages = {104022},
	title = {3d-{SPADE}: {Significance} evaluation of spatio-temporal patterns of various temporal extents},
	url = {https://doi.org/gpshj2},
	volume = {185},
	year = {2019},
	bdsk-url-1 = {https://doi.org/gpshj2},
	bdsk-url-2 = {https://doi.org/10.1016/j.biosystems.2019.104022}}
