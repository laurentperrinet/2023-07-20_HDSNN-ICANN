
@article{bohte_evidence_2004,
	title = {The evidence for neural information processing with precise spike-times: {A} survey},
	volume = {3},
	issn = {1572-9796},
	shorttitle = {The evidence for neural information processing with precise spike-times},
	url = {https://doi.org/10.1023/B:NACO.0000027755.02868.60},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	abstract = {This paper surveys recent findings in neuroscience regarding the behavioral relevancy of the precise timing with which real spiking neurons emit spikes. The literature suggests that in almost any system where the processing-speed of a neural (sub)-system is required to be high, the timing of single spikes can be very precise and reliable. Additionally, new, more refined methods are finding precisely timed spikes where previously none where found. This line of evidence thus provides additional motivation for researching the computational properties of networks of artificial spiking neurons that compute with more precisely timed spikes.},
	language = {en},
	number = {2},
	urldate = {2023-06-01},
	journal = {Natural Computing},
	author = {Bohte, Sander M.},
	month = jun,
	year = {2004},
	keywords = {neural coding, precise spike timing, spiking neural networks, synchrony coding, temporal coding, ⛔ No INSPIRE recid found},
	pages = {195--206},
}

@article{paredes-valles_unsupervised_2020,
	title = {Unsupervised {Learning} of a {Hierarchical} {Spiking} {Neural} {Network} for {Optical} {Flow} {Estimation}: {From} {Events} to {Global} {Motion} {Perception}},
	volume = {42},
	issn = {1939-3539},
	shorttitle = {Unsupervised {Learning} of a {Hierarchical} {Spiking} {Neural} {Network} for {Optical} {Flow} {Estimation}},
	doi = {10.1109/tpami.2019.2903179},
	abstract = {The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Paredes-Vallés, Federico and Scheper, Kirk Y. W. and de Croon, Guido C. H. E.},
	month = aug,
	year = {2020},
	note = {00000 
Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {\#nosource, Biological information theory, Biological system modeling, Biomedical optical imaging, Event-based vision, Neurons, Optical sensors, Vision sensors, Visualization, feature extraction, motion detection, neural nets, neuromorphic computing, unsupervised learning, ⛔ No INSPIRE recid found},
	pages = {2051--2064},
}

@article{haag_fly_2004,
	title = {Fly motion vision is based on {Reichardt} detectors regardless of the signal-to-noise ratio},
	volume = {101},
	issn = {0027-8424},
	doi = {10.1073/pnas.0407368101},
	abstract = {The computational structure of an optimal motion detector was proposed to depend on the signal-to-noise ratio (SNR) of the stimulus: At low SNR, the optimal motion detector should be a correlation or "Reichardt" type, whereas at high SNR, the detector would employ a gradient scheme [Potters, M. \& Bialek, W. (1994) J. Physiol. (Paris) 4, 1755-1775]. Although a large body of experiments supports the Reichardt detector as the processing scheme leading to direction selectivity in fly motion vision, in most of these studies the SNR was rather low. We therefore reinvestigated the question over a much larger SNR range. Using 2-photon microscopy, we found that local dendritic [Ca(2+)] modulations, which are characteristic of Reichardt detectors, occur in response to drifting gratings over a wide range of luminance levels and contrasts. We also explored, as another fingerprint of Reichardt detectors, the dependence of the velocity optimum on the pattern wavelength. Again, we found Reichardt-typical behavior throughout the whole luminance and contrast range tested. Our results, therefore, provide strong evidence that only a single elementary processing scheme is used in fly motion vision.},
	language = {eng},
	number = {46},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Haag, J. and Denk, W. and Borst, A.},
	month = nov,
	year = {2004},
	pmid = {15534201},
	pmcid = {PMC526200},
	note = {00164 },
	keywords = {\#nosource, Algorithms, Animals, Calcium Signaling, Diptera, Electrophysiology, Female, Models, Neurological, Motion, Motion Perception, Optics and Photonics, Photic Stimulation, Vision, Ocular, biology, delay-learning, insects, ⛔ No INSPIRE recid found},
	pages = {16333--16338},
}

@article{rubin_learned_2022,
	title = {Learned {Motor} {Patterns} {Are} {Replayed} in {Human} {Motor} {Cortex} during {Sleep}},
	volume = {42},
	copyright = {Copyright © 2022 Rubin et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/42/25/5007},
	doi = {10.1523/JNEUROSCI.2074-21.2022},
	abstract = {Consolidation of memory is believed to involve offline replay of neural activity. While amply demonstrated in rodents, evidence for replay in humans, particularly regarding motor memory, is less compelling. To determine whether replay occurs after motor learning, we sought to record from motor cortex during a novel motor task and subsequent overnight sleep. A 36-year-old man with tetraplegia secondary to cervical spinal cord injury enrolled in the ongoing BrainGate brain–computer interface pilot clinical trial had two 96-channel intracortical microelectrode arrays placed chronically into left precentral gyrus. Single- and multi-unit activity was recorded while he played a color/sound sequence matching memory game. Intended movements were decoded from motor cortical neuronal activity by a real-time steady-state Kalman filter that allowed the participant to control a neurally driven cursor on the screen. Intracortical neural activity from precentral gyrus and 2-lead scalp EEG were recorded overnight as he slept. When decoded using the same steady-state Kalman filter parameters, intracortical neural signals recorded overnight replayed the target sequence from the memory game at intervals throughout at a frequency significantly greater than expected by chance. Replay events occurred at speeds ranging from 1 to 4 times as fast as initial task execution and were most frequently observed during slow-wave sleep. These results demonstrate that recent visuomotor skill acquisition in humans may be accompanied by replay of the corresponding motor cortex neural activity during sleep.
SIGNIFICANCE STATEMENT Within cortex, the acquisition of information is often followed by the offline recapitulation of specific sequences of neural firing. Replay of recent activity is enriched during sleep and may support the consolidation of learning and memory. Using an intracortical brain–computer interface, we recorded and decoded activity from motor cortex as a human research participant performed a novel motor task. By decoding neural activity throughout subsequent sleep, we find that neural sequences underlying the recently practiced motor task are repeated throughout the night, providing direct evidence of replay in human motor cortex during sleep. This approach, using an optimized brain–computer interface decoder to characterize neural activity during sleep, provides a framework for future studies exploring replay, learning, and memory.},
	language = {en},
	number = {25},
	urldate = {2022-07-02},
	journal = {Journal of Neuroscience},
	author = {Rubin, Daniel B. and Hosman, Tommy and Kelemen, Jessica N. and Kapitonava, Anastasia and Willett, Francis R. and Coughlin, Brian F. and Halgren, Eric and Kimchi, Eyal Y. and Williams, Ziv M. and Simeral, John D. and Hochberg, Leigh R. and Cash, Sydney S.},
	month = jun,
	year = {2022},
	pmid = {35589391},
	keywords = {brain computer interface, learning, memory, replay, sleep, ⛔ No INSPIRE recid found},
	pages = {5007--5020},
}

@article{paz_multiple_2009,
	title = {Multiple forms of activity-dependent intrinsic plasticity in layer {V} cortical neurones in vivo},
	volume = {587},
	copyright = {© 2009 The Authors. Journal compilation © 2009 The Physiological Society},
	issn = {1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.2009.169334},
	doi = {10.1113/jphysiol.2009.169334},
	abstract = {Synaptic plasticity is classically considered as the neuronal substrate for learning and memory. However, activity-dependent changes in neuronal intrinsic excitability have been reported in several learning-related brain regions, suggesting that intrinsic plasticity could also participate to information storage. Compared to synaptic plasticity, there has been little exploration of the properties of induction and expression of intrinsic plasticity in an intact brain. Here, by the means of in vivo intracellular recordings in the rat we have examined how the intrinsic excitability of layer V motor cortex pyramidal neurones is altered following brief periods of repeated firing. Changes in membrane excitability were assessed by modifications in the discharge frequency versus injected current (F–I) curves. Most (∼64\%) conditioned neurones exhibited a long-lasting intrinsic plasticity, which was expressed either by selective changes in the current threshold or in the slope of the F–I curve, or by concomitant changes in both parameters. These modifications in the neuronal input–output relationship led to a global increase or decrease in intrinsic excitability. Passive electrical membrane properties were unaffected by the intracellular conditioning, indicating that intrinsic plasticity resulted from modifications of voltage-gated ion channels. These results demonstrate that neocortical pyramidal neurones can express in vivo a bidirectional use-dependent intrinsic plasticity, modifying their sensitivity to weak inputs and/or the gain of their input–output function. These multiple forms of experience-dependent intrinsic changes, which expand the computational abilities of individual neurones, could shape new network dynamics and thus might participate in the formation of mnemonic motor engrams.},
	language = {en},
	number = {13},
	urldate = {2022-12-09},
	journal = {The Journal of Physiology},
	author = {Paz, Jeanne T. and Mahon, Séverine and Tiret, Pascale and Genet, Stéphane and Delord, Bruno and Charpier, Stéphane},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.2009.169334
tex.ids= Paz2009},
	keywords = {⛔ No INSPIRE recid found},
	pages = {3189--3205},
}

@article{brunel_lapicques_2007,
	title = {Lapicque’s 1907 paper: from frogs to integrate-and-fire},
	volume = {97},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Lapicque’s 1907 paper},
	url = {http://link.springer.com/10.1007/s00422-007-0190-0},
	doi = {10.1007/s00422-007-0190-0},
	language = {en},
	number = {5-6},
	urldate = {2022-12-27},
	journal = {Biological Cybernetics},
	author = {Brunel, Nicolas and van Rossum, Mark C. W.},
	month = dec,
	year = {2007},
	keywords = {⛔ No INSPIRE recid found},
	pages = {337--339},
}

@misc{subramoney_egru_2022,
	title = {{EGRU}: {Event}-based {GRU} for activity-sparse inference and learning},
	shorttitle = {{EGRU}},
	url = {http://arxiv.org/abs/2206.06178},
	abstract = {The scalability of recurrent neural networks (RNNs) is hindered by the sequential dependence of each time step's computation on the previous time step's output. Therefore, one way to speed up and scale RNNs is to reduce the computation required at each time step independent of model size and task. In this paper, we propose a model that reformulates Gated Recurrent Units (GRU) as an event-based activity-sparse model that we call the Event-based GRU (EGRU), where units compute updates only on receipt of input events (event-based) from other units. When combined with having only a small fraction of the units active at a time (activity-sparse), this model has the potential to be vastly more compute efficient than current RNNs. Notably, activity-sparsity in our model also translates into sparse parameter updates during gradient descent, extending this compute efficiency to the training phase. We show that the EGRU demonstrates competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling while maintaining high activity sparsity naturally during inference and training. This sets the stage for the next generation of recurrent networks that are scalable and more suitable for novel neuromorphic hardware.},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Subramoney, Anand and Nazeer, Khaleelulla Khan and Schöne, Mark and Mayr, Christian and Kappel, David},
	month = nov,
	year = {2022},
	note = {arXiv:2206.06178 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ⛔ No INSPIRE recid found},
}

@article{jeremie_ultra-fast_2022,
	title = {Ultra-fast image categorization in vivo and in silico},
	url = {http://arxiv.org/abs/2205.03635},
	doi = {10.48550/arXiv.2205.03635},
	abstract = {Humans are able to robustly categorize images and can, for instance, detect the presence of an animal in a briefly flashed image in as little as 120 ms. Initially inspired by neuroscience, deep-learning algorithms literally bloomed up in the last decade such that the accuracy of machines is at present superior to humans for visual recognition tasks. However, these artificial networks are usually trained and evaluated on very specific tasks, for instance on the 1000 separate categories of ImageNet. In that regard, biological visual systems are more flexible and efficient compared to artificial systems on generic ecological tasks. In order to deepen this comparison, we re-trained the standard VGG Convolutional Neural Network (CNN) on two independent tasks which are ecologically relevant for humans: one task defined as detecting the presence of an animal and the other as detecting the presence of an artifact. We show that retraining the network achieves human-like performance level which is reported in psychophysical tasks. We also compare the accuracy of the detection on an image-by-image basis. This showed in particular that the two models perform better when combining their outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). These re-trained models could reproduce some unexpected behavioral observations from humans psychophysics such as the robustness to rotations (e.g. upside-down or slanted image) or to a grayscale transformation.},
	urldate = {2022-05-30},
	author = {Jérémie, Jean-Nicolas and Perrinet, Laurent U.},
	month = may,
	year = {2022},
	note = {arXiv: 2205.03635 [cs, q-bio]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition, ⛔ No INSPIRE recid found},
}

@article{destexhe_dendrites_2010,
	title = {Dendrites {Do} {It} in {Sequences}},
	volume = {329},
	url = {http://www.science.org/doi/10.1126/science.1196743},
	doi = {10.1126/science.1196743},
	number = {5999},
	urldate = {2022-04-05},
	journal = {Science},
	author = {Destexhe, Alain},
	month = sep,
	year = {2010},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1611--1612},
}

@article{smith_distributed_2018,
	title = {Distributed network interactions and their emergence in developing neocortex},
	volume = {21},
	issn = {1097-6256},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371984/},
	doi = {10.1038/s41593-018-0247-5},
	abstract = {The principles governing the functional organization and development of
long-range network interactions in the neocortex remain poorly understood. Using
in vivo wide-field and 2-photon calcium imaging of spontaneous activity patterns
in mature ferret visual cortex, we find widespread modular correlation patterns
that accurately predict the local structure of visually-evoked orientation
columns several millimeters away. Longitudinal imaging demonstrates that
long-range spontaneous correlations are present early in cortical development
prior to the elaboration of horizontal connections, and predict mature network
structure. Silencing feed-forward drive through retinal or thalamic blockade
does not eliminate early long-range correlated activity, suggesting a cortical
origin. Circuit models containing only local, but heterogeneous, connections are
sufficient to generate long-range correlated activity by confining activity
patterns to a low-dimensional subspace via multi-synaptic short-range
interactions. These results suggest that local connections in early cortical
circuits can generate structured long-range network correlations that guide the
formation of visually-evoked distributed functional networks.},
	number = {11},
	urldate = {2022-03-29},
	journal = {Nature neuroscience},
	author = {Smith, Gordon B. and Hein, Bettina and Whitney, David E. and Fitzpatrick, David and Kaschube, Matthias},
	month = nov,
	year = {2018},
	pmcid = {PMC6371984},
	pmid = {30349107},
	note = {tex.ids= Smith18a},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1600--1608},
}

@inproceedings{haoqi_sun_runtime_2015,
	address = {Killarney, Ireland},
	title = {Runtime detection of activated polychronous neuronal group towards its spatiotemporal analysis},
	isbn = {978-1-4799-1960-4},
	url = {http://ieeexplore.ieee.org/document/7280411/},
	doi = {10.1109/ijcnn.2015.7280411},
	abstract = {Due to the precise spike timing in neural coding, spiking neural network (SNN) possesses richer spatiotemporal dynamics compared to neural networks with ﬁring rate coding. One of the distinct features of SNN, polychronous neuronal group (PNG), receives much attention from both computational neuroscience and machine learning communities. However, all existing algorithms detect PNGs from the spike recording collected after simulation in an ofﬂine manner. There is currently no algorithm that detects PNGs actually being activated in runtime (online manner), which could be potentially used as inputs to higher level neural processing. We propose a runtime detection algorithm particularly for activated PNGs, using PNG readout neurons, to ﬁll this gap. The proposed algorithm can reveal the spatiotemporal PNG patterns embedded in spike trains, which is higher level neuronal dynamics. We demonstrate through an example that for composed input patterns, new PNGs except the constituent PNGs can be easily found using the proposed algorithm. As an important interpretation, we give further insights on how to use PNG readout neurons to construct layered network structure.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {{Haoqi Sun} and {Yan Yang} and Sourina, Olga and {Guang-Bin Huang}},
	month = jul,
	year = {2015},
	note = {00000},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {1--8},
}

@article{fields_plasticity_2019,
	title = {Plasticity of {Myelinating} {Glia}},
	volume = {67},
	issn = {1098-1136},
	doi = {10.1002/glia.23720},
	language = {eng},
	number = {11},
	journal = {Glia},
	author = {Fields, R. Douglas and Richardson, William D.},
	month = nov,
	year = {2019},
	pmid = {31515893},
	note = {00000 },
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {2005--2007},
}

@article{agmon-snir_signal_1993,
	title = {Signal delay and input synchronization in passive dendritic structures},
	volume = {70},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/abs/10.1152/jn.1993.70.5.2066},
	doi = {10.1152/jn.1993.70.5.2066},
	abstract = {1. A novel approach for analyzing transients in passive structures called "the method of moments" is introduced. It provides, as a special case, an analytic method for calculating the time delay and speed of propagation of electrical signals in any passive dendritic tree without the need for numerical simulations. 2. Total dendritic delay (TD) between two points (y, x) is defined as the difference between the centroid (the center of gravity) of the transient current input, I, at point y[tI(y)] and the centroid of the transient voltage response, V, at point x [tV(x)]. The TD measured at the input points is nonzero and is called the local delay (LD). Propagation delay, PD(y, x), is then defined as TD(y, x)--LD(y) whereas the net dendritic delay, NDD(y, 0), of an input point, y, is defined as TD(y, 0) - LD(0), where 0 is the target point, typically the soma. The signal velocity at a point x0 in the tree, theta(x0), is defined as [1/(dtv(x)/dx)[x = x0. 3. With the use of these definitions, several properties of dendritic delay exist. First, the delay between any two points in a given tree is independent of the properties (shape and duration) of the transient current input. Second, the velocity of the signal at any given point (y) in a given direction from (y) does not depend on the morphology of the tree "behind" the signal, and of the input location. Third, TD(y, x) = TD(x, y), for any two points, x, y. 4. Two additional properties are useful for efficiently calculating delays in arbitrary passive trees. 1) The subtrees connected at the ends of any dendritic segment can each be functionally lumped into an equivalent isopotential R-C compartment. 2) The local delay at any given point (y) in a tree is the mean of the local delays of the separate structures (subtrees) connected at y, weighted by the relative input conductance of the corresponding subtrees. 5. Because the definitions for delays utilize difference between centroids, the local delay and the total delay can be interpreted as measures for the time window in which synaptic inputs affect the voltage response at a target/decision point. Large LD or TD is closely associated with a relatively wide time window, whereas small LD or TD imply that inputs have to be well synchronized to affect the decision point. The net dendritic delay may be interpreted as the cost (in terms of delay) of moving a synapse away from the target point.(ABSTRACT TRUNCATED AT 400 WORDS)},
	number = {5},
	urldate = {2021-07-28},
	journal = {Journal of Neurophysiology},
	author = {Agmon-Snir, H. and Segev, I.},
	month = nov,
	year = {1993},
	note = {00000 
Publisher: American Physiological Society},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {2066--2085},
}

@article{mcdougall_myelination_2018,
	title = {Myelination of {Axons} {Corresponds} with {Faster} {Transmission} {Speed} in the {Prefrontal} {Cortex} of {Developing} {Male} {Rats}},
	volume = {5},
	issn = {2373-2822},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6140121/},
	doi = {10.1523/eneuro.0203-18.2018},
	abstract = {, Myelination of prefrontal circuits during adolescence is thought to lead to enhanced cognitive processing and improved behavioral control. However, while standard neuroimaging techniques commonly used in human and animal studies can measure large white matter bundles and residual conduction speed, they cannot directly measure myelination of individual axons or how fast electrical signals travel along these axons. Here we focused on a specific population of prefrontal axons to directly measure conduction velocity and myelin microstructure in developing male rats. An in vitro electrophysiological approach enabled us to isolate monosynaptic projections from the anterior branches of the corpus callosum (corpus callosum-forceps minor, CCFM) to the anterior cingulate subregion of the medial prefrontal cortex (Cg1) and to measure the speed and direction of action potentials propagating along these axons. We found that a large number of axons projecting from the CCFM to neurons in Layer V of Cg1 are ensheathed with myelin between pre-adolescence [postnatal day (PD)15] and mid-adolescence (PD43). This robust increase in axonal myelination is accompanied by a near doubling of transmission speed. As there was no age difference in the diameter of these axons, myelin is likely the driving force behind faster transmission of electrical signals in older animals. These developmental changes in axonal microstructure and physiology may extend to other axonal populations as well, and could underlie some of the improvements in cognitive processing between childhood and adolescence.},
	number = {4},
	urldate = {2021-08-16},
	journal = {eNeuro},
	author = {McDougall, Sean and Vargas Riad, Wanette and Silva-Gotay, Andrea and Tavares, Elizabeth R. and Harpalani, Divya and Li, Geng-Lin and Richardson, Heather N.},
	month = sep,
	year = {2018},
	pmid = {30225359},
	pmcid = {PMC6140121},
	note = {00000 },
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {ENEURO.0203--18.2018},
}

@article{eguchi_emergence_nodate,
	title = {The emergence of polychronization and feature binding in a spiking neural network model of the primate ventral visual system.},
	volume = {125},
	issn = {1939-1471},
	url = {https://psycnet.apa.org/fulltext/2018-25960-001.pdf},
	doi = {10.1037/rev0000103},
	number = {4},
	urldate = {2021-11-08},
	journal = {Psychological Review},
	author = {Eguchi, Akihiro and Isbister, James B. and Ahmad, Nasir and Stringer, Simon},
	note = {00000 
Publisher: US: American Psychological Association},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {545},
}

@inproceedings{barbier_spike_2021,
	address = {Nashville, TN, USA},
	title = {Spike timing-based unsupervised learning of orientation, disparity, and motion representations in a spiking neural network},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522727/},
	doi = {10.1109/CVPRW53098.2021.00152},
	abstract = {Neuromorphic vision sensors present unique advantages over their frame based counterparts. However, unsupervised learning of efﬁcient visual representations from their asynchronous output is still a challenge, requiring a rethinking of traditional image and video processing methods. Here we present a network of leaky integrate and ﬁre neurons that learns representations similar to those of simple and complex cells in the primary visual cortex of mammals from the input of two event-based vision sensors. Through the combination of spike timing-dependent plasticity and homeostatic mechanisms, the network learns visual feature detectors for orientation, disparity, and motion in a fully unsupervised fashion. We validate our approach on a mobile robotic platform.},
	language = {en},
	urldate = {2022-04-15},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Barbier, Thomas and Teuliere, Celine and Triesch, Jochen},
	month = jun,
	year = {2021},
	note = {tex.ids= Barbier2021a
ISSN: 2160-7516},
	keywords = {Conferences, Detectors, Neuromorphics, Neurons, Robot sensing systems, Vision sensors, Visualization, ⛔ No INSPIRE recid found},
	pages = {1377--1386},
}

@article{ganguli_memory_2008,
	title = {Memory traces in dynamical systems},
	volume = {105},
	url = {https://doi.org/bq52bt},
	doi = {10.1073/pnas.0804451105},
	abstract = {{\textless}jats:p{\textgreater}
            To perform nontrivial, real-time computations on a sensory input stream, biological systems must retain a short-term memory trace of their recent inputs. It has been proposed that generic high-dimensional dynamical systems could retain a memory trace for past inputs in their current state. This raises important questions about the fundamental limits of such memory traces and the properties required of dynamical systems to achieve these limits. We address these issues by applying Fisher information theory to dynamical systems driven by time-dependent signals corrupted by noise. We introduce the Fisher Memory Curve (FMC) as a measure of the signal-to-noise ratio (SNR) embedded in the dynamical state relative to the input SNR. The integrated FMC indicates the total memory capacity. We apply this theory to linear neuronal networks and show that the capacity of networks with normal connectivity matrices is exactly 1 and that of any network of N neurons is, at most, N. A nonnormal network achieving this bound is subject to stringent design constraints: It must have a hidden feedforward architecture that superlinearly amplifies its input for a time of order N, and the input connectivity must optimally match this architecture. The memory capacity of networks subject to saturating nonlinearities is further limited, and cannot exceed
            {\textless}jats:inline-formula{\textgreater}
              {\textless}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" overflow="scroll"{\textgreater}
                {\textless}mml:mrow{\textgreater}
                  {\textless}mml:msqrt{\textgreater}
                    {\textless}mml:mi{\textgreater}N{\textless}/mml:mi{\textgreater}
                  {\textless}/mml:msqrt{\textgreater}
                {\textless}/mml:mrow{\textgreater}
              {\textless}/mml:math{\textgreater}
            {\textless}/jats:inline-formula{\textgreater}
            . This limit can be realized by feedforward structures with divergent fan out that distributes the signal across neurons, thereby avoiding saturation. We illustrate the generality of the theory by showing that memory in fluid systems can be sustained by transient nonnormal amplification due to convective instability or the onset of turbulence.
          {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {48},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ganguli, Surya and Huh, Dongsung and Sompolinsky, Haim},
	year = {2008},
	note = {This CSL Item was generated by Manubot v0.5.2 from its persistent identifier (standard\_id).
standard\_id: doi:10.1073/pnas.0804451105},
	keywords = {⛔ No INSPIRE recid found},
	pages = {18970--18975},
}

@misc{vicente-sola_evaluating_2022,
	title = {Evaluating the temporal understanding of neural networks on event-based action recognition with {DVS}-{Gesture}-{Chain}},
	url = {http://arxiv.org/abs/2209.14915},
	doi = {10.48550/arXiv.2209.14915},
	abstract = {Enabling artificial neural networks (ANNs) to have temporal understanding in visual tasks is an essential requirement in order to achieve complete perception of video sequences. A wide range of benchmark datasets is available to allow for the evaluation of such capabilities when using conventional frame-based video sequences. In contrast, evaluating them for systems targeting neuromorphic data is still a challenge due to the lack of appropriate datasets. In this work we define a new benchmark task for action recognition in event-based video sequences, DVS-Gesture-Chain (DVS-GC), which is based on the temporal combination of multiple gestures from the widely used DVS-Gesture dataset. This methodology allows to create datasets that are arbitrarily complex in the temporal dimension. Using our newly defined task, we evaluate the spatio-temporal understanding of different feed-forward convolutional ANNs and convolutional Spiking Neural Networks (SNNs). Our study proves how the original DVS Gesture benchmark could be solved by networks without temporal understanding, unlike the new DVS-GC which demands an understanding of the ordering of events. From there, we provide a study showing how certain elements such as spiking neurons or time-dependent weights allow for temporal understanding in feed-forward networks without the need for recurrent connections. Code available at: https://github.com/VicenteAlex/DVS-Gesture-Chain},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Vicente-Sola, Alex and Manna, Davide L. and Kirkland, Paul and Di Caterina, Gaetano and Bihl, Trevor},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, D.2.13, I.2.10, I.2.6, I.4.8, I.5.2, ⛔ No INSPIRE recid found},
}

@article{takemura_visual_2014,
	title = {A visual motion detection circuit suggested by {Drosophila} connectomics},
	volume = {500},
	doi = {10.1038/nature12450},
	number = {7461},
	journal = {Nature},
	author = {Takemura, Shin-ya and Bharioke, Arjun and Lu, Zhiyuan and Nern, Aljoscha and Vitaladevuni, Shiv and Rivlin, Patricia K and Katz, William T and Olbris, Donald J and Plaza, Stephen M and Winston, Philip and Zhao, Ting and Horne, Jane Anne and Fetter, Richard D and Takemura, Satoko and Blazek, Katerina and Chang, Lei-Ann and Ogundeyi, Omotara and Saunders, Mathew A and Shapiro, Victor and Sigmund, Christopher and Rubin, Gerald M and Scheffer, Louis K and Meinertzhagen, Ian A and Chklovskii, Dmitri B},
	month = apr,
	year = {2014},
	note = {00366
tex.ids= Takemura13
number: 7461
publisher: Nature Publishing Group},
	keywords = {⛔ No INSPIRE recid found},
	pages = {175--181},
}

@article{puchalla_redundancy_2005,
	title = {Redundancy in the {Population} {Code} of the {Retina}},
	volume = {46},
	doi = {10.1016/j.neuron.2005.03.026},
	number = {3},
	journal = {Neuron},
	author = {Puchalla, Jason L and Schneidman, Elad and Harris, Robert A and Berry, Michael J},
	month = may,
	year = {2005},
	pmid = {15882648},
	note = {tex.ids= Puchalla05, Puchalla2005a
publisher: Elsevier},
	keywords = {⛔ No INSPIRE recid found},
	pages = {493--504},
}

@article{vyas_computation_2020,
	title = {Computation {Through} {Neural} {Population} {Dynamics}},
	volume = {43},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-092619-094115},
	doi = {10.1146/annurev-neuro-092619-094115},
	abstract = {Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework.},
	language = {en},
	number = {1},
	urldate = {2021-10-12},
	journal = {Annual Review of Neuroscience},
	author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
	month = jul,
	year = {2020},
	note = {120 citations (Crossref) [2022-09-03]
00000
tex.ids= Vyas2020a},
	keywords = {⛔ No INSPIRE recid found},
	pages = {249--275},
}

@article{orban_neural_2016,
	title = {Neural {Variability} and {Sampling}-{Based} {Probabilistic} {Representations} in the {Visual} {Cortex}},
	volume = {92},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(16)30639-0},
	doi = {10.1016/j.neuron.2016.09.038},
	language = {English},
	number = {2},
	urldate = {2018-09-10},
	journal = {Neuron},
	author = {Orbán, Gergő and Berkes, Pietro and Fiser, József and Lengyel, Máté},
	month = oct,
	year = {2016},
	pmid = {27764674},
	note = {00038
tex.ids= Orban2016},
	keywords = {⛔ No INSPIRE recid found},
	pages = {530--543},
}

@article{chrol-cannon_efficient_2017,
	title = {An efficient method for online detection of polychronous patterns in spiking neural networks},
	volume = {267},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231217311530},
	doi = {10.1016/j.neucom.2017.06.025},
	abstract = {Polychronous neural groups are effective structures for the recognition of precise spike-timing patterns but the detection method is an inefficient multi-stage brute force process that works off-line on pre-recorded simulation data. This work presents a new model of polychronous patterns that can capture precise sequences of spikes directly in the neural simulation. In this scheme, each neuron is assigned a randomized code that is used to tag the post-synaptic neurons whenever a spike is transmitted. This creates a polychronous code that preserves the order of pre-synaptic activity and can be registered in a hash table when the post-synaptic neuron spikes. A polychronous code is a sub-component of a polychronous group that will occur, along with others, when the group is active. We demonstrate the representational and pattern recognition ability of polychronous codes on a direction selective visual task involving moving bars that is typical of a computation performed by simple cells in the cortex. By avoiding the structural and temporal analyses of polychronous group detection methods, the computational efficiency of the proposed algorithm is improved for pattern recognition by almost four orders of magnitude and is well suited for online detection.},
	language = {en},
	urldate = {2021-11-08},
	journal = {Neurocomputing},
	author = {Chrol-Cannon, Joseph and Jin, Yaochu and Grüning, André},
	month = dec,
	year = {2017},
	note = {00000},
	keywords = {⛔ No INSPIRE recid found},
	pages = {644--650},
}

@article{comsa_temporal_2020,
	title = {Temporal {Coding} in {Spiking} {Neural} {Networks} with {Alpha} {Synaptic} {Function}: {Learning} with {Backpropagation}},
	shorttitle = {Temporal {Coding} in {Spiking} {Neural} {Networks} with {Alpha} {Synaptic} {Function}},
	url = {http://arxiv.org/abs/1907.13223},
	abstract = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artiﬁcial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual spikes. In classiﬁcation tasks, the output of the network is indicated by the ﬁrst neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically-plausible alpha synaptic transfer function. Additionally, we use trainable synchronisation pulses that provide bias, add ﬂexibility during training and exploit the decay part of the alpha function. We show that such networks can be successfully trained on noisy Boolean logic tasks and on the MNIST dataset encoded in time. We show that the spiking neural network outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. The spiking network spontaneously discovers two operating modes, mirroring the accuracy-speed trade-oﬀ observed in human decision-making: a highly accurate but slow regime, and a fast but slightly lower-accuracy regime. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks towards energy-eﬃcient, state-based and more complex biologically-inspired neural architectures.},
	language = {en},
	urldate = {2021-01-14},
	journal = {arXiv:1907.13223 [cs, q-bio]},
	author = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
	month = nov,
	year = {2020},
	pmid = {33900924},
	note = {00000 
tex.ids= Comsa2021
arXiv: 1907.13223},
	keywords = {⛔ No INSPIRE recid found},
}

@article{davey_impact_2020,
	title = {Impact of axonal delay on structure development in a multi-layered network},
	url = {http://arxiv.org/abs/1805.03792},
	abstract = {The mechanisms underlying how activity in the visual pathway may give rise through neural plasticity to many of the features observed experimentally in the early stages of visual processing was provided by Linkser in a seminal, three-paper series. Owing to the complexity of multi-layer models, an implicit assumption in Linsker's and subsequent papers has been that propagation delay is homogeneous and plays little functional role in neural behaviour. We relax this assumption to examine the impact of distance-dependent axonal propagation delay on neural learning. We show that propagation delay induces low-pass filtering by dispersing the arrival times of spikes from presynaptic neurons, providing a natural correlation cancellation mechanism for distal connections. The cut-off frequency decreases as the radial propagation delay within a layer increases relative to propagation delay between the layers, introducing an upper limit on temporal resolution. Given that the PSP also acts as a low-pass filter, we show that the effective time constant of each should enable the processing of similar scales of temporal information. This result has implications for the visual system, in which receptive field size and, thus, radial propagation delay, increases with eccentricity. Furthermore, the network response is frequency dependent since higher frequencies require increased input amplitude to compensate for attenuation. This concords with frequency-dependent contrast sensitivity in the visual system, which changes with eccentricity and receptive field size. We further show that the proportion of inhibition relative to excitation is larger where radial propagation delay is long relative to inter-laminar propagation delay. We show that the addition of propagation delay reduces the range in the cell's on-center size, providing stability to variations in homeostatic parameters.},
	urldate = {2021-08-06},
	journal = {arXiv:1805.03792 [q-bio]},
	author = {Davey, Catherine E. and Grayden, David B. and Burkitt, Anthony N.},
	month = nov,
	year = {2020},
	note = {00000 
arXiv: 1805.03792},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@article{senn_activity-dependent_2002,
	title = {Activity-{Dependent} {Development} of {Axonal} and {Dendritic} {Delays}, or, {Why} {Synaptic} {Transmission} {Should} {Be} {Unreliable}},
	volume = {14},
	issn = {0899-7667},
	doi = {10.1162/089976602317250915},
	abstract = {Systematic temporal relations between single neuronal activities or population activities are ubiquitous in the brain. No experimental evidence, however, exists for a direct modification of neuronal delays during Hebbian-type stimulation protocols. We show that in fact an explicit delay adaptation is not needed if one assumes that the synaptic strengths are modified according to the recently observed temporally asymmetric learning rule with the downregulating branch dominating the upregulating branch. During development, slow, unbiased fluctuations in the transmission time, together with temporally correlated network activity, may control neural growth and implicitly induce drifts in the axonal delays and dendritic latencies. These delays and latencies become optimally tuned in the sense that the synaptic response tends to peak in the soma of the postsynaptic cell if this is most likely to fire. The nature of the selection process requires unreliable synapses in order to give successful synapses an evolutionary advantage over the others. The width of the learning function also determines the preferred dendritic delay and the preferred width of the postsynaptic response. Hence, it may implicitly determine whether a synaptic connection provides a precisely timed or a broadly tuned “contextual” signal.},
	number = {3},
	journal = {Neural Computation},
	author = {Senn, W. and Schneider, M. and Ruf, B.},
	month = mar,
	year = {2002},
	note = {00059 
Conference Name: Neural Computation},
	keywords = {delay-learning, review, ⛔ No INSPIRE recid found},
	pages = {583--619},
}

@article{wunderlich_event-based_2021,
	title = {Event-based backpropagation can compute exact gradients for spiking neural networks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-91786-z},
	doi = {10.1038/s41598-021-91786-z},
	abstract = {Spiking neural networks combine analog computation with event-based communication using discrete spikes. While the impressive advances of deep learning are enabled by training non-spiking artificial neural networks using the backpropagation algorithm, applying this algorithm to spiking networks was previously hindered by the existence of discrete spike events and discontinuities. For the first time, this work derives the backpropagation algorithm for a continuous-time spiking neural network and a general loss function by applying the adjoint method together with the proper partial derivative jumps, allowing for backpropagation through discrete spike events without approximations. This algorithm, EventProp, backpropagates errors at spike times in order to compute the exact gradient in an event-based, temporally and spatially sparse fashion. We use gradients computed via EventProp to train networks on the Yin-Yang and MNIST datasets using either a spike time or voltage based loss function and report competitive performance. Our work supports the rigorous study of gradient-based learning algorithms in spiking neural networks and provides insights toward their implementation in novel brain-inspired hardware.},
	language = {en},
	number = {1},
	urldate = {2021-07-05},
	journal = {Scientific Reports},
	author = {Wunderlich, Timo C. and Pehle, Christian},
	month = jun,
	year = {2021},
	note = {15 citations (Crossref) [2022-09-03]
00000 
Cc\_license\_type: cc\_by
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Learning algorithms;Machine learning;Mathematics and computing
Subject\_term\_id: learning-algorithms;machine-learning;mathematics-and-computing},
	keywords = {TOREAD, ⛔ No INSPIRE recid found},
	pages = {1--17},
}

@inproceedings{gibson_predicting_2014,
	title = {Predicting temporal sequences using an event-based spiking neural network incorporating learnable delays},
	doi = {10.1109/ijcnn.2014.6889850},
	abstract = {This paper presents a novel paradigm for a spiking neural network to forecast temporal sequences. The key to the approach is a new model of a spiking neuron that can make multi-step predictions, using learnable temporal delays at both dendrites and axons. This model is able to learn the temporal structure of space-time events, adaptable to multiple scales, with the neurons able to function asynchronously to predict future events in a video sequence. This approach contrasts with conventional neural network approaches that use fixed time steps and iterative prediction. Simulations were conducted to compare the new model to a conventional iterative paradigm on motion sequences from a frame-free event-driven Dynamic Vision Sensor (DVS128, 16k pixels), showing that the new approach consistently has a low prediction error while the iterative paradigm is affected by propagated errors.},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Gibson, Tingting Amy and Henderson, James A. and Wiles, Janet},
	month = jul,
	year = {2014},
	note = {00000 
ISSN: 2161-4407},
	keywords = {⛔ No INSPIRE recid found},
	pages = {3213--3220},
}

@article{auge_survey_2021,
	title = {A {Survey} of {Encoding} {Techniques} for {Signal} {Processing} in {Spiking} {Neural} {Networks}},
	volume = {53},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-021-10562-2},
	doi = {10.1007/s11063-021-10562-2},
	abstract = {Biologically inspired spiking neural networks are increasingly popular in the field of artificial intelligence due to their ability to solve complex problems while being power efficient. They do so by leveraging the timing of discrete spikes as main information carrier. Though, industrial applications are still lacking, partially because the question of how to encode incoming data into discrete spike events cannot be uniformly answered. In this paper, we summarise the signal encoding schemes presented in the literature and propose a uniform nomenclature to prevent the vague usage of ambiguous definitions. Therefore we survey both, the theoretical foundations as well as applications of the encoding schemes. This work provides a foundation in spiking signal encoding and gives an overview over different application-oriented implementations which utilise the schemes.},
	language = {en},
	number = {6},
	urldate = {2022-01-25},
	journal = {Neural Processing Letters},
	author = {Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
	month = dec,
	year = {2021},
	note = {00001},
	keywords = {⛔ No INSPIRE recid found},
	pages = {4693--4710},
}

@article{behnia_processing_2014,
	title = {Processing properties of {ON} and {OFF} pathways for {Drosophila} motion detection},
	volume = {512},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243710/},
	doi = {10.1038/nature13427},
	abstract = {The algorithms and neural circuits that process spatiotemporal changes in luminance to extract visual motion cues have been the focus of intense research. An influential model, the Hassenstein-Reichardt correlator (HRC), relies on differential temporal filtering of two spatially separated input channels, delaying one input signal with respect to the other. Motion in a particular direction causes these delayed and non-delayed luminance signals to arrive simultaneously at a subsequent processing step in the brain; these signals are then nonlinearly amplified to produce a direction-selective response (). Recent work in Drosophila has identified two parallel pathways that selectively respond to either moving light or dark edges,. Each of these pathways requires two critical processing steps to be applied to incoming signals: differential delay between the spatial input channels, and distinct processing of brightness increment and decrement signals. Using in vivo patch-clamp recordings, we demonstrate that four medulla neurons implement these two processing steps. The neurons Mi1 and Tm3 respond selectively to brightness increments, with the response of Mi1 delayed relative to Tm3. Conversely, Tm1 and Tm2 respond selectively to brightness decrements, with the response of Tm1 delayed compared to Tm2. Remarkably, constraining HRC models using these measurements produces outputs consistent with previously measured properties of motion detectors, including temporal frequency tuning and specificity for light vs. dark edges. We propose that Mi1 and Tm3 perform critical processing of the delayed and non-delayed input channels of the correlator responsible for the detection of light edges, while Tm1 and Tm2 play analogous roles in the detection of moving dark edges. Our data shows that specific medulla neurons possess response properties that allow them to implement the algorithmic steps that precede the correlative operation in the HRC, revealing elements of the long-sought neural substrates of motion detection in the fly.},
	number = {7515},
	urldate = {2022-02-11},
	journal = {Nature},
	author = {Behnia, Rudy and Clark, Damon A. and Carter, Adam G. and Clandinin, Thomas R. and Desplan, Claude},
	month = aug,
	year = {2014},
	pmid = {25043016},
	pmcid = {PMC4243710},
	keywords = {⛔ No INSPIRE recid found},
	pages = {427--430},
}

@article{king_human_2021,
	title = {The {Human} {Brain} {Encodes} a {Chronicle} of {Visual} {Events} at {Each} {Instant} of {Time} {Through} the {Multiplexing} of {Traveling} {Waves}},
	volume = {41},
	copyright = {Copyright © 2021 King and Wyart. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/41/34/7224},
	doi = {10.1523/jneurosci.2098-20.2021},
	abstract = {The human brain continuously processes streams of visual input. Yet, a single image typically triggers neural responses that extend beyond 1s. To understand how the brain encodes and maintains successive images, we analyzed with electroencephalography the brain activity of human subjects while they watched ∼5000 visual stimuli presented in fast sequences. First, we confirm that each stimulus can be decoded from brain activity for ∼1s, and we demonstrate that the brain simultaneously represents multiple images at each time instant. Second, we source localize the corresponding brain responses in the expected visual hierarchy and show that distinct brain regions represent, at each time instant, different snapshots of past stimulations. Third, we propose a simple framework to further characterize the dynamical system of these traveling waves. Our results show that a chain of neural circuits, which each consist of (1) a hidden maintenance mechanism and (2) an observable update mechanism, accounts for the dynamics of macroscopic brain representations elicited by visual sequences. Together, these results detail a simple architecture explaining how successive visual events and their respective timings can be simultaneously represented in the brain.
SIGNIFICANCE STATEMENT Our retinas are continuously bombarded with a rich flux of visual input. Yet, how our brain continuously processes such visual streams is a major challenge to neuroscience. Here, we developed techniques to decode and track, from human brain activity, multiple images flashed in rapid succession. Our results show that the brain simultaneously represents multiple successive images at each time instant by multiplexing them along a neural cascade. Dynamical modeling shows that these results can be explained by a hierarchy of neural assemblies that continuously propagate multiple visual contents. Overall, this study sheds new light on the biological basis of our visual experience.},
	language = {en},
	number = {34},
	urldate = {2022-01-19},
	journal = {Journal of Neuroscience},
	author = {King, Jean-Rémi and Wyart, Valentin},
	month = aug,
	year = {2021},
	pmid = {33811150},
	note = {00002 
Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {EEG, decoding, dynamical system, streams, time, visual perception, ⛔ No INSPIRE recid found},
	pages = {7224--7233},
}

@article{zhao_understanding_2011,
	title = {Understanding {Auditory} {Spectro}-{Temporal} {Receptive} {Fields} and {Their} {Changes} with {Input} {Statistics} by {Efficient} {Coding} {Principles}},
	volume = {7},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3158037/},
	doi = {10.1371/journal.pcbi.1002123},
	abstract = {Spectro-temporal receptive fields (STRFs) have been widely used as linear approximations to the signal transform from sound spectrograms to neural responses along the auditory pathway. Their dependence on statistical attributes of the stimuli, such as sound intensity, is usually explained by nonlinear mechanisms and models. Here, we apply an efficient coding principle which has been successfully used to understand receptive fields in early stages of visual processing, in order to provide a computational understanding of the STRFs. According to this principle, STRFs result from an optimal tradeoff between maximizing the sensory information the brain receives, and minimizing the cost of the neural activities required to represent and transmit this information. Both terms depend on the statistical properties of the sensory inputs and the noise that corrupts them. The STRFs should therefore depend on the input power spectrum and the signal-to-noise ratio, which is assumed to increase with input intensity. We analytically derive the optimal STRFs when signal and noise are approximated as Gaussians. Under the constraint that they should be spectro-temporally local, the STRFs are predicted to adapt from being band-pass to low-pass filters as the input intensity reduces, or the input correlation becomes longer range in sound frequency or time. These predictions qualitatively match physiological observations. Our prediction as to how the STRFs should be determined by the input power spectrum could readily be tested, since this spectrum depends on the stimulus ensemble. The potentials and limitations of the efficient coding principle are discussed., Spectro-temporal receptive fields (STRFs) have been widely used as linear approximations of the signal transform from sound spectrograms to neural responses along the auditory pathway. Their dependence on the ensemble of input stimuli has usually been examined mechanistically as a possibly complex nonlinear process. We propose that the STRFs and their dependence on the input ensemble can be understood by an efficient coding principle, according to which the responses of the encoding neurons report the maximum amount of information about the sensory input, subject to limits on the neural cost in representing and transmitting information. This proposal is inspired by the success of the same principle in accounting for receptive fields in the early stages of the visual pathway and their adaptation to input statistics. The principle can account for the STRFs that have been observed, and the way they change with sound intensity. Further, it predicts how the STRFs should change with input correlations, an issue that has not been extensively investigated. In sum, our study provides a computational understanding of the neural transformations of auditory inputs, and makes testable predictions for future experiments.},
	number = {8},
	urldate = {2022-03-08},
	journal = {PLoS Computational Biology},
	author = {Zhao, Lingyun and Zhaoping, Li},
	month = aug,
	year = {2011},
	pmcid = {PMC3158037},
	pmid = {21887121},
	note = {tex.ids= Zhao2011},
	keywords = {⛔ No INSPIRE recid found},
	pages = {e1002123},
}

@article{engel_direct_1991,
	title = {Direct physiological evidence for scene segmentation by temporal coding.},
	volume = {88},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/88/20/9136},
	doi = {10.1073/pnas.88.20.9136},
	abstract = {Theoretical studies have suggested that scene segmentation may be accomplished by a temporal coding mechanism using synchronization of neuronal responses. Here we report a direct experimental test of this hypothesis. Neuronal responses were recorded simultaneously from two to four sites with overlapping receptive fields in cat visual cortex. Correlation analysis revealed that all cells synchronized their responses irrespective of their orientation preference when they were activated by a single light bar. However, when stimulated with two superimposed light bars of different orientations, the same cells segregated into distinct assemblies according to their orientation preferences. Within each of these assemblies responses were synchronized, but correlation was absent between the two assemblies. These results are compatible with the hypothesis that responses to individual objects in a scene are distinguished by synchrony, whereas responses to different objects show no temporal correlation, thus allowing for the segmentation of superimposed stimuli. We conclude that stimulus-specific synchronization of spatially distributed neuronal responses may provide a physiological mechanism for scene segmentation.},
	language = {en},
	number = {20},
	urldate = {2022-01-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Engel, A. K. and König, P. and Singer, W.},
	month = oct,
	year = {1991},
	pmid = {1924376},
	note = {00502 
Publisher: National Academy of Sciences
Section: Research Article},
	keywords = {⛔ No INSPIRE recid found},
	pages = {9136--9140},
}

@article{yang_elementary_2018,
	title = {Elementary {Motion} {Detection} in {Drosophila}: {Algorithms} and {Mechanisms}},
	volume = {4},
	issn = {2374-4642},
	shorttitle = {Elementary {Motion} {Detection} in {Drosophila}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8097889/},
	doi = {10.1146/annurev-vision-091517-034153},
	abstract = {Motion in the visual world provides critical information to guide the behavior of sighted animals. Furthermore, as visual motion estimation requires comparisons of signals across inputs and over time, it represents a paradigmatic and generalizable neural computation. Focusing on the Drosophila visual system, where an explosion of technological advances has recently accelerated experimental progress, we review our understanding of how, algorithmically and mechanistically, motion signals are first computed.},
	urldate = {2022-02-11},
	journal = {Annual review of vision science},
	author = {Yang, Helen H. and Clandinin, Thomas R.},
	month = sep,
	year = {2018},
	pmid = {29949723},
	pmcid = {PMC8097889},
	keywords = {⛔ No INSPIRE recid found},
	pages = {143--163},
}

@article{woods_factors_2015,
	title = {Factors influencing the latency of simple reaction time},
	volume = {9},
	issn = {1662-5161},
	url = {http://www.frontiersin.org/Human_Neuroscience/10.3389/fnhum.2015.00131/abstract},
	doi = {10.3389/fnhum.2015.00131},
	abstract = {Simple reaction time (SRT), the minimal time needed to respond to a stimulus, is a basic measure of processing speed. SRTs were ﬁrst measured by Francis Galton in the 19th century, who reported visual SRT latencies below 190 ms in young subjects. However, recent large-scale studies have reported substantially increased SRT latencies that differ markedly in different laboratories, in part due to timing delays introduced by the computer hardware and software used for SRT measurement. We developed a calibrated and temporally precise SRT test to analyze the factors that inﬂuence SRT latencies in a paradigm where visual stimuli were presented to the left or right hemiﬁeld at varying stimulus onset asynchronies (SOAs). Experiment 1 examined a community sample of 1469 subjects ranging in age from 18 to 65. Mean SRT latencies were short (231, 213 ms when corrected for hardware delays) and increased signiﬁcantly with age (0.55 ms/year), but were unaffected by sex or education. As in previous studies, SRTs were prolonged at shorter SOAs and were slightly faster for stimuli presented in the visual ﬁeld contralateral to the responding hand. Stimulus detection time (SDT) was estimated by subtracting movement initiation time, measured in a speeded ﬁnger tapping test, from SRTs. SDT latencies averaged 131 ms and were unaffected by age. Experiment 2 tested 189 subjects ranging in age from 18 to 82 years in a different laboratory using a larger range of SOAs. Both SRTs and SDTs were slightly prolonged (by 7 ms). SRT latencies increased with age while SDT latencies remained stable. Precise computer-based measurements of SRT latencies show that processing speed is as fast in contemporary populations as in the Victorian era, and that age-related increases in SRT latencies are due primarily to slowed motor output.},
	language = {en},
	urldate = {2022-11-17},
	journal = {Frontiers in Human Neuroscience},
	author = {Woods, David L. and Wyma, John M. and Yund, E. William and Herron, Timothy J. and Reed, Bruce},
	month = mar,
	year = {2015},
	keywords = {⛔ No INSPIRE recid found},
}

@article{yarbus_eye_1961,
	title = {Eye movements during the examination of complicated objects},
	volume = {6(2)},
	issn = {0006-3029},
	journal = {Biofizika},
	author = {Yarbus, A},
	year = {1961},
	pmid = {14040367},
	keywords = {EYE, Eye, Eye Movements, Humans, ⛔ No INSPIRE recid found},
	pages = {52--56},
}

@article{fakche__2022,
	title = {α {Phase}-{Amplitude} {Tradeoffs} {Predict} {Visual} {Perception}},
	volume = {9},
	issn = {2373-2822},
	url = {https://www.eneuro.org/lookup/doi/10.1523/ENEURO.0244-21.2022},
	doi = {10.1523/ENEURO.0244-21.2022},
	language = {en},
	number = {1},
	urldate = {2022-03-26},
	journal = {eneuro},
	author = {Fakche, Camille and VanRullen, Rufin and Marque, Philippe and Dugué, Laura},
	month = jan,
	year = {2022},
	keywords = {EEG, TMS, cortical excitability, phase-amplitude tradeoffs, visual perception, α oscillations, ⛔ No INSPIRE recid found},
	pages = {ENEURO.0244--21.2022},
}

@techreport{grimaldi_precise_2022,
	title = {Precise {Spiking} {Motifs} in {Neurobiological} and {Neuromorphic} {Dat}},
	url = {https://doi.org/10.20944/preprints202211.0332.v1},
	abstract = {Why do neurons communicate through spikes? By definition, spikes are all-or-none neural events which occur at continuous times. In other words, spikes are on one side binary, existing or not without further details, and on the other can occur at any asynchronous time, without the need for a centralized clock. This stands in stark contrast to the analog representation of values and the discretized timing classically used in digital processing and at the base of modern-day neural networks. As neural systems almost systematically use this so-called event-based representation in the living world, a better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the profusion of recorded data. With the growing need for intelligent embedded systems, it also emerges as a new computing paradigm to enable the efficient operation of a new class of sensors and event-based computers, called neuromorphic, which could enable significant gains in computation time and energy consumption \&mdash; a major societal issue in the era of the digital economy and global warming. In this review paper, we provide evidence from biology, theory and engineering that the precise timing of spikes plays a crucial role in our understanding of the efficiency of neural networks.},
	urldate = {2022-12-12},
	author = {Grimaldi, Antoine and Gruel, Amélie and Besnainou, Camille and Martinet, Jean and Perrinet, Laurent Udo},
	year = {2022},
	doi = {10.20944/preprints202211.0332.v1},
	note = {Type: article},
	keywords = {⛔ No INSPIRE recid found},
}

@article{zambrano_sparse_2019,
	title = {Sparse {Computation} in {Adaptive} {Spiking} {Neural} {Networks}},
	volume = {12},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00987},
	doi = {10.3389/fnins.2018.00987},
	abstract = {Artificial Neural Networks (ANNs) are bio-inspired models of neural computation that have proven highly effective. Still, ANNs lack a natural notion of time, and neural units in ANNs exchange analog values in a frame-based manner, a computationally and energetically inefficient form of communication. This contrasts sharply with biological neurons that communicate sparingly and efficiently using isomorphic binary spikes. While Spiking Neural Networks (SNNs) can be constructed by replacing the units of an ANN with spiking neurons (Cao et al., 2015; Diehl et al., 2015) to obtain reasonable performance, these SNNs use Poisson spiking mechanisms with exceedingly high firing rates compared to their biological counterparts. Here we show how spiking neurons that employ a form of neural coding can be used to construct SNNs that match high-performance ANNs and match or exceed state-of-the-art in SNNs on important benchmarks, while requiring firing rates compatible with biological findings. For this, we use spike-based coding based on the firing rate limiting adaptation phenomenon observed in biological spiking neurons. This phenomenon can be captured in fast adapting spiking neuron models, for which we derive the effective transfer function. Neural units in ANNs trained with this transfer function can be substituted directly with adaptive spiking neurons, and the resulting Adaptive SNNs (AdSNNs) can carry out competitive classification in deep neural networks without further modifications. Adaptive spike-based coding additionally allows for the dynamic control of neural coding precision: we show empirically how a simple model of arousal in AdSNNs further halves the average required firing rate and this notion naturally extends to other forms of attention as studied in neuroscience. AdSNNs thus hold promise as a novel and sparsely active model for neural computation that naturally fits to temporally continuous and asynchronous applications.},
	urldate = {2022-12-13},
	journal = {Frontiers in Neuroscience},
	author = {Zambrano, Davide and Nusselder, Roeland and Scholte, H. Steven and Bohté, Sander M.},
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
}

@misc{perrinet_khoei_2017_ploscb_2021,
	title = {Khoei\_2017\_PLoSCB},
	copyright = {MIT},
	url = {https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/1971e39d6dc980d5bd0d52f76400e3f2d33c3160/figures/MBP_dot_spatial_readout.mp4},
	abstract = {Code to replicate paper “The flash-lag effect as a motion-based predictive shift”},
	urldate = {2022-11-18},
	author = {Perrinet, Laurent},
	month = feb,
	year = {2021},
	note = {original-date: 2016-07-25T13:24:06Z},
	keywords = {⛔ No INSPIRE recid found},
}

@article{paugam-moisy_delay_2008,
	series = {Progress in {Modeling}, {Theory}, and {Application} of {Computational} {Intelligenc}},
	title = {Delay learning and polychronization for reservoir computing},
	volume = {71},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231208000507},
	doi = {10.1016/j.neucom.2007.12.027},
	abstract = {We propose a multi-timescale learning rule for spiking neuron networks, in the line of the recently emerging field of reservoir computing. The reservoir is a network model of spiking neurons, with random topology and driven by STDP (spike-time-dependent plasticity), a temporal Hebbian unsupervised learning mode, biologically observed. The model is further driven by a supervised learning algorithm, based on a margin criterion, that affects the synaptic delays linking the network to the readout neurons, with classification as a goal task. The network processing and the resulting performance can be explained by the concept of polychronization, proposed by Izhikevich [Polychronization: computation with spikes, Neural Comput. 18(2) (2006) 245–282], on physiological grounds. The model emphasizes that polychronization can be used as a tool for exploiting the computational power of synaptic delays and for monitoring the topology and activity of a spiking neuron network.},
	language = {en},
	number = {7},
	urldate = {2020-01-09},
	journal = {Neurocomputing},
	author = {Paugam-Moisy, Hélène and Martinez, Régis and Bengio, Samy},
	month = mar,
	year = {2008},
	note = {00129
tex.ids= Paugam-Moisy2008a},
	keywords = {SNN, Spiking Neural Networks (SNN), delay-learning, polychronization, reservoir-computing, supervised, ⛔ No INSPIRE recid found},
	pages = {1143--1158},
}

@article{alemi_learning_2018,
	title = {Learning {Nonlinear} {Dynamics} in {Efficient}, {Balanced} {Spiking} {Networks} {Using} {Local} {Plasticity} {Rules}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11320},
	doi = {10.1609/aaai.v32i1.11320},
	abstract = {The brain uses spikes in neural circuits to perform many dynamical computations. The computations are performed with properties such as spiking efficiency, i.e. minimal number of spikes, and robustness to noise. A major obstacle for learning computations in artificial spiking neural networks with such desired biological properties is due to lack of our understanding of how biological spiking neural networks learn computations. Here, we consider the credit assignment problem, i.e. determining the local contribution of each synapse to the network's global output error, for learning nonlinear dynamical computations in a spiking network with the desired properties of biological networks. We approach this problem by fusing the theory of efficient, balanced neural networks (EBN) with nonlinear adaptive control theory to propose a local learning rule. Locality of learning rules are ensured by feeding back into the network its own error, resulting in a learning rule depending solely on presynaptic inputs and error feedbacks. The spiking efficiency and robustness of the network are guaranteed by maintaining a tight excitatory/inhibitory balance, ensuring that each spike represents a local projection of the global output error and minimizes a loss function. The resulting networks can learn to implement complex dynamics with very small numbers of neurons and spikes, exhibit the same spike train variability as observed experimentally, and are extremely robust to noise and neuronal loss.},
	language = {en},
	number = {1},
	urldate = {2022-02-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Alemi, Alireza and Machens, Christian and Deneve, Sophie and Slotine, Jean-Jacques},
	month = apr,
	year = {2018},
	note = {tex.ids= Alemi},
	keywords = {Dynamical Systems, E-I Balance, Efficiency, Learning, Local Plasticity Rules, Nonlinear Dynamics, Robustness, Spiking Neural Networks, ⛔ No INSPIRE recid found},
}

@article{almeida_myelinated_2017,
	title = {On {Myelinated} {Axon} {Plasticity} and {Neuronal} {Circuit} {Formation} and {Function}},
	volume = {37},
	copyright = {Copyright © 2017 the authors 0270-6474/17/3710023-12\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/37/42/10023},
	doi = {10.1523/jneurosci.3185-16.2017},
	abstract = {Studies of activity-driven nervous system plasticity have primarily focused on the gray matter. However, MRI-based imaging studies have shown that white matter, primarily composed of myelinated axons, can also be dynamically regulated by activity of the healthy brain. Myelination in the CNS is an ongoing process that starts around birth and continues throughout life. Myelin in the CNS is generated by oligodendrocytes and recent evidence has shown that many aspects of oligodendrocyte development and myelination can be modulated by extrinsic signals including neuronal activity. Because modulation of myelin can, in turn, affect several aspects of conduction, the concept has emerged that activity-regulated myelination represents an important form of nervous system plasticity. Here we review our increasing understanding of how neuronal activity regulates oligodendrocytes and myelinated axons in vivo, with a focus on the timing of relevant processes. We highlight the observations that neuronal activity can rapidly tune axonal diameter, promote re-entry of oligodendrocyte progenitor cells into the cell cycle, or drive their direct differentiation into oligodendrocytes. We suggest that activity-regulated myelin formation and remodeling that significantly change axonal conduction properties are most likely to occur over timescales of days to weeks. Finally, we propose that precise fine-tuning of conduction along already-myelinated axons may also be mediated by alterations to the axon itself. We conclude that future studies need to analyze activity-driven adaptations to both axons and their myelin sheaths to fully understand how myelinated axon plasticity contributes to neuronal circuit formation and function.},
	language = {en},
	number = {42},
	urldate = {2021-08-16},
	journal = {Journal of Neuroscience},
	author = {Almeida, Rafael G. and Lyons, David A.},
	month = oct,
	year = {2017},
	pmid = {29046438},
	note = {00000 
Publisher: Society for Neuroscience
Section: Viewpoints},
	keywords = {⛔ No INSPIRE recid found},
	pages = {10023--10034},
}

@article{burger_inhibition_2011,
	title = {Inhibition in the balance: binaurally coupled inhibitory feedback in sound localization circuitry},
	volume = {106},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Inhibition in the balance},
	url = {https://www.physiology.org/doi/10.1152/jn.00205.2011},
	doi = {10.1152/jn.00205.2011},
	abstract = {Interaural time differences (ITDs) are the primary cue animals, including humans, use to localize low-frequency sounds. In vertebrate auditory systems, dedicated ITD processing neural circuitry performs an exacting task, the discrimination of microsecond differences in stimulus arrival time at the two ears by coincidence-detecting neurons. These neurons modulate responses over their entire dynamic range to sounds differing in ITD by mere hundreds of microseconds. The well-understood function of this circuitry in birds has provided a fruitful system to investigate how inhibition contributes to neural computation at the synaptic, cellular, and systems level. Our recent studies in the chicken have made significant progress in bringing together many of these findings to provide a cohesive picture of inhibitory function.},
	language = {en},
	number = {1},
	urldate = {2021-03-22},
	journal = {Journal of Neurophysiology},
	author = {Burger, R. Michael and Fukui, Iwao and Ohmori, Harunori and Rubel, Edwin W.},
	month = jul,
	year = {2011},
	note = {00000},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {4--14},
}

@article{gerstner_why_1993,
	title = {Why spikes? {Hebbian} learning and retrieval of time-resolved excitation patterns},
	volume = {69},
	issn = {1432-0770},
	shorttitle = {Why spikes?},
	url = {https://doi.org/10.1007/BF00199450},
	doi = {10.1007/bf00199450},
	abstract = {Hebbian learning allows a network of spiking neurons to store and retrieve spatio-temporal patterns with a time resolution of 1 ms, despite the long postsynaptic and dendritic integration times. To show this, we introduce and analyze a model of spiking neurons, the spike response model, with a realistic distribution of axonal delays and with realistic postsynaptic potentials. Learning is performed by a local Hebbian rule which is based on the synchronism of presynaptic neurotransmitter release and some short-acting postsynaptic process. The time window of this synchronism determines the temporal resolution of pattern retrieval, which can be initiated by applying a short external stimulus pattern. Furthermore, a rate quantization is found in dependence upon the threshold value of the neurons, i.e., in a given time a pattern runs n times as often as learned, where n is a positive integer (n ⩾ 0). We show that all information about the spike pattern is lost if only mean firing rates (temporal average) or ensemble activities (spatial average) are considered. An average over several retrieval runs in order to generate a post-stimulus time histogram may also deteriorate the signal. The full information on a pattern is contained in the spike raster of a single run. Our results stress the importance, and advantage, of coding by spatio-temporal spike patterns instead of firing rates and average ensemble activity. The implications regarding modelling and experimental data analysis are discussed.},
	language = {en},
	number = {5},
	urldate = {2021-09-17},
	journal = {Biological Cybernetics},
	author = {Gerstner, Wulfram and Ritz, Raphael and van Hemmen, J. Leo},
	month = oct,
	year = {1993},
	note = {00338
tex.ids= Gerstner1993a},
	keywords = {⛔ No INSPIRE recid found},
	pages = {503--515},
}

@article{carr_microseconds_2010,
	title = {Microseconds {Matter}},
	volume = {8},
	issn = {1544-9173},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2893944/},
	doi = {10.1371/journal.pbio.1000405},
	abstract = {This Primer focuses on detection of the small interaural time differences that underlie sound localization.},
	number = {6},
	urldate = {2021-01-07},
	journal = {PLoS Biology},
	author = {Carr, Catherine E. and MacLeod, Katrina M.},
	month = jun,
	year = {2010},
	pmid = {20613856},
	pmcid = {PMC2893944},
	note = {00000 },
	keywords = {biology, delay-learning, ⛔ No INSPIRE recid found},
}

@article{korndorfer_cortical_2017,
	title = {Cortical {Spike} {Synchrony} as a {Measure} of {Input} {Familiarity}},
	volume = {29},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_00987},
	doi = {10.1162/neco_a_00987},
	abstract = {Spike synchrony, which occurs in various cortical areas in response to specific perception, action, and memory tasks, has sparked a long-standing debate on the nature of temporal organization in cortex. One prominent view is that this type of synchrony facilitates the binding or grouping of separate stimulus components. We argue instead for a more general function: a measure of the prior probability of incoming stimuli, implemented by long-range, horizontal, intracortical connections. We show that networks of this kind—pulse-coupled excitatory spiking networks in a noisy environment—can provide a sufficient substrate for stimulus-dependent spike synchrony. This allows for a quick (few spikes) estimate of the match between inputs and the input history as encoded in the network structure. Given the ubiquity of small, strongly excitatory subnetworks in cortex, we thus propose that many experimental observations of spike synchrony can be viewed as signs of input patterns that resemble long-term experience—that is, of patterns with high prior probability.},
	number = {9},
	urldate = {2021-04-28},
	journal = {Neural Computation},
	author = {Korndörfer, Clemens and Ullner, Ekkehard and García-Ojalvo, Jordi and Pipa, Gordon},
	month = sep,
	year = {2017},
	note = {00000},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {2491--2510},
}

@inproceedings{matsubara_spike_2017,
	title = {Spike timing-dependent conduction delay learning model classifying spatio-temporal spike patterns},
	doi = {10.1109/ijcnn.2017.7966073},
	abstract = {Precise spike timing is considered to play a fundamental role in communication and signal processing in biological neural networks. Understanding such mechanism contributes to both deep understanding of biological system and development of engineering applications such as efficient computational architectures. However, the biological mechanism which adjusts and maintains the spike timing still remains unclear. Previous studies have proposed algorithms adjusting synaptic efficacy and axonal conduction delay so that the spike timings get close to the desired spike timings in supervised manner. Supervised learning always requires desired spike timings as teacher signal, and thus it should not be dominant in biological system, which is considered to adapt to environment without teacher. This study proposes a spike timing-dependent learning model adjusting synaptic efficacy and axonal conduction delay in both unsupervised and supervised manners. The proposed learning algorithm approximates Expectation-Maximization algorithm and can classify the input data coded into spatio-temporal spike patterns. Furthermore, the proposed learning algorithm agrees with various results of existing biological experiments such as spike timing-dependent plasticity, and therefore it could be a good candidate of a model of biological delay learning.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Matsubara, T.},
	month = may,
	year = {2017},
	note = {00000 
ISSN: 2161-4407},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1831--1839},
}

@article{schmuker_feed-forward_2021,
	title = {Feed-forward and noise-tolerant detection of feature homogeneity in spiking networks with a latency code},
	volume = {115},
	issn = {0340-1200, 1432-0770},
	url = {https://link.springer.com/10.1007/s00422-021-00866-w},
	doi = {10.1007/s00422-021-00866-w},
	abstract = {In studies of the visual system as well as in computer vision, the focus is often on contrast edges. However, the primate visual system contains a large number of cells that are insensitive to spatial contrast and, instead, respond to uniform homogeneous illumination of their visual field. The purpose of this information remains unclear. Here, we propose a mechanism that detects feature homogeneity in visual areas, based on latency coding and spike time coincidence, in a purely feed-forward and therefore rapid manner. We demonstrate how homogeneity information can interact with information on contrast edges to potentially support rapid image segmentation. Furthermore, we analyze how neuronal crosstalk (noise) affects the mechanism’s performance. We show that the detrimental effects of crosstalk can be partly mitigated through delayed feed-forward inhibition that shapes bi-phasic post-synaptic events. The delay of the feed-forward inhibition allows effectively controlling the size of the temporal integration window and, thereby, the coincidence threshold. The proposed model is based on singlespike latency codes in a purely feed-forward architecture that supports low-latency processing, making it an attractive scheme of computation in spiking neuronal networks where rapid responses and low spike counts are desired.},
	language = {en},
	number = {2},
	urldate = {2021-05-03},
	journal = {Biological Cybernetics},
	author = {Schmuker, Michael and Kupper, Rüdiger and Aertsen, Ad and Wachtler, Thomas and Gewaltig, Marc-Oliver},
	month = apr,
	year = {2021},
	note = {00000},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {161--176},
}

@article{rekabdar_scale_2016,
	title = {A {Scale} and {Translation} {Invariant} {Approach} for {Early} {Classification} of {Spatio}-{Temporal} {Patterns} {Using} {Spiking} {Neural} {Networks}},
	volume = {43},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-015-9436-3},
	doi = {10.1007/s11063-015-9436-3},
	abstract = {This paper addresses the problem of encoding and classifying spatio-temporal patterns, which are typical for human actions or gestures. The proposed method has the following main contributions: (i) it requires a very small number of training examples, (ii) it accepts variable sized input patterns, (iii) it is invariant to scale and translation, and (iv) it enables early recognition, from only partial information of the pattern. The underlying representation employed is a spiking neural network with axonal conductance delay. We designed a novel approach for mapping spatio-temporal patterns to spike trains, which are used to stimulate the network. The pattern features emerge in the network as a result of this stimulation in the form of polychronous neuronal groups, which are used for classification. The proposed method is validated on a set of gestures representing the digits from \$\$0\$\$to \$\$9\$\$, extracted from video data of a human drawing the corresponding digits. The paper presents a comparison with several other standard pattern recognition approaches. The results show that the proposed approach significantly outperforms these methods, it is invariant to scale and translation, and it has the ability to recognize patterns from only partial information.},
	language = {en},
	number = {2},
	urldate = {2021-11-09},
	journal = {Neural Processing Letters},
	author = {Rekabdar, Banafsheh and Nicolescu, Monica and Nicolescu, Mircea and Saffar, Mohammad Taghi and Kelley, Richard},
	month = apr,
	year = {2016},
	note = {00000},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {327--343},
}

@article{roxin_role_2005,
	title = {Role of {Delays} in {Shaping} {Spatiotemporal} {Dynamics} of {Neuronal} {Activity} in {Large} {Networks}},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.94.238103},
	doi = {10.1103/physrevlett.94.238103},
	abstract = {We study the effect of delays on the dynamics of large networks of neurons. We show that delays give rise to a wealth of bifurcations and to a rich phase diagram, which includes oscillatory bumps, traveling waves, lurching waves, standing waves arising via a period-doubling bifurcation, aperiodic regimes, and regimes of multistability. We study the existence and the stability of the various dynamical patterns analytically and numerically in a simplified rate model as a function of the interaction parameters. The results derived in that framework allow us to understand the origin of the diversity of dynamical states observed in large networks of spiking neurons.},
	number = {23},
	urldate = {2021-04-12},
	journal = {Physical Review Letters},
	author = {Roxin, Alex and Brunel, Nicolas and Hansel, David},
	month = jun,
	year = {2005},
	note = {00000 
Publisher: American Physical Society},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {238103},
}

@article{trousdale_generative_2013,
	title = {A generative spike train model with time-structured higher order correlations},
	volume = {7},
	issn = {1662-5188},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3727174/},
	doi = {10.3389/fncom.2013.00084},
	abstract = {Emerging technologies are revealing the spiking activity in ever larger neural ensembles. Frequently, this spiking is far from independent, with correlations in the spike times of different cells. Understanding how such correlations impact the dynamics and function of neural ensembles remains an important open problem. Here we describe a new, generative model for correlated spike trains that can exhibit many of the features observed in data. Extending prior work in mathematical finance, this generalized thinning and shift (GTaS) model creates marginally Poisson spike trains with diverse temporal correlation structures. We give several examples which highlight the model's flexibility and utility. For instance, we use it to examine how a neural network responds to highly structured patterns of inputs. We then show that the GTaS model is analytically tractable, and derive cumulant densities of all orders in terms of model parameters. The GTaS framework can therefore be an important tool in the experimental and theoretical exploration of neural dynamics.},
	urldate = {2021-11-09},
	journal = {Frontiers in Computational Neuroscience},
	author = {Trousdale, James and Hu, Yu and Shea-Brown, Eric and Josić, Krešimir},
	month = jul,
	year = {2013},
	pmid = {23908626},
	pmcid = {PMC3727174},
	note = {00000 },
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {84},
}

@article{tversky_modeling_2002,
	series = {Computational {Neuroscience} {Trends} in {Research} 2002},
	title = {Modeling directional selectivity using self-organizing delay-adaptation maps},
	volume = {44-46},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231202004575},
	doi = {10.1016/s0925-2312(02)00457-5},
	abstract = {Using a delay-adaptation learning rule, we model the activity-dependent development of directionally selective cells in the primary visual cortex. Based on input stimuli, a learning rule shifts delays to create synchronous arrival of spikes at cortical cells. As a result, delays become tuned creating a smooth cortical map of direction selectivity. This result demonstrates how delay adaption can serve as a powerful abstraction for modeling temporal learning in the brain.},
	language = {en},
	urldate = {2021-11-24},
	journal = {Neurocomputing},
	author = {Tversky, Tal and Miikkulainen, Risto},
	month = jun,
	year = {2002},
	note = {00000},
	keywords = {⛔ No INSPIRE recid found},
	pages = {679--684},
}

@inproceedings{wright_learning_2012,
	title = {Learning transmission delays in spiking neural networks: {A} novel approach to sequence learning based on spike delay variance},
	shorttitle = {Learning transmission delays in spiking neural networks},
	doi = {10.1109/ijcnn.2012.6252371},
	abstract = {Transmission delays are an inherent component of spiking neural networks (SNNs) but relatively little is known about how delays are adapted in biological systems and studies on computational learning mechanisms have focused on spike-timing-dependent plasticity (STDP) which adjusts synaptic weights rather than synaptic delays. We propose a novel algorithm for learning temporal delays in SNNs with Gaussian synapses, which we call spike-delay-variance learning (SDVL). A key feature of the algorithm is adaptation of the shape (mean and variance) of the postsynaptic release profiles only, rather than the conventional STDP approach of adapting the network's synaptic weights. The algorithm's ability to learn temporal input sequences was tested in three studies using supervised and unsupervised learning within feed-forward networks. SDVL was able to successfully classify forty spatiotemporal patterns without supervision by providing robust, effective adaption of the postsynaptic release profiles. The results demonstrate how delay learning can contribute to the stability of spiking sequences, and that there is a potential role for adaption of variance as well as mean values in learning algorithms for spiking neural networks.},
	booktitle = {The 2012 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Wright, Paul W and Wiles, Janet},
	month = jun,
	year = {2012},
	note = {00000 
ISSN: 2161-4407},
	keywords = {Jean, SNN, delay-learning, ⛔ No INSPIRE recid found},
	pages = {1--8},
}

@article{hahnloser_ultra-sparse_2002,
	title = {An ultra-sparse code underliesthe generation of neural sequences in a songbird {\textbar} {Nature}},
	volume = {419},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature00974},
	doi = {10.1038/nature00974},
	abstract = {Sequences of motor activity are encoded in many vertebrate brains by complex spatio-temporal patterns of neural activity; however, the neural circuit mechanisms underlying the generation of these pre-motor patterns are poorly understood. In songbirds, one prominent site of pre-motor activity is the forebrain robust nucleus of the archistriatum (RA), which generates stereotyped sequences of spike bursts during song1 and recapitulates these sequences during sleep2. We show that the stereotyped sequences in RA are driven from nucleus HVC (high vocal centre), the principal pre-motor input to RA3,4. Recordings of identified HVC neurons in sleeping and singing birds show that individual HVC neurons projecting onto RA neurons produce bursts sparsely, at a single, precise time during the RA sequence. These HVC neurons burst sequentially with respect to one another. We suggest that at each time in the RA sequence, the ensemble of active RA neurons is driven by a subpopulation of RA-projecting HVC neurons that is active only at that time. As a population, these HVC neurons may form an explicit representation of time in the sequence. Such a sparse representation, a temporal analogue of the ‘grandmother cell’5 concept for object recognition, eliminates the problem of temporal interference during sequence generation and learning attributed to more distributed representations6,7.},
	number = {6902},
	urldate = {2022-02-04},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Kozhevnikov, Alexay A. and Fee, Michale S.},
	month = sep,
	year = {2002},
	note = {01055},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {65--70},
}

@article{weber_spike_2007,
	title = {Spike {Correlations} in a {Songbird} {Agree} with a {Simple} {Markov} {Population} {Model}},
	volume = {3},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030249},
	doi = {10.1371/journal.pcbi.0030249},
	abstract = {The relationships between neural activity at the single-cell and the population levels are of central importance for understanding neural codes. In many sensory systems, collective behaviors in large cell groups can be described by pairwise spike correlations. Here, we test whether in a highly specialized premotor system of songbirds, pairwise spike correlations themselves can be seen as a simple corollary of an underlying random process. We test hypotheses on connectivity and network dynamics in the motor pathway of zebra finches using a high-level population model that is independent of detailed single-neuron properties. We assume that neural population activity evolves along a finite set of states during singing, and that during sleep population activity randomly switches back and forth between song states and a single resting state. Individual spike trains are generated by associating with each of the population states a particular firing mode, such as bursting or tonic firing. With an overall modification of one or two simple control parameters, the Markov model is able to reproduce observed firing statistics and spike correlations in different neuron types and behavioral states. Our results suggest that song- and sleep-related firing patterns are identical on short time scales and result from random sampling of a unique underlying theme. The efficiency of our population model may apply also to other neural systems in which population hypotheses can be tested on recordings from small neuron groups.},
	number = {12},
	urldate = {2022-02-04},
	journal = {PLOS Computational Biology},
	author = {Weber, Andrea P. and Hahnloser, Richard H. R.},
	year = {2007},
	note = {00018},
	keywords = {Action potentials, Behavior, Bird song, Birds, Ground state, Neurons, Single neuron function, Sleep, ⛔ No INSPIRE recid found},
	pages = {e249},
}

@article{long_support_2010,
	title = {Support for a synaptic chain model of neuronal sequence generation},
	volume = {468},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature09514},
	doi = {10.1038/nature09514},
	language = {en},
	number = {7322},
	urldate = {2022-02-04},
	journal = {Nature},
	author = {Long, Michael A. and Jin, Dezhe Z. and Fee, Michale S.},
	month = nov,
	year = {2010},
	note = {00360
tex.ids= Long10a},
	keywords = {⛔ No INSPIRE recid found},
	pages = {394--399},
}

@article{kollmorgen_dynamic_2014,
	title = {Dynamic {Alignment} {Models} for {Neural} {Coding}},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003508},
	doi = {10.1371/journal.pcbi.1003508},
	abstract = {Recently, there have been remarkable advances in modeling the relationships between the sensory environment, neuronal responses, and behavior. However, most models cannot encompass variable stimulus-response relationships such as varying response latencies and state or context dependence of the neural code. Here, we consider response modeling as a dynamic alignment problem and model stimulus and response jointly by a mixed pair hidden Markov model (MPH). In MPHs, multiple stimulus-response relationships (e.g., receptive fields) are represented by different states or groups of states in a Markov chain. Each stimulus-response relationship features temporal flexibility, allowing modeling of variable response latencies, including noisy ones. We derive algorithms for learning of MPH parameters and for inference of spike response probabilities. We show that some linear-nonlinear Poisson cascade (LNP) models are a special case of MPHs. We demonstrate the efficiency and usefulness of MPHs in simulations of both jittered and switching spike responses to white noise and natural stimuli. Furthermore, we apply MPHs to extracellular single and multi-unit data recorded in cortical brain areas of singing birds to showcase a novel method for estimating response lag distributions. MPHs allow simultaneous estimation of receptive fields, latency statistics, and hidden state dynamics and so can help to uncover complex stimulus response relationships that are subject to variable timing and involve diverse neural codes.},
	number = {3},
	urldate = {2022-02-04},
	journal = {PLOS Computational Biology},
	author = {Kollmorgen, Sepp and Hahnloser, Richard H. R.},
	month = mar,
	year = {2014},
	note = {00007},
	keywords = {Action potentials, Algorithms, Covariance, Hidden Markov models, Neurons, Probability distribution, Sequence alignment, White noise, ⛔ No INSPIRE recid found},
	pages = {e1003508},
}

@inproceedings{perrinet_sparse_2002,
	title = {Sparse {Image} {Coding} {Using} an {Asynchronous} {Spiking} {Neural} {Network}},
	abstract = {In order to explore coding strategies in the retina, we use a wavelet-like transform which output is sparse, as is observed in biological retinas [4]. This transform is defined in the context of a one-pass feed-forward spiking neural network, and the output is the list of its neurons' spikes: it is recursively constructed using a greedy matching pursuit scheme which first selects higher contrast energy values.},
	author = {Perrinet, Laurent and Samuelides, Manuel},
	month = aug,
	year = {2002},
	keywords = {\#nosource, area-v1, receptive field, sparse coding, ⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@inproceedings{state_training_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Training {Delays} in {Spiking} {Neural} {Networks}},
	isbn = {978-3-030-30487-4},
	doi = {10.1007/978-3-030-30487-4_54},
	abstract = {Spiking Neural Networks (SNNs) are a promising computational paradigm, both to understand biological information processing and for low-power, embedded chips. Although SNNs are known to encode information in the precise timing of spikes, conventional artificial learning algorithms do not take this into account directly. In this work, we implement the spike timing by training the synaptic delays in a single layer SNN. We use two different approaches: a classical gradient descent and a direct algebraic method that is based on a complex-valued encoding of the spikes. Both algorithms are equally able to correctly solve simple detection tasks. Our work provides new optimization methods for the data analysis of highly time-dependent data and training methods for neuromorphic chips.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2019: {Theoretical} {Neural} {Computation}},
	publisher = {Springer International Publishing},
	author = {State, Laura and Vilimelis Aceituno, Pau},
	editor = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
	year = {2019},
	pages = {713--717},
}

@misc{kohn_utah_2016,
	title = {Utah array extracellular recordings of spontaneous and visually evoked                 activity from anesthetized macaque primary visual cortex ({V1}).},
	url = {http://crcns.org/data-sets/vc/pvc-11},
	language = {en},
	urldate = {2022-12-19},
	publisher = {CRCNS.org},
	author = {Kohn, A. and Smith, M.A.},
	year = {2016},
	doi = {10.6080/K0NC5Z4X},
	keywords = {Macaque, Neuroscience, Primary visual cortex, ⛔ No INSPIRE recid found},
}

@article{gewaltig_propagation_2001,
	title = {Propagation of cortical synfire activity: survival probability in single trials and stability in the mean},
	volume = {14},
	issn = {0893-6080},
	shorttitle = {Propagation of cortical synfire activity},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000703},
	doi = {10.1016/S0893-6080(01)00070-3},
	abstract = {The synfire hypothesis states that under appropriate conditions volleys of synchronized spikes (pulse packets) can propagate through the cortical network by traveling along chains of groups of cortical neurons. Here, we present results from network simulations, taking full account of the variability in pulse packet realizations. We repeatedly stimulated a synfire chain of model neurons and estimated activity (a) and temporal jitter (σ) of the spike response for each neuron group in the chain in many trials. The survival probability of the activity was assessed for each point in (a, σ)-space. The results confirm and extend our earlier predictions based on single neuron properties and a deterministic state-space analysis [Diesmann, M., Gewaltig, M.-O., \& Aertsen, A. (1999). Stable propagation of synchronous spiking in cortical neural networks. Nature, 402, 529–533].},
	language = {en},
	number = {6},
	urldate = {2022-10-26},
	journal = {Neural Networks},
	author = {Gewaltig, Marc-Oliver and Diesmann, Markus and Aertsen, Ad},
	month = jul,
	year = {2001},
	keywords = {Action Potentials, Animals, Cell Membrane, Cerebral Cortex, Cortical dynamics, Humans, Integrate-and-fire neurons, Models, Statistical, Nerve Net, Neural Networks, Computer, Neurons, Pulse packets, Single-trial analysis, Spike patterns, Spiking neurons, Synaptic Transmission, Synfire chains, Variability},
	pages = {657--673},
}

@inproceedings{thanasoulis_delay-based_2021,
	title = {Delay-{Based} {Neural} {Computation}: {Pulse} {Routing} {Architecture} and {Benchmark} {Application} in {FPGA}},
	shorttitle = {Delay-{Based} {Neural} {Computation}},
	doi = {10.1109/ICECS53924.2021.9665468},
	abstract = {Neuromorphic engineering implements large-scale systems that provide a high integration density of power efficient synapse-and-neuron blocks. This represents a promising alternative to the numerical simulations for studying the dynamics of spiking neural networks. A key aspect of these systems is the implementation of communication and routing of pulse events produced by the neural network. In this paper we present a measurement methodology and results of a neural benchmark that tests the configurable delays, multicasting and connectivity implemented by a routing logic for neuromorphic hardware. Pulses are handled according to their timestamp and transmitted with configurable delays and routing to different post-synaptic neurons. The results show the suitability of communication and routing logic for delay-based neural computation and point out effects of time discretization in resolution of pulse timestamps.},
	booktitle = {2021 28th {IEEE} {International} {Conference} on {Electronics}, {Circuits}, and {Systems} ({ICECS})},
	author = {Thanasoulis, Vasilis and Vogginger, Bernhard and Partzsch, Johannes and Mayr, Christian},
	month = nov,
	year = {2021},
	keywords = {Benchmark testing, Hardware, Multicast communication, Neuromorphic engineering, Neurons, Numerical simulation, Routing},
	pages = {1--5},
}

@article{gerstner_neuronal_1996,
	title = {A neuronal learning rule for sub-millisecond temporal coding},
	volume = {383},
	copyright = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/383076a0},
	doi = {10.1038/383076a0},
	abstract = {A PARADOX that exists in auditory and electrosensory neural systems1,2 is that they encode behaviourally relevant signals in the range of a few microseconds with neurons that are at least one order of magnitude slower. The importance of temporal coding in neural information processing is not clear yet3–8. A central question is whether neuronal firing can be more precise than the time constants of the neuronal processes involved9. Here we address this problem using the auditory system of the barn owl as an example. We present a modelling study based on computer simulations of a neuron in the laminar nucleus. Three observations explain the paradox. First, spiking of an 'integrate-and-fire' neuron driven by excitatory postsynaptic potentials with a width at half-maximum height of 250 μs, has an accuracy of 25 μs if the presynaptic signals arrive coherently. Second, the necessary degree of coherence in the signal arrival times can be attained during ontogenetic development by virtue of an unsupervised hebbian learning rule. Learning selects connections with matching delays from a broad distribution of axons with random delays. Third, the learning rule also selects the correct delays from two independent groups of inputs, for example, from the left and right ear.},
	language = {en},
	number = {6595},
	urldate = {2021-01-07},
	journal = {Nature},
	author = {Gerstner, Wulfram and Kempter, Richard and van Hemmen, J. Leo and Wagner, Hermann},
	month = sep,
	year = {1996},
	note = {Number: 6595
Publisher: Nature Publishing Group},
	pages = {76--78},
}

@article{pena_cochlear_2001,
	title = {Cochlear and {Neural} {Delays} for {Coincidence} {Detection} in {Owls}},
	volume = {21},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6763915/},
	doi = {10.1523/JNEUROSCI.21-23-09455.2001},
	abstract = {The auditory system uses delay lines and coincidence detection to measure the interaural time difference (ITD). Both axons and the cochlea could provide such delays. The stereausis theory assumes that differences in wave propagation time along the basilar membrane can provide the necessary delays, if the coincidence detectors receive input from fibers innervating different loci on the left and right basilar membranes. If this hypothesis were true, the left and right inputs to coincidence detectors should differ in their frequency tuning. The owl's nucleus laminaris contains coincidence detector neurons that receive input from the left and right cochlear nuclei. Monaural frequency-tuning curves of nucleus laminaris neurons showed small interaural differences. In addition, their preferred ITDs were not correlated with the interaural frequency mismatches. Instead, the preferred ITD of the neuron agrees with that predicted from the distribution of axonal delays. Thus, there is no need to invoke mechanisms other than neural delays to explain the detection of ITDs by the barn owl's laminaris neurons.},
	number = {23},
	urldate = {2021-01-07},
	journal = {The Journal of Neuroscience},
	author = {Peña, José Luis and Viete, Svenja and Funabiki, Kazuo and Saberi, Kourosh and Konishi, Masakazu},
	month = dec,
	year = {2001},
	pmid = {11717379},
	pmcid = {PMC6763915},
	keywords = {biology, delay-learning},
	pages = {9455--9459},
}

@article{gerstein_random_1964,
	title = {Random {Walk} {Models} for the {Spike} {Activity} of a {Single} {Neuron}},
	volume = {4},
	issn = {0006-3495},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1367440/},
	abstract = {Quantitative methods for the study of the statistical properties of spontaneously occurring spike trains from single neurons have recently been presented. Such measurements suggest a number of descriptive mathematical models. One of these, based on a random walk towards an absorbing barrier, can describe a wide range of neuronal activity in terms of two parameters. These parameters are readily associated with known physiological mechanisms.},
	number = {1 Pt 1},
	urldate = {2022-03-02},
	journal = {Biophysical Journal},
	author = {Gerstein, George L. and Mandelbrot, Benoit},
	month = jan,
	year = {1964},
	pmid = {14104072},
	pmcid = {PMC1367440},
	keywords = {⛔ No INSPIRE recid found},
	pages = {41--68},
}

@article{le_mouel_supervised_2014,
	title = {Supervised learning with decision margins in pools of spiking neurons},
	volume = {37},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-014-0505-9},
	doi = {10.1007/s10827-014-0505-9},
	abstract = {Learning to categorise sensory inputs by generalising from a few examples whose category is precisely known is a crucial step for the brain to produce appropriate behavioural responses. At the neuronal level, this may be performed by adaptation of synaptic weights under the influence of a training signal, in order to group spiking patterns impinging on the neuron. Here we describe a framework that allows spiking neurons to perform such “supervised learning”, using principles similar to the Support Vector Machine, a well-established and robust classifier. Using a hinge-loss error function, we show that requesting a margin similar to that of the SVM improves performance on linearly non-separable problems. Moreover, we show that using pools of neurons to discriminate categories can also increase the performance by sharing the load among neurons.},
	language = {en},
	number = {2},
	urldate = {2022-03-07},
	journal = {Journal of Computational Neuroscience},
	author = {Le Mouel, Charlotte and Harris, Kenneth D. and Yger, Pierre},
	month = oct,
	year = {2014},
	keywords = {⛔ No INSPIRE recid found},
	pages = {333--344},
}

@article{javanshir_advancements_2022,
	title = {Advancements in {Algorithms} and {Neuromorphic} {Hardware} for {Spiking} {Neural} {Networks}},
	volume = {34},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01499},
	doi = {10.1162/neco_a_01499},
	abstract = {Artificial neural networks (ANNs) have experienced a rapid advancement for their success in various application domains, including autonomous driving and drone vision. Researchers have been improving the performance efficiency and computational requirement of ANNs inspired by the mechanisms of the biological brain. Spiking neural networks (SNNs) provide a power-efficient and brain-inspired computing paradigm for machine learning applications. However, evaluating large-scale SNNs on classical von Neumann architectures (central processing units/graphics processing units) demands a high amount of power and time. Therefore, hardware designers have developed neuromorphic platforms to execute SNNs in and approach that combines fast processing and low power consumption. Recently, field-programmable gate arrays (FPGAs) have been considered promising candidates for implementing neuromorphic solutions due to their varied advantages, such as higher flexibility, shorter design, and excellent stability. This review aims to describe recent advances in SNNs and the neuromorphic hardware platforms (digital, analog, hybrid, and FPGA based) suitable for their implementation. We present that biological background of SNN learning, such as neuron models and information encoding techniques, followed by a categorization of SNN training. In addition, we describe state-of-the-art SNN simulators. Furthermore, we review and present FPGA-based hardware implementation of SNNs. Finally, we discuss some future directions for research in this field.},
	number = {6},
	urldate = {2022-12-16},
	journal = {Neural Computation},
	author = {Javanshir, Amirhossein and Nguyen, Thanh Thi and Mahmud, M. A. Parvez and Kouzani, Abbas Z.},
	month = may,
	year = {2022},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1289--1328},
}

@article{ben-yishai_traveling_1996,
	title = {Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs} in a {Cortical} {Network} {Module}},
	abstract = {Recent studies have shown that local cortical feedback can have an important effect on the response of neurons in primary visual cortex to the orientation of visual stimuli. In this work, we study the role of the cortical feedback in shaping the spatiotemporal patterns of activity in cortex. Two questions are addressed: one, what are the limitations on the ability of cortical neurons to lock their activity to rotating oriented stimuli within a single receptive ﬁeld? Two, can the local architecture of visual cortex lead to the generation of spontaneous traveling pulses of activity? We study these issues analytically by a population-dynamic model of a hypercolumn in visual cortex. The order parameter that describes the macroscopic behavior of the network is the time-dependent population vector of the network. We ﬁrst study the network dynamics under the inﬂuence of a weakly tuned input that slowly rotates within the receptive ﬁeld. We show that if the cortical interactions have strong spatial modulation, the network generates a sharply tuned activity proﬁle that propagates across the hypercolumn in a path that is completely locked to the stimulus rotation. The resultant rotating population vector maintains a constant angular lag relative to the stimulus, the magnitude of which grows with the stimulus rotation frequency. Beyond a critical frequency the population vector does not lock to the stimulus but executes a quasi-periodic motion with an average frequency that is smaller than that of the stimulus. In the second part we consider the stable intrinsic state of the cortex under the inﬂuence of isotropic stimulation. We show that if the local inhibitory feedback is sufﬁciently strong, the network does not settle into a stationary state but develops spontaneous traveling pulses of activity. Unlike recent models of wave propagation in cortical networks, the connectivity pattern in our model is spatially symmetric, hence the direction of propagation of these waves is arbitrary. The interaction of these waves with an external-oriented stimulus is studied. It is shown that the system can lock to a weakly tuned rotating stimulus if the stimulus frequency is close to the frequency of the intrinsic wave.},
	language = {en},
	journal = {Journal of Computational Neuroscience},
	author = {Ben-Yishai, Rani and Hansel, David and Sompolinsky, Haim},
	year = {1996},
	keywords = {\#nosource, orientation selectivity, population vector, primary visual cortex, ⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@article{bienenstock_model_1995,
	title = {A model of neocortex},
	volume = {6},
	issn = {0954-898X},
	url = {https://doi.org/10.1088/0954-898X_6_2_004},
	doi = {10.1088/0954-898X_6_2_004},
	abstract = {Prompted by considerations about (i) the compositionality of cognitive functions, (ii) the physiology of individual cortical neurons, (iii) the role of accurately timed spike patterns in cortex, and (iv) the regulation of global cortical activity, we suggest that the dynamics of cortex on the 1-ms time scale may be described as the activation of circuits of the synfire-chain type (Abeles 1982, 1991). We suggest that the fundamental computational unit in cortex may be a wave-like spatio-temporal pattern of synfire type, and that the binding mechanism underlying compositionality in cognition may be the accurate synchronization of synfire waves that propagate simultaneously on distinct, weakly coupled, synfire chains. We propose that Hebbian synaptic plasticity may result in a superposition of synfire chains in cortical connectivity, whereby a given neuron participates in many distinct chains. We investigate the behaviour of a much-simplified model of cortical dynamics devised along these principles. Calculations and numerical experiments are performed based on an assumption of randomness of stored chains, in the style of statistical physics. It is demonstrated that: (i) there exists a critical value for the total length of stored chains; (ii) this storage capacity is linear in the network's size; (iii) the behaviour of the network around the critical point is characterized by the self-regulation of the number of synfire waves coactive in the network at any given time.},
	number = {2},
	urldate = {2022-12-16},
	journal = {Network: Computation in Neural Systems},
	author = {Bienenstock, Elie},
	month = jan,
	year = {1995},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1088/0954-898X\_6\_2\_004},
	keywords = {⛔ No INSPIRE recid found},
	pages = {179--224},
}

@article{pang_fast_2019,
	title = {Fast and flexible sequence induction in spiking neural networks via rapid excitability changes},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.44324},
	doi = {10.7554/elife.44324},
	abstract = {Cognitive flexibility likely depends on modulation of the dynamics underlying how biological neural networks process information. While dynamics can be reshaped by gradually modifying connectivity, less is known about mechanisms operating on faster timescales. A compelling entrypoint to this problem is the observation that exploratory behaviors can rapidly cause selective hippocampal sequences to ‘replay’ during rest. Using a spiking network model, we asked whether simplified replay could arise from three biological components: fixed recurrent connectivity; stochastic ‘gating’ inputs; and rapid gating input scaling via long-term potentiation of intrinsic excitability (LTP-IE). Indeed, these enabled both forward and reverse replay of recent sensorimotor-evoked sequences, despite unchanged recurrent weights. LTP-IE ‘tags’ specific neurons with increased spiking probability under gating input, and ordering is reconstructed from recurrent connectivity. We further show how LTP-IE can implement temporary stimulus-response mappings. This elucidates a novel combination of mechanisms that might play a role in rapid cognitive flexibility.},
	urldate = {2022-02-02},
	journal = {eLife},
	author = {Pang, Rich and Fairhall, Adrienne L},
	editor = {Salinas, Emilio and Marder, Eve and Salinas, Emilio},
	month = may,
	year = {2019},
	note = {00008
tex.ids= Pang2019
publisher: eLife Sciences Publications, Ltd},
	keywords = {cognitive flexibility, excitability, neural network, neurocentric, sequence, short-term memory, ⛔ No INSPIRE recid found},
	pages = {e44324},
}

@article{carr_circuit_1990,
	title = {A circuit for detection of interaural time differences in the brain stem of the barn owl},
	volume = {10},
	copyright = {© 1990 by Society for Neuroscience},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/10/10/3227},
	doi = {10.1523/JNEUROSCI.10-10-03227.1990},
	abstract = {Detection of interaural time differences underlies azimuthal sound localization in the barn owl Tyto alba. Axons of the cochlear nucleus magnocellularis, and their targets in the binaural nucleus laminaris, form the circuit responsible for encoding these interaural time differences. The nucleus laminaris receives bilateral inputs from the cochlear nucleus magnocellularis such that axons from the ipsilateral cochlear nucleus enter the nucleus laminaris dorsally, while contralateral axons enter from the ventral side. This interdigitating projection to the nucleus laminaris is tonotopic, and the afferents are both sharply tuned and matched in frequency to the neighboring afferents. Recordings of phase-locked spikes in the afferents show an orderly change in the arrival time of the spikes as a function of distance from the point of their entry into the nucleus laminaris. The same range of conduction time (160 mu sec) was found over the 700-mu m depth of the nucleus laminaris for all frequencies examined (4-7.5 kHz) and corresponds to the range of interaural time differences available to the barn owl. The estimated conduction velocity in the axons is low (3-5 m/sec) and may be regulated by short internodal distances (60 mu m) within the nucleus laminaris. Neurons of the nucleus laminaris have large somata and very short dendrites. These cells are frequency selective and phase-lock to both monaural and binaural stimuli. The arrival time of phase-locked spikes in many of these neurons differs between the ipsilateral and contralateral inputs. When this disparity is nullified by imposition of an appropriate interaural time difference, the neurons respond maximally. The number of spikes elicited in response to a favorable interaural time difference is roughly double that elicited by a monaural stimulus. Spike counts for unfavorable interaural time differences fall well below monaural response levels. These findings indicate that the magnocellular afferents work as delay lines, and the laminaris neurons work as co- incidence detectors. The orderly distribution of conduction times, the predictability of favorable interaural time differences from monaural phase responses, and the pattern of the anatomical projection from the nucleus laminaris to the central nucleus of the inferior colliculus suggest that interaural time differences and their phase equivalents are mapped in each frequency band along the dorsoventral axis of the nucleus laminaris.},
	language = {en},
	number = {10},
	urldate = {2022-12-16},
	journal = {Journal of Neuroscience},
	author = {Carr, C. E. and Konishi, M.},
	month = oct,
	year = {1990},
	pmid = {2213141},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {\#nosource, ⛔ No INSPIRE recid found},
	pages = {3227--3246},
}

@article{carr_axonal_1988,
	title = {Axonal delay lines for time measurement in the owl's brainstem.},
	volume = {85},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC282419/},
	abstract = {Interaural time difference is an important cue for sound localization. In the barn owl (Tyto alba) neuronal sensitivity to this disparity originates in the brainstem nucleus laminaris. Afferents from the ipsilateral and contralateral magnocellular cochlear nuclei enter the nucleus laminaris through its dorsal and ventral surfaces, respectively, and interdigitate in the nucleus. Intracellular recordings from these afferents show orderly changes in conduction delay with depth in the nucleus. These changes are comparable to the range of interaural time differences available to the owl. Thus, these afferent axons act as delay lines and provide anatomical and physiological bases for a neuronal map of interaural time differences in the nucleus laminaris.},
	number = {21},
	urldate = {2021-01-07},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Carr, C E and Konishi, M},
	month = nov,
	year = {1988},
	pmid = {3186725},
	pmcid = {PMC282419},
	note = {00395 },
	keywords = {biology, delay-learning, ⛔ No INSPIRE recid found},
	pages = {8311--8315},
}

@article{baldi_how_1994,
	title = {How delays affect neural dynamics and learning},
	volume = {5},
	issn = {1941-0093},
	doi = {10.1109/72.298231},
	abstract = {We investigate the effects of delays on the dynamics and, in particular, on the oscillatory properties of simple neural network models. We extend previously known results regarding the effects of delays on stability and convergence properties. We treat in detail the case of ring networks for which we derive simple conditions for oscillating behavior and several formulas to predict the regions of bifurcation, the periods of the limit cycles and the phases of the different neurons. These results in turn can readily be applied to more complex and more biologically motivated architectures, such as layered networks. In general, the main result is that delays tend to increase the period of oscillations and broaden the spectrum of possible frequencies, in a quantifiable way. Simulations show that the theoretically predicted values are in excellent agreement with the numerically observed behavior. Adaptable delays are then proposed as one additional mechanism through which neural systems could tailor their own dynamics. Accordingly, we derive recurrent backpropagation learning formulas for the adjustment of delays and other parameters in networks with delayed interactions and discuss some possible applications.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Transactions on Neural Networks},
	author = {Baldi, P. and Atiya, A.F.},
	month = jul,
	year = {1994},
	note = {00000 
Conference Name: IEEE Transactions on Neural Networks},
	keywords = {⛔ No INSPIRE recid found},
	pages = {612--621},
}

@misc{rowland_perception_2023,
	title = {Perception and propagation of activity through the cortical hierarchy is determined by neural variability},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.12.28.474343v2},
	doi = {10.1101/2021.12.28.474343},
	abstract = {The brains of higher organisms are composed of anatomically and functionally distinct regions performing specialised tasks; but regions do not operate in isolation. Orchestration of complex behaviours requires communication between brain regions, but how neural activity dynamics are organised to facilitate reliable transmission is not well understood. We studied this process directly by generating neural activity that propagates between brain regions and drives behaviour, allowing us to assess how populations of neurons in sensory cortex cooperate to transmit information. We achieved this by imaging two hierarchically organised and densely interconnected regions, the primary and secondary somatosensory cortex (S1 and S2) in mice while performing two-photon photostimulation of S1 neurons and assigning behavioural salience to the photostimulation. We found that the probability of perception is determined not only by the strength of the photostimulation signal, but also by the variability of S1 neural activity. Therefore, maximising the signal-to-noise ratio of the stimulus representation in cortex relative to the noise or variability in cortex is critical to facilitate activity propagation and perception. Further, we show that propagated, behaviourally salient activity elicits balanced, persistent, and generalised activation of the downstream region. Hence, our work adds to existing understanding of cortical function by identifying how population activity is formatted to ensure robust transmission of information, allowing specialised brain regions to communicate and coordinate behaviour.},
	language = {en},
	urldate = {2023-02-23},
	publisher = {bioRxiv},
	author = {Rowland, James M. and Plas, Thijs L. van der and Loidolt, Matthias and Lees, Robert M. and Keeling, Joshua and Dehning, Jonas and Akam, Thomas and Priesemann, Viola and Packer, Adam M.},
	month = jan,
	year = {2023},
	note = {Pages: 2021.12.28.474343
Section: New Results},
	keywords = {⛔ No INSPIRE recid found},
}

@misc{perrinet_polychronies_2022,
	title = {Polychronies (2022 / 2025)},
	url = {https://laurentperrinet.github.io/grant/polychronies/},
	abstract = {A grant from the Ph.D. program in Integrative and Clinical Neuroscience (Post-doctoral position, 2022 / 2025).},
	language = {en-us},
	urldate = {2023-01-18},
	journal = {Novel visual computations},
	author = {Perrinet, Laurent U.},
	month = jul,
	year = {2022},
	keywords = {⛔ No INSPIRE recid found},
}

@article{mackevicius_unsupervised_2019,
	title = {Unsupervised discovery of temporal sequences in high-dimensional datasets, with applications to neuroscience},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.38471},
	doi = {10.7554/eLife.38471},
	abstract = {Identifying low-dimensional features that describe large-scale neural recordings is a major challenge in neuroscience. Repeated temporal patterns (sequences) are thought to be a salient feature of neural dynamics, but are not succinctly captured by traditional dimensionality reduction techniques. Here, we describe a software toolbox—called seqNMF—with new methods for extracting informative, non-redundant, sequences from high-dimensional neural data, testing the significance of these extracted patterns, and assessing the prevalence of sequential structure in data. We test these methods on simulated data under multiple noise conditions, and on several real neural and behavioral data sets. In hippocampal data, seqNMF identifies neural sequences that match those calculated manually by reference to behavioral events. In songbird data, seqNMF discovers neural sequences in untutored birds that lack stereotyped songs. Thus, by identifying temporal structure directly from neural data, seqNMF enables dissection of complex neural circuits without relying on temporal references from stimuli or behavioral outputs.},
	urldate = {2023-01-24},
	journal = {eLife},
	author = {Mackevicius, Emily L and Bahle, Andrew H and Williams, Alex H and Gu, Shijie and Denisenko, Natalia I and Goldman, Mark S and Fee, Michale S},
	editor = {Colgin, Laura and Behrens, Timothy E},
	month = feb,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {Zebra finch, matrix factorization, sequence, unsupervised, ⛔ No INSPIRE recid found},
	pages = {e38471},
}

@article{guo_complex_2022,
	title = {Complex spiking neural networks with synaptic time-delay based on anti-interference function},
	issn = {1871-4099},
	url = {https://doi.org/10.1007/s11571-022-09803-4},
	doi = {10.1007/s11571-022-09803-4},
	abstract = {The research on a brain-like model with bio-interpretability is conductive to promoting its information processing ability in the field of artificial intelligence. Biological results show that the synaptic time-delay can improve the information processing abilities of the nervous system, which are an important factor related to the formation of brain cognitive functions. However, the synaptic plasticity with time-delay of a brain-like model still lacks bio-interpretability. In this study, combining excitatory and inhibitory synapses, we construct the complex spiking neural networks (CSNNs) with synaptic time-delay that more conforms biological characteristics, in which the topology has scale-free property and small-world property, and the nodes are represented by an Izhikevich neuron model. Then, the information processing abilities of CSNNs with different types of synaptic time-delay are comparatively evaluated based on the anti-interference function, and the mechanism of this function is discussed. Using two indicators of the anti-interference function and three kinds of noise, our simulation results consistently verify that: (i) From the perspective of anti-interference function, an CSNN with synaptic random time-delay outperforms an CSNN with synaptic fixed time-delay, which in turn outperforms an CSNN with synaptic none time-delay. The results imply that brain-like networks with more bio-interpretable synaptic time-delay have stronger information processing abilities. (ii) The synaptic plasticity is the intrinsic factor of the anti-interference function of CSNNs with different types of synaptic time-delay. (iii) The synaptic random time-delay makes an CSNN present better topological characteristics, which can improve the information processing ability of a brain-like network. It implies that synaptic time-delay is a factor that affects the anti-interference function at the level of performance.},
	language = {en},
	urldate = {2022-05-02},
	journal = {Cognitive Neurodynamics},
	author = {Guo, Lei and Zhang, Sijia and Wu, Youxi and Xu, Guizhi},
	month = apr,
	year = {2022},
	keywords = {\#nosource, Anti-interference, Complex network, Spiking neural network, Synaptic plasticity, Synaptic time-delay, ⛔ No INSPIRE recid found},
}

@article{zierenberg_homeostatic_2018,
	title = {Homeostatic {Plasticity} and {External} {Input} {Shape} {Neural} {Network} {Dynamics}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031018},
	doi = {10.1103/PhysRevX.8.031018},
	abstract = {In vitro and in vivo spiking activity clearly differ. Whereas networks in vitro develop strong bursts separated by periods of very little spiking activity, in vivo cortical networks show continuous activity. This is puzzling considering that both networks presumably share similar single-neuron dynamics and plasticity rules. We propose that the defining difference between in vitro and in vivo dynamics is the strength of external input. In vitro, networks are virtually isolated, whereas in vivo every brain area receives continuous input. We analyze a model of spiking neurons in which the input strength, mediated by spike rate homeostasis, determines the characteristics of the dynamical state. In more detail, our analytical and numerical results on various network topologies show consistently that under increasing input, homeostatic plasticity generates distinct dynamic states, from bursting, to close-to-critical, reverberating, and irregular states. This implies that the dynamic state of a neural network is not fixed but can readily adapt to the input strengths. Indeed, our results match experimental spike recordings in vitro and in vivo: The in vitro bursting behavior is consistent with a state generated by very low network input ({\textless}0.1\%), whereas in vivo activity suggests that on the order of 1\% recorded spikes are input driven, resulting in reverberating dynamics. Importantly, this predicts that one can abolish the ubiquitous bursts of in vitro preparations, and instead impose dynamics comparable to in vivo activity by exposing the system to weak long-term stimulation, thereby opening new paths to establish an in vivo-like assay in vitro for basic as well as neurological studies.},
	number = {3},
	urldate = {2023-02-23},
	journal = {Physical Review X},
	author = {Zierenberg, Johannes and Wilting, Jens and Priesemann, Viola},
	month = jul,
	year = {2018},
	note = {Publisher: American Physical Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {031018},
}

@article{mikulasch_local_2021,
	title = {Local dendritic balance enables learning of efficient representations in networks of spiking neurons},
	volume = {118},
	copyright = {Copyright © 2021 the Author(s). Published by PNAS.},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2021925118},
	doi = {10.1073/pnas.2021925118},
	abstract = {How can neural networks learn to efficiently represent complex and high-dimensional
inputs via local plasticity mechanisms? Classical models of rep...},
	language = {EN},
	number = {50},
	urldate = {2023-02-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Priesemann, Viola},
	month = dec,
	year = {2021},
	note = {Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences
Publisher: Proceedings of the National Academy of Sciences},
	keywords = {⛔ No INSPIRE recid found},
	pages = {e2021925118},
}

@article{mikulasch_where_2023,
	title = {Where is the error? {Hierarchical} predictive coding through dendritic error computation},
	volume = {46},
	issn = {0166-2236},
	shorttitle = {Where is the error?},
	url = {https://www.sciencedirect.com/science/article/pii/S0166223622001862},
	doi = {10.1016/j.tins.2022.09.007},
	abstract = {Top-down feedback in cortex is critical for guiding sensory processing, which has prominently been formalized in the theory of hierarchical predictive coding (hPC). However, experimental evidence for error units, which are central to the theory, is inconclusive and it remains unclear how hPC can be implemented with spiking neurons. To address this, we connect hPC to existing work on efficient coding in balanced networks with lateral inhibition and predictive computation at apical dendrites. Together, this work points to an efficient implementation of hPC with spiking neurons, where prediction errors are computed not in separate units, but locally in dendritic compartments. We then discuss the correspondence of this model to experimentally observed connectivity patterns, plasticity, and dynamics in cortex.},
	language = {en},
	number = {1},
	urldate = {2023-02-23},
	journal = {Trends in Neurosciences},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	month = jan,
	year = {2023},
	keywords = {cortical hierarchy, inference, predictive processing, pyramidal neuron, sensory processing, voltage-dependent plasticity, ⛔ No INSPIRE recid found},
	pages = {45--59},
}

@article{keeley_modeling_2020,
	series = {Whole-brain interactions between neural circuits},
	title = {Modeling statistical dependencies in multi-region spike train data},
	volume = {65},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438820301720},
	doi = {10.1016/j.conb.2020.11.005},
	abstract = {Neural computations underlying cognition and behavior rely on the coordination of neural activity across multiple brain areas. Understanding how brain areas interact to process information or generate behavior is thus a central question in neuroscience. Here we provide an overview of statistical approaches for characterizing statistical dependencies in multi-region spike train recordings. We focus on two classes of models in particular: regression-based models and shared latent variable models. Regression-based models describe interactions in terms of a directed transformation of information from one region to another. Shared latent variable models, on the other hand, seek to describe interactions in terms of sources that capture common fluctuations in spiking activity across regions. We discuss the advantages and limitations of each of these approaches and future directions for the field. We intend this review to be an introduction to the statistical methods in multi-region models for computational neuroscientists and experimentalists alike.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Current Opinion in Neurobiology},
	author = {Keeley, Stephen L and Zoltowski, David M and Aoi, Mikio C and Pillow, Jonathan W},
	month = dec,
	year = {2020},
	keywords = {⛔ No INSPIRE recid found},
	pages = {194--202},
}

@article{perkel_neuronal_nodate,
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	language = {en},
	author = {Perkel, Donald H and Gerstein, George L and Moore, George P},
	note = {tex.ids= Perkel},
	keywords = {⛔ No INSPIRE recid found, ❓ Multiple DOI},
}

@article{dangelo_event_2022,
	title = {Event driven bio-inspired attentive system for the {iCub} humanoid robot on {SpiNNaker}},
	volume = {2},
	issn = {2634-4386},
	url = {https://dx.doi.org/10.1088/2634-4386/ac6b50},
	doi = {10.1088/2634-4386/ac6b50},
	abstract = {Attention leads the gaze of the observer towards interesting items, allowing a detailed analysis only for selected regions of a scene. A robot can take advantage of the perceptual organisation of the features in the scene to guide its attention to better understand its environment. Current bottom–up attention models work with standard RGB cameras requiring a significant amount of time to detect the most salient item in a frame-based fashion. Event-driven cameras are an innovative technology to asynchronously detect contrast changes in the scene with a high temporal resolution and low latency. We propose a new neuromorphic pipeline exploiting the asynchronous output of the event-driven cameras to generate saliency maps of the scene. In an attempt to further decrease the latency, the neuromorphic attention model is implemented in a spiking neural network on SpiNNaker, a dedicated neuromorphic platform. The proposed implementation has been compared with its bio-inspired GPU counterpart, and it has been benchmarked against ground truth fixational maps. The system successfully detects items in the scene, producing saliency maps comparable with the GPU implementation. The asynchronous pipeline achieves an average of 16 ms latency to produce a usable saliency map.},
	number = {2},
	journal = {Neuromorphic Computing and Engineering},
	author = {D’Angelo, Giulia and Perrett, Adam and Iacono, Massimiliano and Furber, Steve and Bartolozzi, Chiara},
	month = may,
	year = {2022},
	note = {Publisher: IOP Publishing},
	keywords = {⛔ No INSPIRE recid found},
	pages = {024008},
}

@incollection{geoffrois_learning_1994,
	address = {Boston, MA},
	title = {Learning by {Delay} {Modifications}},
	isbn = {978-1-4615-2714-5},
	url = {https://doi.org/10.1007/978-1-4615-2714-5_22},
	abstract = {In order to account for real time processing by the visual system, it has been proposed that single spike arrival times encode analog information, which can be processed faster than firing rates by a lateral inhibitory network. We show that learning can occur in such a scheme by introducing variable delays, resulting in both fast and adaptive processing. The A-current, able to control delays, and known to be involved in learning, is proposed as a biological substrate. The model offers a complementary mechanism for modeling receptive field plasticity.},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {Computation in {Neurons} and {Neural} {Systems}},
	publisher = {Springer US},
	author = {Geoffrois, Edouard and Edeline, Jean-Marc and Vibert, Jean-François},
	editor = {Eeckman, Frank H.},
	year = {1994},
	doi = {10.1007/978-1-4615-2714-5_22},
	keywords = {⛔ No INSPIRE recid found},
	pages = {133--138},
}

@article{bernert_fully_2017,
	title = {Fully unsupervised online spike sorting based on an artificial spiking neural network},
	url = {https://www.biorxiv.org/content/10.1101/236224v1},
	doi = {10.1101/236224},
	abstract = {Spike sorting is a crucial step of neural data processing widely used in neuroscience and neuroprosthetics. However, current methods remain not fully automatic and require heavy computations making them not embeddable in implantable devices. To overcome these limitations, we propose a novel method based on an artificial spiking neural network designed to process neural data online and completely automatically. An input layer continuously encodes the data stream into artificial spike trains, which are then processed by two further layers to output artificial trains of spikes reproducing the real spiking activity present in the input signal. The proposed method can be adapted to process several channels simultaneously in the case of tetrode recordings. It outperforms two existing algorithms at low SNR and has the advantage to be compatible with neuromorphic computing and the perspective of being embedded in very low-power analog systems for future implantable devices serving neurorehabilitation applications.},
	language = {en},
	urldate = {2022-04-08},
	author = {Bernert, Marie and Yvert, Blaise},
	month = dec,
	year = {2017},
	keywords = {\#nosource},
	pages = {236224},
}

@article{bernert_attention-based_2018,
	title = {An {Attention}-{Based} {Spiking} {Neural} {Network} for {Unsupervised} {Spike}-{Sorting}},
	volume = {29},
	issn = {0129-0657},
	url = {https://www.worldscientific.com/doi/10.1142/S0129065718500594},
	doi = {10.1142/s0129065718500594},
	abstract = {Bio-inspired computing using artificial spiking neural networks promises performances outperforming currently available computational approaches. Yet, the number of applications of such networks remains limited due to the absence of generic training procedures for complex pattern recognition, which require the design of dedicated architectures for each situation. We developed a spike-timing-dependent plasticity (STDP) spiking neural network (SSN) to address spike-sorting, a central pattern recognition problem in neuroscience. This network is designed to process an extracellular neural signal in an online and unsupervised fashion. The signal stream is continuously fed to the network and processed through several layers to output spike trains matching the truth after a short learning period requiring only few data. The network features an attention mechanism to handle the scarcity of action potential occurrences in the signal, and a threshold adaptation mechanism to handle patterns with different sizes. This method outperforms two existing spike-sorting algorithms at low signal-to-noise ratio (SNR) and can be adapted to process several channels simultaneously in the case of tetrode recordings. Such attention-based STDP network applied to spike-sorting opens perspectives to embed neuromorphic processing of neural data in future brain implants.},
	number = {08},
	urldate = {2021-01-26},
	journal = {International Journal of Neural Systems},
	author = {Bernert, Marie and Yvert, Blaise},
	month = dec,
	year = {2018},
	keywords = {\#nosource},
	pages = {1850059},
}

@article{madadi_asl_delay-dependent_2022,
	title = {Delay-dependent transitions of phase synchronization and coupling symmetry between neurons shaped by spike-timing-dependent plasticity},
	issn = {1871-4099},
	url = {https://doi.org/10.1007/s11571-022-09850-x},
	doi = {10.1007/s11571-022-09850-x},
	abstract = {Synchronization plays a key role in learning and memory by facilitating the communication between neurons promoted by synaptic plasticity. Spike-timing-dependent plasticity (STDP) is a form of synaptic plasticity that modifies the strength of synaptic connections between neurons based on the coincidence of pre- and postsynaptic spikes. In this way, STDP simultaneously shapes the neuronal activity and synaptic connectivity in a feedback loop. However, transmission delays due to the physical distance between neurons affect neuronal synchronization and the symmetry of synaptic coupling. To address the question that how transmission delays and STDP can jointly determine the emergent pairwise activity-connectivity patterns, we studied phase synchronization properties and coupling symmetry between two bidirectionally coupled neurons using both phase oscillator and conductance-based neuron models. We show that depending on the range of transmission delays, the activity of the two-neuron motif can achieve an in-phase/anti-phase synchronized state and its connectivity can attain a symmetric/asymmetric coupling regime. The coevolutionary dynamics of the neuronal system and the synaptic weights due to STDP stabilizes the motif in either one of these states by transitions between in-phase/anti-phase synchronization states and symmetric/asymmetric coupling regimes at particular transmission delays. These transitions crucially depend on the phase response curve (PRC) of the neurons, but they are relatively robust to the heterogeneity of transmission delays and potentiation-depression imbalance of the STDP profile.},
	language = {en},
	urldate = {2022-09-18},
	journal = {Cognitive Neurodynamics},
	author = {Madadi Asl, Mojtaba and Ramezani Akbarabadi, Saeideh},
	month = jul,
	year = {2022},
	keywords = {\#nosource, Coupling symmetry, Spike-timing-dependent plasticity, Synaptic plasticity, Synchronization, Transmission delay},
}

@article{warner_probabilistic_2022,
	title = {A probabilistic latent variable model for detecting structure in binary data},
	url = {http://arxiv.org/abs/2201.11108},
	abstract = {We introduce a novel, probabilistic binary latent variable model to detect noisy or approximate repeats of patterns in sparse binary data. The model is based on the ”Noisy-OR model” [5], used previously for disease and topic modelling. The model’s capability is demonstrated by extracting structure in recordings from retinal neurons, but it can be widely applied to discover and model latent structure in noisy binary data. In the context of spiking neural data, the task is to “explain” spikes of individual neurons in terms of groups of neurons, ”Cell Assemblies” (CAs), that often fire together, due to mutual interactions or other causes. The model infers sparse activity in a set of binary latent variables, each describing the activity of a cell assembly. When the latent variable of a cell assembly is active, it reduces the probabilities of neurons belonging to this assembly to be inactive. The conditional probability kernels of the latent components are learned from the data in an expectation maximization scheme, involving inference of latent states and parameter adjustments to the model. We thoroughly validate the model on synthesized spike trains constructed to statistically resemble recorded retinal responses to white noise stimulus and natural movie stimulus in data. We also apply our model to spiking responses recorded in retinal ganglion cells (RGCs) during stimulation with a movie and discuss the found structure.},
	language = {en},
	urldate = {2022-02-16},
	journal = {arXiv:2201.11108 [cs, q-bio, stat]},
	author = {Warner, Christopher and Ruda, Kiersten and Sommer, Friedrich T.},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.11108},
	keywords = {\#nosource, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{ghosh_synchronization_2021,
	title = {Synchronization in time-varying networks},
	url = {http://arxiv.org/abs/2109.07618},
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	urldate = {2021-09-21},
	journal = {arXiv:2109.07618 [physics]},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	month = sep,
	year = {2021},
	note = {00000 
arXiv: 2109.07618},
	keywords = {\#nosource},
}

@article{grossberger_unsupervised_2018,
	title = {Unsupervised clustering of temporal patterns in high-dimensional neuronal ensembles using a novel dissimilarity measure},
	volume = {14},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006283},
	doi = {10/gdvbsx},
	abstract = {Temporally ordered multi-neuron patterns likely encode information in the brain. We introduce an unsupervised method, SPOTDisClust (Spike Pattern Optimal Transport Dissimilarity Clustering), for their detection from high-dimensional neural ensembles. SPOTDisClust measures similarity between two ensemble spike patterns by determining the minimum transport cost of transforming their corresponding normalized cross-correlation matrices into each other (SPOTDis). Then, it performs density-based clustering based on the resulting inter-pattern dissimilarity matrix. SPOTDisClust does not require binning and can detect complex patterns (beyond sequential activation) even when high levels of out-of-pattern “noise” spiking are present. Our method handles efficiently the additional information from increasingly large neuronal ensembles and can detect a number of patterns that far exceeds the number of recorded neurons. In an application to neural ensemble data from macaque monkey V1 cortex, SPOTDisClust can identify different moving stimulus directions on the sole basis of temporal spiking patterns.},
	language = {en},
	number = {7},
	urldate = {2021-11-30},
	journal = {PLOS Computational Biology},
	author = {Grossberger, Lukas and Battaglia, Francesco P. and Vinck, Martin},
	year = {2018},
	keywords = {\#nosource},
	pages = {e1006283},
}

@article{huning_synaptic_1998,
	title = {Synaptic {Delay} {Learning} in {Pulse}-{Coupled} {Neurons}},
	volume = {10},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976698300017665},
	doi = {10/cthfps},
	abstract = {We present rules for the unsupervised learning of coincidence between excitatory postsynaptic potentials (EPSPs) by the adjustment of post-synaptic delays between the transmitter binding and the opening of ion channels. Starting from a gradient descent scheme, we develop a robust and more biological threshold rule by which EPSPs from different synapses can be gradually pulled into coincidence. The synaptic delay changes are determined from the summed potential—at the site where the coincidence is to be established—and from postulated synaptic learning functions that accompany the individual EPSPs. According to our scheme, templates for the detection of spatiotemporal patterns of synaptic activation can be learned, which is demonstrated by computer simulation. Finally, we discuss possible relations to biological mechanisms.},
	number = {3},
	urldate = {2021-09-16},
	journal = {Neural Computation},
	author = {Hüning, Harald and Glünder, Helmut and Palm, Günther},
	month = apr,
	year = {1998},
	keywords = {\#nosource},
	pages = {555--565},
}

@article{izhikevich_polychronous_2009,
	title = {Polychronous {Wavefront} {Computations}},
	volume = {19},
	issn = {0218-1274, 1793-6551},
	url = {https://www.izhikevich.org/publications/polychronous_wavefront_computations.htm},
	doi = {10/db98d7},
	abstract = {There is great interest in methods for computing that do not involve digital machines. Many computational paradigms were inspired by brain research, such as Boolean neuronal logic [McCulloch \& Pitts, 1943], the perceptron [Rosenblatt, 1958], attractor neural networks [Hopfield, 1982] and cellular neural nets [Chua \& Yang, 1988]. All these paradigms abstract biological circuits to artificial neural networks, i.e. interconnected units (neurons) that perform computations based on the connections between the units (synapses). Here we present a novel computational framework based on polychronous wavefront dynamics. It is entirely different from an artificial neural network paradigm, rather it is based on temporal and spatial patterns of activity in pulse-propagating media and their interaction with transponders, which create pulses in response to receiving appropriate inputs, e.g. two coincident input pulses. A pulse propagates as a circular wave from its source to other transponders. Computations result from interactions between transponders, and they are encoded by the exact physical locations of transponders and by precise timings of pulses. We illustrate temporal pattern recognition, reverberating memory, temporal signal analysis and basic logical operations using polychronous wavefront computations. This work reveals novel principles for designing nanoscale computational devices.},
	language = {en},
	number = {05},
	urldate = {2019-09-10},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Izhikevich, Eugene M. and Hoppensteadt, Frank C.},
	month = may,
	year = {2009},
	keywords = {\#nosource, polychronization},
	pages = {1733--1739},
}

@article{izhikevich_polychronization_2006,
	title = {Polychronization: {Computation} with {Spikes}},
	volume = {18},
	issn = {0899-7667},
	shorttitle = {Polychronization},
	url = {https://doi.org/10.1162/089976606775093882},
	doi = {10/bgh4qv},
	abstract = {We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as “attention to memories.”},
	number = {2},
	urldate = {2018-09-24},
	journal = {Neural Computation},
	author = {Izhikevich, Eugene M.},
	month = feb,
	year = {2006},
	note = {00000},
	keywords = {\#nosource},
	pages = {245--282},
}

@article{foss_multistability_2000,
	title = {Multistability in {Recurrent} {Neural} {Loops} {Arising} {From} {Delay}},
	volume = {84},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.2000.84.2.975},
	doi = {10/gmvbsh},
	abstract = {The dynamics of a recurrent inhibitory neural loop composed of a periodically spiking Aplysia motoneuron reciprocally connected to a computer are investigated as a function of the time delay, τ, for propagation around the loop. It is shown that for certain choices of τ, multiple qualitatively different neural spike trains co-exist. A mathematical model is constructed for the dynamics of this pulsed-coupled recurrent loop in which all parameters are readily measured experimentally: the phase resetting curve of the neuron for a given simulated postsynaptic current and τ. For choices of the parameters for which multiple spiking patterns co-exist in the experimental paradigm, the model exhibits multistability. Numerical simulations suggest that qualitatively similar results will occur if the motoneuron is replaced by several other types of neurons and that once τ becomes sufficiently long, multistability will be the dominant form of dynamical behavior. These observations suggest that great care must be taken in determining the etiology of qualitative changes in neural spiking patterns, particularly when propagation times around polysynaptic loops are long.},
	language = {en},
	number = {2},
	urldate = {2021-09-16},
	journal = {Journal of Neurophysiology},
	author = {Foss, Jennifer and Milton, John},
	month = aug,
	year = {2000},
	keywords = {\#nosource},
	pages = {975--985},
}

@article{haessig_event-based_2020,
	title = {Event-{Based} {Computation} for {Touch} {Localization} {Based} on {Precise} {Spike} {Timing}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00420},
	doi = {10/ghvnxj},
	abstract = {Precise spike timing and temporal coding are used extensively within the nervous system of insects and in the sensory periphery of higher order animals. However, conventional Artificial Neural Networks (ANNs) and machine learning algorithms cannot take advantage of this coding strategy, due to their rate-based representation of signals. Even in the case of artificial Spiking Neural Networks (SNNs), identifying applications where temporal coding outperforms the rate coding strategies of ANNs is still an open challenge. Neuromorphic sensory-processing systems provide an ideal context for exploring the potential advantages of temporal coding, as they are able to efficiently extract the information required to cluster or classify spatio-temporal activity patterns from relative spike timing. Here we propose a neuromorphic model inspired by the sand scorpion to explore the benefits of temporal coding, and validate it in an event-based sensory-processing task. The task consists in localizing a target using only the relative spike timing of eight spatially-separated vibration sensors. We propose two different approaches in which the SNNs learns to cluster spatio-temporal patterns in an unsupervised manner and we demonstrate how the task can be solved both analytically and through numerical simulation of multiple SNN models. We argue that the models presented are optimal for spatio-temporal pattern classification using precise spike timing in a task that could be used as a standard benchmark for evaluating event-based sensory processing models based on temporal coding.},
	urldate = {2021-10-20},
	journal = {Frontiers in Neuroscience},
	author = {Haessig, Germain and Milde, Moritz B. and Aceituno, Pau Vilimelis and Oubari, Omar and Knight, James C. and van Schaik, André and Benosman, Ryad B. and Indiveri, Giacomo},
	year = {2020},
	keywords = {\#nosource},
	pages = {420},
}

@article{eurich_dynamics_1999,
	title = {Dynamics of {Self}-{Organized} {Delay} {Adaptation}},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1594},
	doi = {10/dj9c7q},
	abstract = {Adaptation of interaction delays is essential for the functioning of many natural and technical systems. We introduce a novel framework for studying the dynamics of delay adaptation in systems which optimize coincidence of inputs. For the important case of periodically modulated input we derive conditions for the existence and stability of solutions which constrain the set of mechanisms for reliable delay adaptation. Using numerical examples we show that our approach is applicable to more general than periodic input patterns such as Poissonian point processes with coordinated rate fluctuations.},
	number = {7},
	urldate = {2021-09-16},
	journal = {Physical Review Letters},
	author = {Eurich, Christian W. and Pawelzik, Klaus and Ernst, Udo and Cowan, Jack D. and Milton, John G.},
	month = feb,
	year = {1999},
	note = {00082 
Publisher: American Physical Society},
	keywords = {\#nosource},
	pages = {1594--1597},
}

@book{flourens_recherches_1842,
	title = {Recherches expérimentales sur les propriétés et les fonctions du système nerveux, dans les animaux vertébrés},
	url = {https://archive.org/details/b30796520/page/n7/mode/2up},
	language = {fr},
	publisher = {J.-B. Balliere},
	author = {Flourens, Marie Jean Pierre},
	year = {1842},
}

@misc{jeremie_ultrafast_2022,
	title = {Ultrafast image categorization in biology and neural models},
	url = {http://arxiv.org/abs/2205.03635},
	abstract = {Humans are able to categorize images very efficiently, in particular to detect very rapidly the presence of an animal. Recently, deep learning algorithms have achieved higher accuracy than humans for a large set of visual recognition tasks. However, the tasks on which these artificial networks are usually trained and evaluated are usually very specialized which do not generalize well, for example with an accuracy drop following a rotation of the image. In this regard, biological visual systems are more flexible and efficient than artificial systems for more generic tasks, such as detecting an animal. To further the comparison between biological and artificial neural networks, we retrained the standard VGG16 convolutional neural network (CNN) on two independent tasks that are ecologically relevant to humans: detecting the presence of an animal or an artifact. We show that retraining the network achieves a human-like level of performance, comparable to what is reported in psychophysical tasks. Moreover, we show that categorization is better when combining the models' outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). Furthermore, these re-trained models were able to reproduce some unexpected behavioral observations of human psychophysics, such as robustness to rotations (e.g., an upside-down or tilted image) or to a grayscale transformation. Finally, we quantified the number of CNN layers needed to achieve such performance, showing that good accuracy for ultrafast image categorization could be achieved with only a few layers, challenging the belief that image recognition would require a deep sequential analysis of visual objects.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Jérémie, Jean-Nicolas and Perrinet, Laurent U.},
	month = oct,
	year = {2022},
	note = {arXiv:2205.03635 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
}

@article{kass_statistical_2005,
	title = {Statistical issues in the analysis of neuronal data},
	volume = {94},
	doi = {10.1152/jn.00648.2004},
	number = {1},
	journal = {Journal of neurophysiology},
	author = {Kass, Robert E. and Ventura, Valérie and Brown, Emery N.},
	year = {2005},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {8--25},
}

@misc{stringer_mouselandrastermap_2020,
	title = {{MouseLand}/rastermap: {A} multi-dimensional embedding algorithm},
	url = {https://github.com/MouseLand/rastermap},
	urldate = {2022-12-19},
	author = {Stringer, Carsen},
	year = {2020},
	keywords = {⛔ No INSPIRE recid found},
}

@article{simoncelli_characterization_2003,
	title = {Characterization of {Neural} {Responses} with {Stochastic} {Stimuli}},
	url = {http://pillowlab.princeton.edu/pubs/simoncelli03c-preprint.pdf},
	language = {en},
	author = {Simoncelli, Eero P and Paninski, Liam and Pillow, Jonathan and Schwartz, Odelia},
	year = {2003},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@inproceedings{deneve_bayesian_2004,
	title = {Bayesian inference in spiking neurons},
	volume = {17},
	url = {https://papers.nips.cc/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html},
	abstract = {We propose a new interpretation of spiking neurons as Bayesian integra-          tors accumulating evidence over time about events in the external world          or the body, and communicating to other neurons their certainties about          these events. In this model, spikes signal the occurrence of new infor-          mation, i.e. what cannot be predicted from the past activity. As a result,          firing statistics are close to Poisson, albeit providing a deterministic rep-          resentation of probabilities. We proceed to develop a theory of Bayesian          inference in spiking neural networks, recurrent interactions implement-          ing a variant of belief propagation.},
	urldate = {2022-12-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Deneve, Sophie},
	year = {2004},
	keywords = {⛔ No DOI found, ⛔ No INSPIRE recid found},
}

@misc{nadafian_bio-plausible_2020,
	title = {Bio-plausible {Unsupervised} {Delay} {Learning} for {Extracting} {Temporal} {Features} in {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2011.09380},
	abstract = {The plasticity of the conduction delay between neurons plays a fundamental role in learning. However, the exact underlying mechanisms in the brain for this modulation is still an open problem. Understanding the precise adjustment of synaptic delays could help us in developing effective brain-inspired computational models in providing aligned insights with the experimental evidence. In this paper, we propose an unsupervised biologically plausible learning rule for adjusting the synaptic delays in spiking neural networks. Then, we provided some mathematical proofs to show that our learning rule gives a neuron the ability to learn repeating spatio-temporal patterns. Furthermore, the experimental results of applying an STDP-based spiking neural network equipped with our proposed delay learning rule on Random Dot Kinematogram indicate the efficacy of the proposed delay learning rule in extracting temporal features.},
	publisher = {arXiv},
	author = {Nadafian, Alireza and Ganjtabesh, Mohammad},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, ⛔ No INSPIRE recid found},
}

@article{perrinet_sparse_2004,
	series = {New {Aspects} in {Neurocomputing}: 10th {European} {Symposium} on {Artificial} {Neural} {Networks} 2002},
	title = {Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit},
	volume = {57},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231204000670},
	doi = {10.1016/j.neucom.2004.01.010},
	abstract = {In order to account for the rapidity of visual processing, we explore visual coding strategies using a one-pass feed-forward spiking neural network. We based our model on the work of Van Rullen and Thorpe Neural Comput. 13 (6) (2001) 1255, which constructs a retinal representation using an orthogonal wavelet transform. This strategy provides a spike code, thanks to a rank order coding scheme which offers an alternative to the classical spike frequency coding scheme. We extended this model to efficient representations in arbitrary linear generative models by implementing lateral interactions on top of this feed-forward model. This method uses a matching pursuit scheme—recursively detecting in the image the best match with the elements of a dictionary and then subtracting it—and which may similarly define a visual spike code. In particular, this transform could be used with large and arbitrary dictionaries, so that we may define an over-complete representation which may define an efficient sparse spike coding scheme in arbitrary multi-layered architectures. We show here extensions of this method of computing with spike events, introducing an adaptive scheme leading to the emergence of V1-like receptive fields and then a model of bottom-up saliency pursuit.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Neurocomputing},
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	month = mar,
	year = {2004},
	keywords = {Natural images statistics, Parallel asynchronous processing, Sparse coding, Ultra-rapid categorization, Vision, Wavelet Hansform, ⛔ No INSPIRE recid found},
	pages = {125--134},
}

@article{perrinet_coding_2004,
	title = {Coding static natural images using spiking event times: do neurons cooperate?},
	volume = {15},
	doi = {10.1109/TNN.2004.833303},
	number = {5},
	journal = {IEEE Transactions on neural networks},
	author = {Perrinet, Laurent and Samuelides, Manuel and Thorpe, Simon},
	year = {2004},
	note = {Publisher: IEEE},
	keywords = {Biomembranes, Central nervous system, Filters, Fires, Image coding, Image reconstruction, Neurons, Retina, Statistics, Wavelet transforms, ⛔ No INSPIRE recid found},
	pages = {1164--1175},
}

@article{konishi_coding_2003,
	title = {Coding of auditory space},
	volume = {26},
	issn = {0147-006X},
	doi = {10.1146/annurev.neuro.26.041002.131123},
	abstract = {Behavioral, anatomical, and physiological approaches can be integrated in the study of sound localization in barn owls. Space representation in owls provides a useful example for discussion of place and ensemble coding. Selectivity for space is broad and ambiguous in low-order neurons. Parallel pathways for binaural cues and for different frequency bands converge on high-order space-specific neurons, which encode space more precisely. An ensemble of broadly tuned place-coding neurons may converge on a single high-order neuron to create an improved labeled line. Thus, the two coding schemes are not alternate methods. Owls can localize sounds by using either the isomorphic map of auditory space in the midbrain or forebrain neural networks in which space is not mapped.},
	language = {eng},
	journal = {Annual Review of Neuroscience},
	author = {Konishi, Masakazu},
	year = {2003},
	pmid = {14527266},
	keywords = {Animals, Auditory Pathways, Auditory Perception, Behavior, Animal, Brain, Brain Mapping, Forms and Records Control, Models, Neurological, Nerve Net, Neural Inhibition, Sound Localization, Space Perception, Strigiformes, ⛔ No INSPIRE recid found},
	pages = {31--55},
}

@article{lazar_time_2004,
	title = {Time encoding with an integrate-and-fire neuron with a refractory period},
	volume = {58-60},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231204000177},
	doi = {10.1016/j.neucom.2004.01.022},
	abstract = {Time encoding is a formal method of mapping amplitude information into a time sequence. We show that under simple conditions, bandlimited stimuli encoded with an integrate-and-ÿre neuron with an absolute refractory period can be recovered loss-free from the neural spike train at its output. We provide an algorithm for perfect recovery and derive conditions for its convergence. c 2003 Elsevier B.V. All rights reserved.},
	language = {en},
	urldate = {2022-12-19},
	journal = {Neurocomputing},
	author = {Lazar, Aurel A.},
	month = jun,
	year = {2004},
	keywords = {⛔ No INSPIRE recid found},
	pages = {53--58},
}

@article{golding_dendritic_2002,
	title = {Dendritic spikes as a mechanism for cooperative long-term potentiation},
	volume = {418},
	copyright = {2002 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature00854},
	doi = {10.1038/nature00854},
	abstract = {Strengthening of synaptic connections following coincident pre- and postsynaptic activity was proposed by Hebb as a cellular mechanism for learning1. Contemporary models assume that multiple synapses must act cooperatively to induce the postsynaptic activity required for hebbian synaptic plasticity2,3,4,5. One mechanism for the implementation of this cooperation is action potential firing, which begins in the axon, but which can influence synaptic potentiation following active backpropagation into dendrites6. Backpropagation is limited, however, and action potentials often fail to invade the most distal dendrites7,8,9,10. Here we show that long-term potentiation of synapses on the distal dendrites of hippocampal CA1 pyramidal neurons does require cooperative synaptic inputs, but does not require axonal action potential firing and backpropagation. Rather, locally generated and spatially restricted regenerative potentials (dendritic spikes) contribute to the postsynaptic depolarization and calcium entry necessary to trigger potentiation of distal synapses. We find that this mechanism can also function at proximal synapses, suggesting that dendritic spikes participate generally in a form of synaptic potentiation that does not require postsynaptic action potential firing in the axon.},
	language = {en},
	number = {6895},
	urldate = {2022-12-16},
	journal = {Nature},
	author = {Golding, Nace L. and Staff, Nathan P. and Spruston, Nelson},
	month = jul,
	year = {2002},
	note = {Number: 6895
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {326--331},
}

@article{mel_synaptic_2017,
	title = {Synaptic plasticity in dendrites: complications and coping strategies},
	volume = {43},
	issn = {1873-6882},
	shorttitle = {Synaptic plasticity in dendrites},
	doi = {10.1016/j.conb.2017.03.012},
	abstract = {The elaborate morphology, nonlinear membrane mechanisms and spatiotemporally varying synaptic activation patterns of dendrites complicate the expression, compartmentalization and modulation of synaptic plasticity. To grapple with this complexity, we start with the observation that neurons in different brain areas face markedly different learning problems, and dendrites of different neuron types contribute to the cell's input-output function in markedly different ways. By committing to specific assumptions regarding a neuron's learning problem and its input-output function, specific inferences can be drawn regarding the synaptic plasticity mechanisms and outcomes that we 'ought' to expect for that neuron. Exploiting this assumption-driven approach can help both in interpreting existing experimental data and designing future experiments aimed at understanding the brain's myriad learning processes.},
	language = {eng},
	journal = {Current Opinion in Neurobiology},
	author = {Mel, Bartlett W. and Schiller, Jackie and Poirazi, Panayiota},
	month = apr,
	year = {2017},
	pmid = {28453975},
	keywords = {Dendrites, Humans, Learning, Models, Neurological, Neuronal Plasticity, Synapses, ⛔ No INSPIRE recid found},
	pages = {177--186},
}

@article{jeffress_place_1948,
	title = {A place theory of sound localization},
	volume = {41},
	issn = {0021-9940},
	doi = {10.1037/h0061495},
	abstract = {The author presents a place theory of sound localization based upon the time difference of stimulation of the 2 ears. The hypothesis depends upon the known slow rate of conduction in small nerve fibers and the phenomenon of spatial summation. It assumes that some secondary fibers of the auditory tract divide, sending branches homolaterally and contralaterally. There is a further assumption that these neurons make synaptic connection with other fibers on each side, the latter neurones synapsing with both contralateral and homolateral neurones. Then, if the sound is in a median plane, the summation effect would be maximal in a central group of the tertiary fibers on each side. If the sound source is shifted, the summation effect would result in a shifting of the transmission through synapses in the tertiary zone. This provides a spatial change in the pattern of nerve discharge as a consequence of a temporal change in the binaural stimulation. The anatomical location of such a center is suggested either in the inferior colliculus or the medial geniculate body. Possible experimental procedures are suggested. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Journal of Comparative and Physiological Psychology},
	author = {Jeffress, Lloyd A.},
	year = {1948},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Auditory Localization, Ear (Anatomy), Neurons, Temporal Frequency, ⛔ No INSPIRE recid found},
	pages = {35--39},
}

@incollection{middlebrooks_chapter_2015,
	series = {The {Human} {Auditory} {System}},
	title = {Chapter 6 - {Sound} localization},
	volume = {129},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444626301000068},
	abstract = {The auditory system derives locations of sound sources from spatial cues provided by the interaction of sound with the head and external ears. Those cues are analyzed in specific brainstem pathways and then integrated as cortical representation of locations. The principal cues for horizontal localization are interaural time differences (ITDs) and interaural differences in sound level (ILDs). Vertical and front/back localization rely on spectral-shape cues derived from direction-dependent filtering properties of the external ears. The likely first sites of analysis of these cues are the medial superior olive (MSO) for ITDs, lateral superior olive (LSO) for ILDs, and dorsal cochlear nucleus (DCN) for spectral-shape cues. Localization in distance is much less accurate than that in horizontal and vertical dimensions, and interpretation of the basic cues is influenced by additional factors, including acoustics of the surroundings and familiarity of source spectra and levels. Listeners are quite sensitive to sound motion, but it remains unclear whether that reflects specific motion detection mechanisms or simply detection of changes in static location. Intact auditory cortex is essential for normal sound localization. Cortical representation of sound locations is highly distributed, with no evidence for point-to-point topography. Spatial representation is strictly contralateral in laboratory animals that have been studied, whereas humans show a prominent right-hemisphere dominance.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	author = {Middlebrooks, John C.},
	editor = {Aminoff, Michael J. and Boller, François and Swaab, Dick F.},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-444-62630-1.00006-8},
	keywords = {HRTF, Spatial hearing, auditory cortex, auditory motion, distance perception, interaural level difference, interaural time difference, precedence effect, superior colliculus, superior olivary complex, ⛔ No INSPIRE recid found},
	pages = {99--116},
}

@article{rinberg_speed-accuracy_2006,
	title = {Speed-{Accuracy} {Tradeoff} in {Olfaction}},
	volume = {51},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627306005538},
	doi = {10.1016/j.neuron.2006.07.013},
	abstract = {The basic psychophysical principle of speed-accuracy tradeoff (SAT) has been used to understand key aspects of neuronal information processing in vision and audition, but the principle of SAT is still debated in olfaction. In this study we present the direct observation of SAT in olfaction. We developed a behavioral paradigm for mice in which both the duration of odorant sampling and the difficulty of the odor discrimination task were controlled by the experimenter. We observed that the accuracy of odor discrimination increases with the duration of imposed odorant sampling, and that the rate of this increase is slower for harder tasks. We also present a unifying picture of two previous, seemingly disparate experiments on timing of odorant sampling in odor discrimination tasks. The presence of SAT in olfaction provides strong evidence for temporal integration in olfaction and puts a constraint on models of olfactory processing.},
	language = {en},
	number = {3},
	urldate = {2022-12-16},
	journal = {Neuron},
	author = {Rinberg, Dmitry and Koulakov, Alexei and Gelperin, Alan},
	month = aug,
	year = {2006},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	pages = {351--358},
}

@incollection{cleland_construction_2014,
	title = {Construction of {Odor} {Representations} by {Olfactory} {Bulb} {Microcircuits}},
	volume = {208},
	isbn = {978-0-444-63350-7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444633507000073},
	abstract = {Like other sensory systems, the olfactory system transduces specific features of the external environment and must construct an organized sensory representation from these highly fragmented inputs. As with these other systems, this representation is not accurate per se, but is constructed for utility, and emphasizes certain, presumably useful, features over others. I here describe the cellular and circuit mechanisms of the peripheral olfactory system that underlie this process of sensory construction, emphasizing the distinct architectures and properties of the two prominent computational layers in the olfactory bulb. Notably, while the olfactory system solves essentially similar conceptual problems to other sensory systems, such as contrast enhancement, activity normalization, and extending dynamic range, its peculiarities often require qualitatively different computational algorithms than are deployed in other sensory modalities. In particular, the olfactory modality is intrinsically high dimensional, and lacks a simple, externally defined basis analogous to wavelength or pitch on which elemental odor stimuli can be quantitatively compared. Accordingly, the quantitative similarities of the receptive fields of different odorant receptors (ORs) vary according to the statistics of the odor environment. To resolve these unusual challenges, the olfactory bulb appears to utilize unique nontopographical computations and intrinsic learning mechanisms to perform the necessary high-dimensional, similarity-dependent computations. In sum, the early olfactory system implements a coordinated set of early sensory transformations directly analogous to those in other sensory systems, but accomplishes these with unique circuit architectures adapted to the properties of the olfactory modality.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Cleland, Thomas A.},
	year = {2014},
	doi = {10.1016/B978-0-444-63350-7.00007-3},
	keywords = {⛔ No INSPIRE recid found},
	pages = {177--203},
}

@article{kashiwadani_synchronized_1999,
	title = {Synchronized {Oscillatory} {Discharges} of {Mitral}/{Tufted} {Cells} {With} {Different} {Molecular} {Receptive} {Ranges} in the {Rabbit} {Olfactory} {Bulb}},
	volume = {82},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.1999.82.4.1786},
	doi = {10.1152/jn.1999.82.4.1786},
	abstract = {Individual glomeruli in the mammalian olfactory bulb represent a single or a few type(s) of odorant receptors. Signals from different types of receptors are thus sorted out into different glomeruli. How does the neuronal circuit in the olfactory bulb contribute to the combination and integration of signals received by different glomeruli? Here we examined electrophysiologically whether there were functional interactions between mitral/tufted cells associated with different glomeruli in the rabbit olfactory bulb. First, we made simultaneous recordings of extracellular single-unit spike responses of mitral/tufted cells and oscillatory local field potentials in the dorsomedial fatty acid–responsive region of the olfactory bulb in urethan-anesthetized rabbits. Using periodic artificial inhalation, the olfactory epithelium was stimulated with a homologous series ofn-fatty acids or n-aliphatic aldehydes. The odor-evoked spike discharges of mitral/tufted cells tended to phase-lock to the oscillatory local field potential, suggesting that spike discharges of many cells occur synchronously during odor stimulation. We then made simultaneous recordings of spike discharges from pairs of mitral/tufted cells located 300–500 μm apart and performed a cross-correlation analysis of their spike responses to odor stimulation. In ∼27\% of cell pairs examined, two cells with distinct molecular receptive ranges showed synchronized oscillatory discharges when olfactory epithelium was stimulated with one or a mixture of odorant(s) effective in activating both. The results suggest that the neuronal circuit in the olfactory bulb causes synchronized spike discharges of specific pairs of mitral/tufted cells associated with different glomeruli and the synchronization of odor-evoked spike discharges may contribute to the temporal binding of signals derived from different types of odorant receptor.},
	number = {4},
	urldate = {2022-12-16},
	journal = {Journal of Neurophysiology},
	author = {Kashiwadani, Hideki and Sasaki, Yasnory F. and Uchida, Naoshige and Mori, Kensaku},
	month = oct,
	year = {1999},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1786--1792},
}

@article{vidyasagar_multiple_1996,
	title = {Multiple mechanisms underlying the orientation selectivity of visual cortical neurones},
	volume = {19},
	issn = {0166-2236},
	url = {https://www.sciencedirect.com/science/article/pii/S016622369620027X},
	doi = {10.1016/S0166-2236(96)20027-X},
	abstract = {For over three decades, the mechanism of orientation selectivity of visual cortical neurones has been hotly debated. While intracortical inhibition has been implicated as playing a vital role, it has been difficult to observe it clearly. On the basis of recent findings, we propose a model in which the visual cortex brings together a number of different mechanisms for generating orientation-selective responses. Orientation biases in the thalamo-cortical input fibres provide an initial weak selectivity either directly in the excitatory input or by acting via cortical interneurones. This weak selectivity of postsynaptic potentials is then amplified by voltage-sensitive conductances of the cell membrane and excitatory and inhibitory intracortical circuitry, resulting in the sharp tuning seen in the spike discharges of visual cortical cells.},
	language = {en},
	number = {7},
	urldate = {2022-12-16},
	journal = {Trends in Neurosciences},
	author = {Vidyasagar, T. R. and Pei, X. and Volgushev, M.},
	month = jul,
	year = {1996},
	keywords = {⛔ No INSPIRE recid found},
	pages = {272--277},
}

@article{chagnac-amitai_horizontal_1989,
	title = {Horizontal spread of synchronized activity in neocortex and its control by {GABA}-mediated inhibition},
	volume = {61},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/abs/10.1152/jn.1989.61.4.747},
	doi = {10.1152/jn.1989.61.4.747},
	abstract = {1. Suppression of GABAA receptor-mediated inhibition disrupts the neural activity of neocortex and can lead to synchronized discharges that mimic those of partial epilepsy. We have studied the role of GABAA-mediated inhibition in controlling the synchronization and horizontal (tangential) spread of cortical activity. 2. Slices of rat SmI were maintained in vitro and focally stimulated in layer VI while recording with a horizontal array of extracellular electrodes. Inhibition was slightly suppressed by adding low concentrations of the GABAA antagonists bicuculline or bicuculline methiodide to the bathing medium. Under control conditions neural activity was narrowly confined to a vertical strip of cortex. The horizontal spread of activity expanded about twofold in the presence of antagonist concentrations (less than or equal to 0.5 microM) that were expected to suppress GABAA function by no more than 10-20\%. 3. At antagonist concentrations between 0.4 and 1.0 microM, evoked epileptiform activity appeared. These threshold-dose epileptiform events showed wide variations in size and duration (even at the same recording site), very variable distances of horizontal propagation, specific sites of propagation failure, reversals of propagation direction, and directional asymmetries in their probability of propagation. This contrasts with activity observed previously (Ref. 9) in high bicuculline concentrations (greater than or equal to 10 microM): large, stereotyped events that propagate reliably without decrement or reflection. 4. Intracellular recordings were obtained from pyramidal neurons in layers II/III in the presence of less than or equal to 1 microM bicuculline. Inhibitory postsynaptic potentials (IPSPs) were observed during both primary evoked responses and propagating epileptiform events and were often comparable in size and duration to those in untreated cortex. Epileptiform field potentials were always correlated with synaptic activity in single cells, but the pattern and type of PSPs varied with the form of the field potentials. Large amplitude epileptiform events coincided with an overwhelming inhibition of upper layer neurons. 5. We conclude that 1) the horizontal spread of normal cortical activity is strongly constrained by GABAA-mediated IPSPs, 2) a relatively small reduction in the efficacy of inhibition leads to a large increase in the spread of excitation, 3) initiation and propagation of synchronized epileptiform activity can occur even in the presence of robust cortical inhibition, and 4) the character of epileptiform activity is strongly affected by the influences of inhibition.},
	number = {4},
	urldate = {2022-12-16},
	journal = {Journal of Neurophysiology},
	author = {Chagnac-Amitai, Y. and Connors, B. W.},
	month = apr,
	year = {1989},
	note = {Publisher: American Physiological Society},
	keywords = {⛔ No INSPIRE recid found},
	pages = {747--758},
}

@article{ballard_dual_2011,
	title = {Dual {Roles} for {Spike} {Signaling} in {Cortical} {Neural} {Populations}},
	volume = {5},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2011.00022},
	abstract = {A prominent feature of signaling in cortical neurons is that of randomness in the action potential. The output of a typical pyramidal cell can be well fit with a Poisson model, and variations in the Poisson rate repeatedly have been shown to be correlated with stimuli. However while the rate provides a very useful characterization of neural spike data, it may not be the most fundamental description of the signaling code. Recent data showing γ frequency range multi-cell action potential correlations, together with spike timing dependent plasticity, are spurring a re-examination of the classical model, since precise timing codes imply that the generation of spikes is essentially deterministic. Could the observed Poisson randomness and timing determinism reflect two separate modes of communication, or do they somehow derive from a single process? We investigate in a timing-based model whether the apparent incompatibility between these probabilistic and deterministic observations may be resolved by examining how spikes could be used in the underlying neural circuits. The crucial component of this model draws on dual roles for spike signaling. In learning receptive fields from ensembles of inputs, spikes need to behave probabilistically, whereas for fast signaling of individual stimuli, the spikes need to behave deterministically. Our simulations show that this combination is possible if deterministic signals using γ latency coding are probabilistically routed through different members of a cortical cell population at different times. This model exhibits standard features characteristic of Poisson models such as orientation tuning and exponential interval histograms. In addition, it makes testable predictions that follow from the γ latency coding.},
	urldate = {2022-12-16},
	journal = {Frontiers in Computational Neuroscience},
	author = {Ballard, Dana and Jehee, Janneke},
	year = {2011},
	keywords = {⛔ No INSPIRE recid found},
}

@article{feller_dynamic_1997,
	title = {Dynamic {Processes} {Shape} {Spatiotemporal} {Properties} of {Retinal} {Waves}},
	volume = {19},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S089662730080940X},
	doi = {10.1016/S0896-6273(00)80940-X},
	abstract = {In the developing mammalian retina, spontaneous waves of action potentials are present in the ganglion cell layer weeks before vision. These waves are known to be generated by a synaptically connected network of amacrine cells and retinal ganglion cells, and exhibit complex spatiotemporal patterns, characterized by shifting domains of coactivation. Here, we present a novel dynamical model consisting of two coupled populations of cells that quantitatively reproduces the experimentally observed domain sizes, interwave intervals, and wavefront velocity profiles. Model and experiment together show that the highly correlated activity generated by retinal waves can be explained by a combination of random spontaneous activation of cells and the past history of local retinal activity.},
	language = {en},
	number = {2},
	urldate = {2022-12-16},
	journal = {Neuron},
	author = {Feller, Marla B. and Butts, Daniel A. and Aaron, Holly L. and Rokhsar, Daniel S. and Shatz, Carla J.},
	month = aug,
	year = {1997},
	keywords = {⛔ No INSPIRE recid found},
	pages = {293--306},
}

@misc{fang_incorporating_2021,
	title = {Incorporating {Learnable} {Membrane} {Time} {Constant} to {Enhance} {Learning} of {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.05785},
	doi = {10.48550/arXiv.2007.05785},
	abstract = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothee and Huang, Tiejun and Tian, Yonghong},
	month = aug,
	year = {2021},
	note = {arXiv:2007.05785 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ⛔ No INSPIRE recid found},
}

@article{vinje_sparse_2000,
	title = {Sparse {Coding} and {Decorrelation} in {Primary} {Visual} {Cortex} {During} {Natural} {Vision}},
	volume = {287},
	url = {https://www.science.org/doi/10.1126/science.287.5456.1273},
	doi = {10.1126/science.287.5456.1273},
	abstract = {Theoretical studies suggest that primary visual cortex (area V1) uses a sparse code to efficiently represent natural scenes. This issue was investigated by recording from V1 neurons in awake behaving macaques during both free viewing of natural scenes and conditions simulating natural vision. Stimulation of the nonclassical receptive field increases the selectivity and sparseness of individual V1 neurons, increases the sparseness of the population response distribution, and strongly decorrelates the responses of neuron pairs. These effects are due to both excitatory and suppressive modulation of the classical receptive field by the nonclassical receptive field and do not depend critically on the spatiotemporal structure of the stimuli. During natural vision, the classical and nonclassical receptive fields function together to form a sparse representation of the visual world. This sparse code may be computationally efficient for both early vision and higher visual processing.},
	number = {5456},
	urldate = {2022-12-15},
	journal = {Science},
	author = {Vinje, William E. and Gallant, Jack L.},
	month = feb,
	year = {2000},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1273--1276},
}

@article{decharms_primary_1996,
	title = {Primary cortical representation of sounds by the coordination of action-potential timing},
	volume = {381},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/381610a0},
	doi = {10.1038/381610a0},
	abstract = {CORTICAL population coding could in principle rely on either the mean rate of neuronal action potentials, or the relative timing of action potentials, or both. When a single sensory stimulus drives many neurons to fire at elevated rates, the spikes of these neurons become tightly synchronized1,2, which could be involved in 'binding' together individual firing-rate feature representations into a unified object percept3. Here we demonstrate that the relative timing of cortical action potentials can signal stimulus features themselves, a function even more basic than feature grouping. Populations of neurons in the primary auditory cortex can coordinate the relative timing of their action potentials such that spikes occur closer together in time during continuous stimuli. In this way cortical neurons can signal stimuli even when their firing rates do not change. Population coding based on relative spike timing can systematically signal stimulus features, it is topographically mapped, and it follows the stimulus time course even where mean firing rate does not.},
	language = {en},
	number = {6583},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {deCharms, R. Christopher and Merzenich, Michael M.},
	month = jun,
	year = {1996},
	note = {Number: 6583
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {610--613},
}

@article{maunsell_functional_1983,
	title = {Functional properties of neurons in middle temporal visual area of the macaque monkey. {I}. {Selectivity} for stimulus direction, speed, and orientation},
	volume = {49},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.1983.49.5.1127},
	doi = {10.1152/jn.1983.49.5.1127},
	abstract = {1. Recordings were made from single units in the middle temporal visual area (MT) of anesthetized, paralyzed macaque monkeys. A computer-driven stimulator was used to make quantitative tests of selectivity for stimulus direction, speed, and orientation. The data were taken from 168 units that were histologically identified as being in MT. 2. The results confirm previous reports of a high degree of direction selectivity in MT. The response above background to stimuli moving in a unit's preferred direction was, an average, 10.9 times that to stimuli moving in the opposite direction. There was a marked tendency for nearby units to have similar preferred directions. 3. Most units were also sharply tuned for the speed of stimulus motion. For some cells the response fell to less than half-maximal at speeds only a factor of two from the optimum; on average, responses were greater than half-maximal only over a 7.7-fold range of speed. The distribution of preferred speeds for different units was unimodal, with a peak near 32 degrees/s; the total range of preferred speeds extended from 2 to 256 degrees/s. Nearby units generally responded best to similar speeds of motion. 4. Most units in MT showed selectivity for stimulus orientation when tested with stationary, flashed bars. However, stationary stimuli generally elicited only brief responses; when averaged over the duration of the stimulus, the responses were much less than those to moving stimuli. The preferred orientation was usually, but not always, perpendicular to the preferred direction of movement. 5. A comparison of the results of the present study with a previous quantitative investigation in the owl monkey shows a striking similarity in response properties in MT of the two species. 6. The presence of both direction and speed selectivity in MT of the macaque suggests that this area is more specialized for the analysis of visual motion than has been previously recognized.},
	language = {en},
	number = {5},
	urldate = {2022-12-15},
	journal = {Journal of Neurophysiology},
	author = {Maunsell, J. H. and Van Essen, D. C.},
	month = may,
	year = {1983},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1127--1147},
}

@article{montemurro_phase--firing_2008,
	title = {Phase-of-{Firing} {Coding} of {Natural} {Visual} {Stimuli} in {Primary} {Visual} {Cortex}},
	volume = {18},
	issn = {0960-9822},
	url = {http://www.cell.com/current-biology/abstract/S0960-9822(08)00168-1},
	doi = {10.1016/j.cub.2008.02.023},
	language = {English},
	number = {5},
	urldate = {2022-12-15},
	journal = {Current Biology},
	author = {Montemurro, Marcelo A. and Rasch, Malte J. and Murayama, Yusuke and Logothetis, Nikos K. and Panzeri, Stefano},
	month = mar,
	year = {2008},
	pmid = {18328702},
	note = {Publisher: Elsevier},
	keywords = {SYSNEURO, ⛔ No INSPIRE recid found},
	pages = {375--380},
}

@article{butts_temporal_2007,
	title = {Temporal precision in the neural code and the timescales of natural vision},
	volume = {449},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature06105},
	doi = {10.1038/nature06105},
	abstract = {In mammalian visual system, spikes evoked by visual stimuli have millisecond-scale timing even though the relevant timescales of visual processing themselves are much slower. It has therefore long been debated whether spike timing itself carries some form of the neural code. Now experiments in the lateral geniculate nucleus of cats, the part of the brain that is the primary processor of visual information, show that spike timing precision is not absolute for all classes of visual stimuli. Rather, the degree of precision is relative to the timescale of the stimulus, and this relatively high level of precision is required to construct an accurate representation of the stimulus.},
	language = {en},
	number = {7158},
	urldate = {2022-12-15},
	journal = {Nature},
	author = {Butts, Daniel A. and Weng, Chong and Jin, Jianzhong and Yeh, Chun-I. and Lesica, Nicholas A. and Alonso, Jose-Manuel and Stanley, Garrett B.},
	month = sep,
	year = {2007},
	note = {Number: 7158
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary, ⛔ No INSPIRE recid found},
	pages = {92--95},
}

@article{gouras_graded_1960,
	title = {Graded potentials of bream retina},
	volume = {152},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363334/},
	abstract = {Images
null},
	number = {3},
	urldate = {2022-12-14},
	journal = {The Journal of Physiology},
	author = {Gouras, P.},
	month = jul,
	year = {1960},
	pmid = {13828605},
	pmcid = {PMC1363334},
	keywords = {⛔ No INSPIRE recid found},
	pages = {487--505},
}

@article{bryant_spike_1976,
	title = {Spike initiation by transmembrane current: a white-noise analysis.},
	volume = {260},
	copyright = {© 1976 The Physiological Society},
	issn = {1469-7793},
	shorttitle = {Spike initiation by transmembrane current},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1976.sp011516},
	doi = {10.1113/jphysiol.1976.sp011516},
	abstract = {1. Those features of a transmembrane current correlated with spike initiation were examined in Aplysia neurones using a Gaussian white-noise stimulus. This stimulus has the advantages that it presents numerous wave forms in random order without prejudgement as to their efficacies, and that it allows straightforward statistical calculations. 2. Stimulation with a repeating segment of Gaussian white-noise current revealed remarkable invariance in the firing times of the tested neurones and indicated a high degree of reliability of their response. 3. Frequencies (less than 5 Hz) involved in spike triggering propagated faithfully for up to several millimetres, justifying intrasomatic current injection to examine spike initiation at the trigger locus. 4. Examination of current wave forms preceding spikes indicated that a wide variety could be effective. Hence, a statistical analysis was performed, including computation of probability densities, averages, standard deviations and correlation coefficients of pairs of current values. Each statistic was displayed as a function of time before the spike. 5. The average current trajectory preceding a spike was multiphasic and depended on the presence and polarity of a d.c. bias. An early relatively small inward- or outward-going phase was followed by a large outward phase before the spike. The early phase tended to oppose the polarity of the d.c. bias. 6. The late outward phase of the average current trajectory reached a maximum 40–75 msec before triggering the action potential (AP) and returned to near zero values at the moment of triggering. The fact that the current peak occurs in advance of the AP may be partially explained by a phase delay between the transmembrane current and potential. The failure of the average current trajectory to return to control values immediately following the peak argues for a positive role of the declining phase in spike triggering. 7. Probability densities preceding spikes were Gaussian, indicating that the average was also the most probable value. Although the densities were broad, confirming that spikes were preceded by a wide variety of current wave forms, their standard deviations were reduced significantly with respect to controls, suggesting preferred status of the average current trajectory in spike triggering. 8. The matrix of correlation coefficients between current pairs suggested that spikes tended to be preceded by wave forms that in part kept close to the average current trajectory and in part preserved its shape. 9. The average first and second derivatives of spike-evoking epochs revealed that current slope and acceleration, respectively, were most crucial in the last 200 msec before spike triggering, and that these dynamic stimulus components were more important for a cell maintained under a depolarizing, rather than a hyperpolarizing bias. 10...},
	language = {en},
	number = {2},
	urldate = {2022-12-13},
	journal = {The Journal of Physiology},
	author = {Bryant, H L and Segundo, J P},
	year = {1976},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1976.sp011516},
	keywords = {⛔ No INSPIRE recid found},
	pages = {279--314},
}

@article{perrinet_feature_2004,
	series = {Decoding and interfacing the brain: from neuronal assemblies to cyborgs},
	title = {Feature detection using spikes: {The} greedy approach},
	volume = {98},
	issn = {0928-4257},
	shorttitle = {Feature detection using spikes},
	url = {https://www.sciencedirect.com/science/article/pii/S0928425705000161},
	doi = {10.1016/j.jphysparis.2005.09.012},
	abstract = {A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (VI) solving this problem in the case where the signal may be described by a robust linear generative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to ‘Matching Pursuit’ and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neurons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this transformation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.},
	language = {en},
	number = {4},
	urldate = {2022-12-13},
	journal = {Journal of Physiology-Paris},
	author = {Perrinet, Laurent},
	month = jul,
	year = {2004},
	keywords = {Distributed probabilistic representation, Inverse linear model, Matching pursuit, Neuronal representation, Over-complete dictionaries, Sparse spike coding, Spike-event computation, ⛔ No INSPIRE recid found},
	pages = {530--539},
}

@book{dilorenzo_spike_2013,
	title = {Spike {Timing}: {Mechanisms} and {Function}},
	isbn = {978-1-4398-3815-0},
	shorttitle = {Spike {Timing}},
	abstract = {Neuronal communication forms the basis for all behavior, from the smallest movement to our grandest thought processes. Among the many mechanisms that support these functions, spike timing is among the most powerful and—until recently—perhaps the least studied. In the last two decades, however, the study of spike timing has exploded. The heightened interest is due to several factors. These include the development of physiological tools for measuring the activity of neural ensembles and analytical tools for assessing and characterizing spike timing. These advances are coupled with a growing appreciation of spike timing’s theoretical importance for the design principles of the brain.  Spike Timing: Mechanisms and Function examines the function of spike timing in sensory, motor, and integrative processes, providing readers with a broad perspective on how spike timing is produced and used by the nervous system. It brings together the work and ideas of leaders in the field to address current thinking as well as future possibilities.   The first section of the book describes the foundation for quantitative analysis and theory. It examines the information contained in spike timing, how it can be quantified, and how neural systems can extract it. The second section explores how input-output relationships are reflected in spike timing across a range of sensory systems.  Drawing together multiple perspectives, including theoretical and computational studies as well as experimental studies in a range of model systems, the book provides a firm background for investigators to consider spike timing as it applies to their own work. It also offers a glimpse of future advances related to mechanisms of spike timing and its role in neural function, such as the development of novel computational technologies.},
	language = {en},
	publisher = {CRC Press},
	author = {DiLorenzo, Patricia M. and Victor, Jonathan D.},
	month = may,
	year = {2013},
	note = {Google-Books-ID: KTHUIMUpQCUC},
	keywords = {Computers / Software Development \& Engineering / Systems Analysis \& Design, Medical / Biotechnology, Science / Life Sciences / Biophysics, Science / Life Sciences / Neuroscience, Science / Physics / General, Technology \& Engineering / Biomedical, ⛔ No INSPIRE recid found},
}

@inproceedings{arnold_conduction_2021,
	title = {Conduction delay plasticity can robustly learn spatiotemporal patterns embedded in noise},
	doi = {10.1109/IJCNN52387.2021.9533934},
	abstract = {Noise and temporal dynamics are ubiquitous in neural systems yet the computational consequences of these two phenomena interacting are not well studied. Temporal dynamics in spiking networks are often considered only implicitly as part of membrane time constants or synaptic transfer functions. We explicitly model temporal structure using plastic conduction delays between neuron's and characterise the influence of different kinds of noise on learning including temporal jitter, dropout, pattern size, and pattern presentation frequency. We simplify the conduction delay plasticity (CDP) rule called synaptic delay variance learning (SDVL) and demonstrate it is robust to several kinds of noise including; internal pattern jitter, number of pattern afferents, pattern presentation rate, and pattern spike dropout. In particular, after unsupervised training the simplified version of SDVL can achieve an accuracy of up to 99.7 percent averaged over 100 trials. These results demonstrate that learning algorithms based on explicitly modelling temporal structure in inputs can be functional and robust for unsupervised learning of spatiotemporal patterns across a range of noise conditions.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Arnold, Joshua and Stratton, Peter and Wiles, Janet},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Biological systems, Conduction delay, Delays, Jitter, Noise measurement, Spatiotemporal phenomena, Training, Transfer functions, delay learning, noise robust, plasticity, spiking neural network, ⛔ No INSPIRE recid found},
	pages = {1--10},
}

@article{rubinsky_spatio-temporal_2008,
	title = {Spatio-temporal motifs ‘remembered’ in neuronal networks following profound hypothermia},
	volume = {21},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608008001202},
	doi = {10.1016/j.neunet.2008.06.008},
	abstract = {Surgical procedures using hypothermic temperatures have been linked to complications such as seizures, impaired mental development and impaired memory. Although there is some evidence that the profound hypothermia ({\textless}12 ∘C) used in these procedures may be contributing to these neurological impairments, skepticism remains because of lack of evidence from experimental studies isolating the effects of hypothermia on neuronal networks. In order to attain a better understanding of profound hypothermia effects on neurons during surgical procedures, we applied cold to a cultured in-vitro neuronal network. The typical pattern of activity of such cultures is in the form of synchronized bursts, in which most of the recorded neurons fire action potentials in a short time period. In most cases, the bursting activity shows one or more repeating precise spatio-temporal patterns (motifs) that are sustained over long periods of time. In this experimental study, neuronal networks grown on microelectrode arrays (MEA) are subjected to profound hypothermia for an hour and the collective dynamics of the network as a whole are assessed. We show, by using a similarity analysis that compares changes in the time delays between neuronal activation at different burst motifs, that neuronal networks survive total inhibition by profound hypothermia and retain their intrinsic synchronized burst motifs even with substantial generalized neuronal degeneration. By applying multiple sessions of cold, we also show a marked monotonic reduction in the rate of burst firing and in the number of spikes of each neuron after each session.},
	language = {en},
	number = {9},
	urldate = {2022-11-19},
	journal = {Neural Networks},
	author = {Rubinsky, Liel and Raichman, Nadav and Lavee, Jacob and Frenk, Hanan and Ben-Jacob, Eshel},
	month = nov,
	year = {2008},
	keywords = {Low temperature, Microelectrode arrays, Neuronal cultures, Synchronized bursting events, ⛔ No INSPIRE recid found},
	pages = {1232--1237},
}

@article{coull_distinction_2022,
	title = {The distinction between temporal order and duration processing, and implications for schizophrenia},
	volume = {1},
	doi = {10.1038/s44159-022-00038-y},
	number = {5},
	journal = {Nature Reviews Psychology},
	author = {Coull, Jennifer T. and Giersch, Anne},
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {⛔ No INSPIRE recid found},
	pages = {257--271},
}

@article{furber_overview_2013,
	title = {Overview of the {SpiNNaker} {System} {Architecture}},
	volume = {62},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/6226357/},
	doi = {10.1109/TC.2012.142},
	number = {12},
	urldate = {2022-11-13},
	journal = {IEEE Transactions on Computers},
	author = {Furber, Steve B. and Lester, David R. and Plana, Luis A. and Garside, Jim D. and Painkras, Eustace and Temple, Steve and Brown, Andrew D.},
	month = dec,
	year = {2013},
	keywords = {⛔ No INSPIRE recid found},
	pages = {2454--2467},
}

@article{merolla_million_2014,
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1254642},
	doi = {10.1126/science.1254642},
	abstract = {Modeling computer chips on real brains
            
              Computers are nowhere near as versatile as our own brains. Merolla
              et al.
              applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.
            
            
              Science
              , this issue p.
              668
            
          , 
            A large-scale computer chip mimics many features of a real brain.
          , 
            Inspired by the brain’s structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	language = {en},
	number = {6197},
	urldate = {2022-11-13},
	journal = {Science},
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	month = aug,
	year = {2014},
	keywords = {⛔ No INSPIRE recid found},
	pages = {668--673},
}

@book{furber_spinnaker_2020,
	title = {{SpiNNaker}: {A} {Spiking} {Neural} {Network} {Architecture}},
	isbn = {978-1-68083-652-3 978-1-68083-653-0},
	shorttitle = {{SpiNNaker}},
	url = {https://nowpublishers.com/article/BookDetails/9781680836523},
	urldate = {2022-11-13},
	publisher = {Now Publishers},
	editor = {Furber, Steve and Bogdan, Petrut},
	year = {2020},
	doi = {10.1561/9781680836523},
	keywords = {⛔ No INSPIRE recid found},
}

@article{davies_loihi_2018,
	title = {Loihi: {A} {Neuromorphic} {Manycore} {Processor} with {On}-{Chip} {Learning}},
	volume = {38},
	issn = {0272-1732, 1937-4143},
	shorttitle = {Loihi},
	url = {https://ieeexplore.ieee.org/document/8259423/},
	doi = {10.1109/MM.2018.112130359},
	number = {1},
	urldate = {2022-11-13},
	journal = {IEEE Micro},
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	month = jan,
	year = {2018},
	keywords = {⛔ No INSPIRE recid found},
	pages = {82--99},
}

@article{fields_myelin_2020,
	title = {Myelin makes memories},
	volume = {23},
	issn = {1546-1726},
	doi = {10.1038/s41593-020-0606-x},
	language = {eng},
	number = {4},
	journal = {Nature Neuroscience},
	author = {Fields, R. Douglas and Bukalo, Olena},
	month = apr,
	year = {2020},
	pmid = {32094969},
	keywords = {Animals, Memory, Memory Consolidation, Mice, Myelin Sheath, ⛔ No INSPIRE recid found},
	pages = {469--470},
}

@article{madadi_asl_dendritic_2018,
	title = {Dendritic and {Axonal} {Propagation} {Delays} {May} {Shape} {Neuronal} {Networks} {With} {Plastic} {Synapses}},
	volume = {9},
	issn = {1664-042X},
	doi = {10.3389/fphys.2018.01849},
	abstract = {Biological neuronal networks are highly adaptive and plastic. For instance, spike-timing-dependent plasticity (STDP) is a core mechanism which adapts the synaptic strengths based on the relative timing of pre- and postsynaptic spikes. In various fields of physiology, time delays cause a plethora of biologically relevant dynamical phenomena. However, time delays increase the complexity of model systems together with the computational and theoretical analysis burden. Accordingly, in computational neuronal network studies propagation delays were often neglected. As a downside, a classic STDP rule in oscillatory neurons without propagation delays is unable to give rise to bidirectional synaptic couplings, i.e., loops or uncoupled states. This is at variance with basic experimental results. In this mini review, we focus on recent theoretical studies focusing on how things change in the presence of propagation delays. Realistic propagation delays may lead to the emergence of neuronal activity and synaptic connectivity patterns, which cannot be captured by classic STDP models. In fact, propagation delays determine the inventory of attractor states and shape their basins of attractions. The results reviewed here enable to overcome fundamental discrepancies between theory and experiments. Furthermore, these findings are relevant for the development of therapeutic brain stimulation techniques aiming at shifting the diseased brain to more favorable attractor states.},
	language = {eng},
	journal = {Frontiers in Physiology},
	author = {Madadi Asl, Mojtaba and Valizadeh, Alireza and Tass, Peter A.},
	year = {2018},
	pmid = {30618847},
	pmcid = {PMC6307091},
	keywords = {living systems, mathematical modeling, propagation delays, spike-timing-dependent plasticity, synchronization, ⛔ No INSPIRE recid found},
	pages = {1849},
}

@article{neftci_surrogate_2019,
	title = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}: {Bringing} the {Power} of {Gradient}-{Based} {Optimization} to {Spiking} {Neural} {Networks}},
	volume = {36},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8891809/},
	doi = {10.1109/MSP.2019.2931595},
	number = {6},
	urldate = {2022-11-14},
	journal = {IEEE Signal Processing Magazine},
	author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
	month = nov,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {51--63},
}

@article{fields_new_2015,
	title = {A new mechanism of nervous system plasticity: activity-dependent myelination},
	volume = {16},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	shorttitle = {A new mechanism of nervous system plasticity},
	url = {https://www.nature.com/articles/nrn4023},
	doi = {10.1038/nrn4023},
	abstract = {The precise timing of impulse transmission along axons is crucial for synaptic plasticity and brain oscillations, and is partly determined by myelin thickness. In this Opinion article, R. Douglas Fields discusses how electrical activity influences myelin thickness and thus conduction velocity and circuit properties.},
	language = {en},
	number = {12},
	urldate = {2021-01-07},
	journal = {Nature Reviews Neuroscience},
	author = {Fields, R. Douglas},
	month = dec,
	year = {2015},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {biology, delay-learning, myelination, ⛔ No INSPIRE recid found},
	pages = {756--767},
}

@article{steadman_disruption_2020,
	title = {Disruption of {Oligodendrogenesis} {Impairs} {Memory} {Consolidation} in {Adult} {Mice}},
	volume = {105},
	issn = {0896-6273},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7579726/},
	doi = {10.1016/j.neuron.2019.10.013},
	abstract = {The generation of myelin-forming oligodendrocytes persists throughout life and is regulated by neural activity. Here we tested whether experience-driven changes in oligodendrogenesis are important for memory consolidation. We found that water maze learning promotes oligodendrogenesis and de
novo myelination in the cortex and associated white matter tracts. Preventing these learning-induced increases in oligodendrogenesis without affecting existing oligodendrocytes impaired memory consolidation of water maze, as well as contextual fear, memories. These results suggest that de
novo myelination tunes activated circuits, promoting coordinated activity that is important for memory consolidation. Consistent with this, contextual fear learning increased the coupling of hippocampal sharp wave ripples and cortical spindles, and these learning-induced increases in ripple-spindle coupling were blocked when oligodendrogenesis was suppressed. Our results identify a non-neuronal form of plasticity that remodels hippocampal-cortical networks following learning and is required for memory consolidation., 
          
        , Experience-dependent de
novo myelination may fine-tune activated circuits by promoting brain synchrony, important for memory consolidation. Steadman et al. find that blocking this form of adaptive myelination prevents learning-induced increases in coordinated activity and impairs memory consolidation.},
	number = {1},
	urldate = {2022-11-14},
	journal = {Neuron},
	author = {Steadman, Patrick E. and Xia, Frances and Ahmed, Moriam and Mocle, Andrew J. and Penning, Amber R.A. and Geraghty, Anna C. and Steenland, Hendrik W. and Monje, Michelle and Josselyn, Sheena A. and Frankland, Paul W.},
	month = jan,
	year = {2020},
	pmid = {31753579},
	pmcid = {PMC7579726},
	keywords = {⛔ No INSPIRE recid found},
	pages = {150--164.e6},
}

@article{pan_preservation_2020,
	title = {Preservation of a remote fear memory requires new myelin formation},
	volume = {23},
	issn = {1546-1726},
	doi = {10.1038/s41593-019-0582-1},
	abstract = {Experience-dependent myelination is hypothesized to shape neural circuit function and subsequent behavioral output. Using a contextual fear memory task in mice, we demonstrate that fear learning induces oligodendrocyte precursor cells to proliferate and differentiate into myelinating oligodendrocytes in the medial prefrontal cortex. Transgenic animals that cannot form new myelin exhibit deficient remote, but not recent, fear memory recall. Recording population calcium dynamics by fiber photometry, we observe that the neuronal response to conditioned context cues evolves over time in the medial prefrontal cortex, but not in animals that cannot form new myelin. Finally, we demonstrate that pharmacological induction of new myelin formation with clemastine fumarate improves remote memory recall and promotes fear generalization. Thus, bidirectional manipulation of myelin plasticity functionally affects behavior and neurophysiology, which suggests that neural activity during fear learning instructs the formation of new myelin, which in turn supports the consolidation and/or retrieval of remote fear memories.},
	language = {eng},
	number = {4},
	journal = {Nature Neuroscience},
	author = {Pan, Simon and Mayoral, Sonia R. and Choi, Hye Sun and Chan, Jonah R. and Kheirbek, Mazen A.},
	month = apr,
	year = {2020},
	pmid = {32042175},
	pmcid = {PMC7213814},
	keywords = {Animals, Cell Proliferation, Conditioning, Classical, Fear, Memory, Long-Term, Mice, Mice, Transgenic, Myelin Sheath, Oligodendrocyte Precursor Cells, Oligodendrocyte Transcription Factor 2, Prefrontal Cortex, ⛔ No INSPIRE recid found},
	pages = {487--499},
}

@article{wan_impaired_2020,
	title = {Impaired {Postnatal} {Myelination} in a {Conditional} {Knockout} {Mouse} for the {Ferritin} {Heavy} {Chain} in {Oligodendroglial} {Cells}},
	volume = {40},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7531557/},
	doi = {10.1523/JNEUROSCI.1281-20.2020},
	abstract = {To define the importance of iron storage in oligodendrocyte development and function, the ferritin heavy subunit (Fth) was specifically deleted in oligodendroglial cells. Blocking Fth synthesis in Sox10 or NG2-positive oligodendrocytes during the first or the third postnatal week significantly reduces oligodendrocyte iron storage and maturation. The brain of Fth KO animals presented an important decrease in the expression of myelin proteins and a substantial reduction in the percentage of myelinated axons. This hypomyelination was accompanied by a decline in the number of myelinating oligodendrocytes and with a reduction in proliferating oligodendrocyte progenitor cells (OPCs). Importantly, deleting Fth in Sox10-positive oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte quantities. We also tested the capacity of Fth-deficient OPCs to remyelinate the adult brain in the cuprizone model of myelin injury and repair. Fth deletion in NG2-positive OPCs significantly reduces the number of mature oligodendrocytes and myelin production throughout the remyelination process. Furthermore, the corpus callosum of Fth KO animals presented a significant decrease in the percentage of remyelinated axons and a substantial reduction in the average myelin thickness. These results indicate that Fth synthesis during the first three postnatal weeks is important for an appropriate oligodendrocyte development, and suggest that Fth iron storage in adult OPCs is also essential for an effective remyelination of the mouse brain., SIGNIFICANCE STATEMENT To define the importance of iron storage in oligodendrocyte function, we have deleted the ferritin heavy chain (Fth) specifically in the oligodendrocyte lineage. Fth ablation in oligodendroglial cells throughout early postnatal development significantly reduces oligodendrocyte maturation and myelination. In contrast, deletion of Fth in oligodendroglial cells after postnatal day 60 has no effect on myelin production and/or oligodendrocyte numbers. We have also tested the consequences of disrupting Fth iron storage in oligodendrocyte progenitor cells (OPCs) after demyelination. We have found that Fth deletion in NG2-positive OPCs significantly delays the remyelination process in the adult brain. Therefore, Fth iron storage is essential for early oligodendrocyte development as well as for OPC maturation in the demyelinated adult brain.},
	number = {40},
	urldate = {2022-11-14},
	journal = {The Journal of Neuroscience},
	author = {Wan, Rensheng and Cheli, Veronica T. and Santiago-González, Diara A. and Rosenblum, Shaina L. and Wan, Qiuchen and Paez, Pablo M.},
	month = sep,
	year = {2020},
	pmid = {32868463},
	pmcid = {PMC7531557},
	keywords = {⛔ No INSPIRE recid found},
	pages = {7609--7624},
}

@article{xue_demyelination_2021,
	title = {Demyelination of the {Optic} {Nerve}: {An} {Underlying} {Factor} in {Glaucoma}?},
	volume = {13},
	issn = {1663-4365},
	shorttitle = {Demyelination of the {Optic} {Nerve}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8593209/},
	doi = {10.3389/fnagi.2021.701322},
	abstract = {Neurodegenerative disorders are characterized by typical neuronal degeneration and axonal loss in the central nervous system (CNS). Demyelination occurs when myelin or oligodendrocytes experience damage. Pathological changes in demyelination contribute to neurodegenerative diseases and worsen clinical symptoms during disease progression. Glaucoma is a neurodegenerative disease characterized by progressive degeneration of retinal ganglion cells (RGCs) and the optic nerve. Since it is not yet well understood, we hypothesized that demyelination could play a significant role in glaucoma. Therefore, this study started with the morphological and functional manifestations of demyelination in the CNS. Then, we discussed the main mechanisms of demyelination in terms of oxidative stress, mitochondrial damage, and immuno-inflammatory responses. Finally, we summarized the existing research on the relationship between optic nerve demyelination and glaucoma, aiming to inspire effective treatment plans for glaucoma in the future.},
	urldate = {2022-11-13},
	journal = {Frontiers in Aging Neuroscience},
	author = {Xue, Jingfei and Zhu, Yingting and Liu, Zhe and Lin, Jicheng and Li, Yangjiani and Li, Yiqing and Zhuo, Yehong},
	month = nov,
	year = {2021},
	pmid = {34795572},
	pmcid = {PMC8593209},
	keywords = {⛔ No INSPIRE recid found},
	pages = {701322},
}

@article{nave_axonal_2006,
	title = {Axonal regulation of myelination by neuregulin 1},
	volume = {16},
	issn = {0959-4388},
	doi = {10.1016/j.conb.2006.08.008},
	abstract = {Neuregulins comprise a family of epidermal growth factor-like ligands that interact with ErbB receptor tyrosine kinases to control many aspects of neural development. One of the most dramatic effects of neuregulin-1 is on glial cell differentiation. The membrane-bound neuregulin-1 type III isoform is an axonal ligand for glial ErbB receptors that regulates the early Schwann cell lineage, including the generation of precursors. Recent studies have shown that the amount of neuregulin-1 type III expressed on axons also dictates the glial phenotype, with a threshold level triggering Schwann cell myelination. Remarkably, neuregulin-1 type III also regulates Schwann cell membrane growth to adjust myelin sheath thickness to match axon caliber precisely. Whether this signaling system operates in central nervous system myelination remains an open question of major importance for human demyelinating diseases.},
	language = {eng},
	number = {5},
	journal = {Current Opinion in Neurobiology},
	author = {Nave, Klaus-Armin and Salzer, James L.},
	month = oct,
	year = {2006},
	pmid = {16962312},
	keywords = {Animals, Axons, Cell Differentiation, Cell Lineage, Humans, Myelin Sheath, Neuregulin-1, Neuroglia, Protein Isoforms, Receptor, ErbB-2, Receptor, ErbB-3, Signal Transduction, Stem Cells, ⛔ No INSPIRE recid found},
	pages = {492--500},
}

@article{baraban_ca2_2018,
	title = {Ca2+ activity signatures of myelin sheath formation and growth in vivo},
	volume = {21},
	issn = {1097-6256},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5742537/},
	doi = {10.1038/s41593-017-0040-x},
	abstract = {During myelination, individual oligodendrocytes initially over-produce short myelin sheaths that are either retracted or stabilised. By live imaging oligodendrocyte Ca2+ activity in vivo, we find that high-amplitude long-duration Ca2+ transients in sheaths prefigure retractions, mediated by calpain. Following stabilisation, myelin sheaths grow along axons, and we find that higher frequency Ca2+ transient activity in sheaths precedes faster elongation. Our data implicate local Ca2+ signalling in regulating distinct stages of myelination.},
	number = {1},
	urldate = {2022-11-13},
	journal = {Nature neuroscience},
	author = {Baraban, Marion and Koudelka, Sigrid and Lyons, David A},
	month = jan,
	year = {2018},
	pmid = {29230058},
	pmcid = {PMC5742537},
	keywords = {⛔ No INSPIRE recid found},
	pages = {19--23},
}

@article{kuhn_oligodendrocytes_2019,
	title = {Oligodendrocytes in {Development}, {Myelin} {Generation} and {Beyond}},
	volume = {8},
	issn = {2073-4409},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6912544/},
	doi = {10.3390/cells8111424},
	abstract = {Oligodendrocytes are the myelinating cells of the central nervous system (CNS) that are generated from oligodendrocyte progenitor cells (OPC). OPC are distributed throughout the CNS and represent a pool of migratory and proliferative adult progenitor cells that can differentiate into oligodendrocytes. The central function of oligodendrocytes is to generate myelin, which is an extended membrane from the cell that wraps tightly around axons. Due to this energy consuming process and the associated high metabolic turnover oligodendrocytes are vulnerable to cytotoxic and excitotoxic factors. Oligodendrocyte pathology is therefore evident in a range of disorders including multiple sclerosis, schizophrenia and Alzheimer’s disease. Deceased oligodendrocytes can be replenished from the adult OPC pool and lost myelin can be regenerated during remyelination, which can prevent axonal degeneration and can restore function. Cell population studies have recently identified novel immunomodulatory functions of oligodendrocytes, the implications of which, e.g., for diseases with primary oligodendrocyte pathology, are not yet clear. Here, we review the journey of oligodendrocytes from the embryonic stage to their role in homeostasis and their fate in disease. We will also discuss the most common models used to study oligodendrocytes and describe newly discovered functions of oligodendrocytes.},
	number = {11},
	urldate = {2022-11-13},
	journal = {Cells},
	author = {Kuhn, Sarah and Gritti, Laura and Crooks, Daniel and Dombrowski, Yvonne},
	month = nov,
	year = {2019},
	pmid = {31726662},
	pmcid = {PMC6912544},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1424},
}

@article{cullen_periaxonal_2021,
	title = {Periaxonal and nodal plasticities modulate action potential conduction in the adult mouse brain},
	volume = {34},
	issn = {2211-1247},
	url = {https://www.cell.com/cell-reports/abstract/S2211-1247(20)31630-2},
	doi = {10.1016/j.celrep.2020.108641},
	language = {English},
	number = {3},
	urldate = {2022-11-13},
	journal = {Cell Reports},
	author = {Cullen, Carlie L. and Pepper, Renee E. and Clutterbuck, Mackenzie T. and Pitman, Kimberley A. and Oorschot, Viola and Auderset, Loic and Tang, Alexander D. and Ramm, Georg and Emery, Ben and Rodger, Jennifer and Jolivet, Renaud B. and Young, Kaylene M.},
	month = jan,
	year = {2021},
	pmid = {33472075},
	note = {Publisher: Elsevier},
	keywords = {action potential, computational modeling, conduction velocity, myelin, node of Ranvier, oligodendrocyte, periaxonal space, plasticity, spatial learning, transcranial magnetic stimulation, ⛔ No INSPIRE recid found},
}

@article{gibson_neuronal_2014,
	title = {Neuronal {Activity} {Promotes} {Oligodendrogenesis} and {Adaptive} {Myelination} in the {Mammalian} {Brain}},
	volume = {344},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4096908/},
	doi = {10.1126/science.1252304},
	abstract = {Myelination of the central nervous system requires the generation of functionally mature oligodendrocytes from oligodendrocyte precursor cells (OPCs). Electrically active neurons may influence OPC function and selectively instruct myelination of an active neural circuit. In this work, we use optogenetic stimulation of the premotor cortex in awake, behaving mice to demonstrate that neuronal activity elicits a mitogenic response of neural progenitor cells and OPCs, promotes oligodendrogenesis, and increases myelination within the deep layers of the premotor cortex and subcortical white matter. We further show that this neuronal activity–regulated oligodendrogenesis and myelination is associated with improved motor function of the corresponding limb. Oligodendrogenesis and myelination appear necessary for the observed functional improvement, as epigenetic blockade of oligodendrocyte differentiation and myelin changes prevents the activity-regulated behavioral improvement.},
	number = {6183},
	urldate = {2022-11-13},
	journal = {Science (New York, N.Y.)},
	author = {Gibson, Erin M. and Purger, David and Mount, Christopher W. and Goldstein, Andrea K. and Lin, Grant L. and Wood, Lauren S. and Inema, Ingrid and Miller, Sarah E. and Bieri, Gregor and Zuchero, J. Bradley and Barres, Ben A. and Woo, Pamelyn J. and Vogel, Hannes and Monje, Michelle},
	month = may,
	year = {2014},
	pmid = {24727982},
	pmcid = {PMC4096908},
	keywords = {⛔ No INSPIRE recid found},
	pages = {1252304},
}

@article{spencer_compensation_2018,
	title = {Compensation for {Traveling} {Wave} {Delay} {Through} {Selection} of {Dendritic} {Delays} {Using} {Spike}-{Timing}-{Dependent} {Plasticity} in a {Model} of the {Auditory} {Brainstem}},
	volume = {12},
	issn = {1662-5188},
	doi = {10.3389/fncom.2018.00036},
	abstract = {Asynchrony among synaptic inputs may prevent a neuron from responding to behaviorally relevant sensory stimuli. For example, "octopus cells" are monaural neurons in the auditory brainstem of mammals that receive input from auditory nerve fibers (ANFs) representing a broad band of sound frequencies. Octopus cells are known to respond with finely timed action potentials at the onset of sounds despite the fact that due to the traveling wave delay in the cochlea, synaptic input from the auditory nerve is temporally diffuse. This paper provides a proof of principle that the octopus cells' dendritic delay may provide compensation for this input asynchrony, and that synaptic weights may be adjusted by a spike-timing dependent plasticity (STDP) learning rule. This paper used a leaky integrate and fire model of an octopus cell modified to include a "rate threshold," a property that is known to create the appropriate onset response in octopus cells. Repeated audio click stimuli were passed to a realistic auditory nerve model which provided the synaptic input to the octopus cell model. A genetic algorithm was used to find the parameters of the STDP learning rule that reproduced the microscopically observed synaptic connectivity. With these selected parameter values it was shown that the STDP learning rule was capable of adjusting the values of a large number of input synaptic weights, creating a configuration that compensated the traveling wave delay of the cochlea.},
	language = {eng},
	journal = {Frontiers in Computational Neuroscience},
	author = {Spencer, Martin J. and Meffin, Hamish and Burkitt, Anthony N. and Grayden, David B.},
	year = {2018},
	pmid = {29922141},
	pmcid = {PMC5996126},
	keywords = {auditory brainstem, cochlear nucleus, dendritic delay, octopus cells, spike-timing dependent plasticity, ⛔ No INSPIRE recid found},
	pages = {36},
}

@book{mead_analog_1989,
	title = {Analog {VLSI} {Implementation} of {Neural} {Systems}},
	isbn = {978-0-7923-9040-4},
	abstract = {This volume contains the proceedings of a workshop on Analog Integrated Neural Systems held May 8, 1989, in connection with the International Symposium on Circuits and Systems. The presentations were chosen to encompass the entire range of topics currently under study in this exciting new discipline. Stringent acceptance requirements were placed on contributions: (1) each description was required to include detailed characterization of a working chip, and (2) each design was not to have been published previously. In several cases, the status of the project was not known until a few weeks before the meeting date. As a result, some of the most recent innovative work in the field was presented. Because this discipline is evolving rapidly, each project is very much a work in progress. Authors were asked to devote considerable attention to the shortcomings of their designs, as well as to the notable successes they achieved. In this way, other workers can now avoid stumbling into the same traps, and evolution can proceed more rapidly (and less painfully). The chapters in this volume are presented in the same order as the corresponding presentations at the workshop. The first two chapters are concerned with fmding solutions to complex optimization problems under a predefmed set of constraints. The first chapter reports what is, to the best of our knowledge, the first neural-chip design. In each case, the physics of the underlying electronic medium is used to represent a cost function in a natural way, using only nearest-neighbor connectivity.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Mead, Carver and Ismail, Mohammed},
	month = aug,
	year = {1989},
	note = {Google-Books-ID: 9e29dOiXeiMC},
	keywords = {Computers / CAD-CAM, Computers / Computer Vision \& Pattern Recognition, Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / Circuits / General, Technology \& Engineering / Electronics / General, Technology \& Engineering / Imaging Systems, ⛔ No INSPIRE recid found},
}

@article{benjamin_neurogrid_2014,
	title = {Neurogrid: {A} {Mixed}-{Analog}-{Digital} {Multichip} {System} for {Large}-{Scale} {Neural} {Simulations}},
	volume = {102},
	issn = {1558-2256},
	shorttitle = {Neurogrid},
	doi = {10.1109/JPROC.2014.2313565},
	abstract = {In this paper, we describe the design of Neurogrid, a neuromorphic system for simulating large-scale neural models in real time. Neuromorphic systems realize the function of biological neural systems by emulating their structure. Designers of such systems face three major design choices: 1) whether to emulate the four neural elements-axonal arbor, synapse, dendritic tree, and soma-with dedicated or shared electronic circuits; 2) whether to implement these electronic circuits in an analog or digital manner; and 3) whether to interconnect arrays of these silicon neurons with a mesh or a tree network. The choices we made were: 1) we emulated all neural elements except the soma with shared electronic circuits; this choice maximized the number of synaptic connections; 2) we realized all electronic circuits except those for axonal arbors in an analog manner; this choice maximized energy efficiency; and 3) we interconnected neural arrays in a tree network; this choice maximized throughput. These three choices made it possible to simulate a million neurons with billions of synaptic connections in real time-for the first time-using 16 Neurocores integrated on a board that consumes three watts.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R. and Bussat, Jean-Marie and Alvarez-Icaza, Rodrigo and Arthur, John V. and Merolla, Paul A. and Boahen, Kwabena},
	month = may,
	year = {2014},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Analog circuits, Computer architecture, Electronic circuits, Integrated circuit modeling, Nerve fibers, Neural networks, Neuroscience, Random access memory, Synchronous digital hierarchy, application specific integrated circuits, asynchronous circuits, brain modeling, computational neuroscience, interconnection networks, mixed analog-digital integrated circuits, neural network hardware, neuromorphic electronic systems, ⛔ No INSPIRE recid found},
	pages = {699--716},
}

@article{duncan_neuron-oligodendrocyte_2021,
	title = {Neuron-{Oligodendrocyte} {Interactions} in the {Structure} and {Integrity} of {Axons}},
	volume = {9},
	issn = {2296-634X},
	url = {https://www.frontiersin.org/articles/10.3389/fcell.2021.653101},
	doi = {10.3389/fcell.2021.653101},
	abstract = {The myelination of axons by oligodendrocytes is a highly complex cell-to-cell interaction. Oligodendrocytes and axons have a reciprocal signaling relationship in which oligodendrocytes receive cues from axons that direct their myelination, and oligodendrocytes subsequently shape axonal structure and conduction. Oligodendrocytes are necessary for the maturation of excitatory domains on the axon including nodes of Ranvier, help buffer potassium, and support neuronal energy metabolism. Disruption of the oligodendrocyte-axon unit in traumatic injuries, Alzheimer’s disease and demyelinating diseases such as multiple sclerosis results in axonal dysfunction and can culminate in neurodegeneration. In this review, we discuss the mechanisms by which demyelination and loss of oligodendrocytes compromise axons. We highlight the intra-axonal cascades initiated by demyelination that can result in irreversible axonal damage. Both the restoration of oligodendrocyte myelination or neuroprotective therapies targeting these intra-axonal cascades are likely to have therapeutic potential in disorders in which oligodendrocyte support of axons is disrupted.},
	urldate = {2022-11-13},
	journal = {Frontiers in Cell and Developmental Biology},
	author = {Duncan, Greg J. and Simkins, Tyrell J. and Emery, Ben},
	year = {2021},
	keywords = {⛔ No INSPIRE recid found},
}

@article{stimberg_brian_2019,
	title = {Brian 2, an intuitive and efficient neural simulator},
	volume = {8},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/47314},
	doi = {10.7554/eLife.47314},
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	language = {en},
	urldate = {2022-11-14},
	journal = {eLife},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
	month = aug,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {e47314},
}

@article{hazan_bindsnet_2018,
	title = {{BindsNET}: {A} {Machine} {Learning}-{Oriented} {Spiking} {Neural} {Networks} {Library} in {Python}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {{BindsNET}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2018.00089},
	doi = {10.3389/fninf.2018.00089},
	abstract = {The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared toward machine learning and reinforcement learning. Our software, called BindsNET1, enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on the PyTorch deep neural networks library, facilitating the implementation of spiking neural networks on fast CPU and GPU computational platforms. Moreover, the BindsNET framework can be adjusted to utilize other existing computing and hardware backends; e.g., TensorFlow and SpiNNaker. We provide an interface with the OpenAI gym library, allowing for training and evaluation of spiking networks on reinforcement learning environments. We argue that this package facilitates the use of spiking networks for large-scale machine learning problems and show some simple examples by using BindsNET in practice.},
	urldate = {2022-11-14},
	journal = {Frontiers in Neuroinformatics},
	author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
	year = {2018},
	keywords = {⛔ No INSPIRE recid found},
}

@inproceedings{farquhar_field_2006,
	address = {Island of Kos, Greece},
	title = {A {Field} {Programmable} {Neural} {Array}},
	isbn = {978-0-7803-9389-9},
	url = {http://ieeexplore.ieee.org/document/1693534/},
	doi = {10.1109/ISCAS.2006.1693534},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2006 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	publisher = {IEEE},
	author = {Farquhar, E. and Gordon, C. and Hasler, P.},
	year = {2006},
	keywords = {⛔ No INSPIRE recid found},
	pages = {4114--4117},
}

@article{markram_introducing_2011,
	series = {Proceedings of the 2nd {European} {Future} {Technologies} {Conference} and {Exhibition} 2011 ({FET} 11)},
	title = {Introducing the {Human} {Brain} {Project}},
	volume = {7},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050911006806},
	doi = {10.1016/j.procs.2011.12.015},
	abstract = {The Human Brain Project (HBP) is a candidate project in the European Union's FET Flagship Program, funded by the ICT Program in the Seventh Framework Program. The project will develop a new integrated strategy for understanding the human brain and a novel research platform that will integrate all the data and knowledge we can acquire about the structure and function of the brain and use it to build unifying models that can be validated by simulations running on supercomputers. The project will drive the development of supercomputing for the life sciences, generate new neuroscientific data as a benchmark for modeling, develop radically new tools for informatics, modeling and simulation, and build virtual laboratories for collaborative basic and clinical studies, drug simulation and virtual prototyping of neuroprosthetic, neuromorphic, and robotic devices.},
	language = {en},
	urldate = {2022-11-14},
	journal = {Procedia Computer Science},
	author = {Markram, Henry and Meier, Karlheinz and Lippert, Thomas and Grillner, Sten and Frackowiak, Richard and Dehaene, Stanislas and Knoll, Alois and Sompolinsky, Haim and Verstreken, Kris and DeFelipe, Javier and Grant, Seth and Changeux, Jean-Pierre and Saria, Alois},
	month = jan,
	year = {2011},
	keywords = {HPC, Human brain, medicine, modeling, neuroinformatics, neuromorphics, neuroprosthetics, neurorobotics, neuroscience, simulation, supercomputing, ⛔ No INSPIRE recid found},
	pages = {39--42},
}

@inproceedings{schemmel_wafer-scale_2010,
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	doi = {10.1109/ISCAS.2010.5536970},
	abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project "FACETS" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a unified neural system description language, called PyNN, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within FACETS is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Schemmel, Johannes and Brüderle, Daniel and Grübl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	month = may,
	year = {2010},
	note = {ISSN: 2158-1525},
	keywords = {Biological neural networks, Biological system modeling, Biological tissues, Biology computing, Hardware, Large-scale systems, Neuromorphics, Numerical simulation, Semiconductor device modeling, Usability, ⛔ No INSPIRE recid found},
	pages = {1947--1950},
}

@inproceedings{diehl_efficient_2014,
	address = {Beijing, China},
	title = {Efficient implementation of {STDP} rules on {SpiNNaker} neuromorphic hardware},
	isbn = {978-1-4799-1484-5 978-1-4799-6627-1},
	url = {https://ieeexplore.ieee.org/document/6889876},
	doi = {10.1109/IJCNN.2014.6889876},
	abstract = {Recent development of neuromorphic hardware offers great potential to speed up simulations of neural networks. SpiNNaker is a neuromorphic hardware and software system designed to be scalable and ﬂexible enough to implement a variety of different types of simulations of neural systems, including spiking simulations with plasticity and learning. Spiketiming dependent plasticity (STDP) rules are the most common form of learning used in spiking networks. However, to date very few such rules have been implemented on SpiNNaker, in part because implementations must be designed to ﬁt the specialized nature of the hardware. Here we explain how general STDP rules can be efﬁciently implemented in the SpiNNaker system. We give two examples of applications of the implemented rule: learning of a temporal sequence, and balancing inhibition and excitation of a neural network. Comparing the results from the SpiNNaker system to a conventional double-precision simulation, we ﬁnd that the network behavior is comparable, and the ﬁnal weights differ by less than 3\% between the two simulations, while the SpiNNaker simulation runs much faster, since it runs in real time, independent of network size.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Diehl, Peter U. and Cook, Matthew},
	month = jul,
	year = {2014},
	keywords = {⛔ No INSPIRE recid found},
	pages = {4288--4295},
}

@article{neckar_braindrop_2019,
	title = {Braindrop: {A} mixed-signal neuromorphic architecture with a dynamical systems-based programming model},
	volume = {107},
	doi = {10.1109/JPROC.2018.2881432},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Neckar, Alexander and Fok, Sam and Benjamin, Ben V. and Stewart, Terrence C. and Oza, Nick N. and Voelker, Aaron R. and Eliasmith, Chris and Manohar, Rajit and Boahen, Kwabena},
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {144--164},
}

@inproceedings{wang_fpga_2014,
	title = {An {FPGA} design framework for large-scale spiking neural networks},
	doi = {10.1109/ISCAS.2014.6865169},
	abstract = {We present an FPGA design framework for large-scale spiking neural networks, particularly the ones with a high-density of connections or all-to-all connections. The proposed FPGA design framework is based on a reconfigurable neural layer, which is implemented using a time-multiplexing approach to achieve up to 200,000 virtual neurons with one physical neuron using only a fraction of the hardware resources in commercial-off-the-shelf FPGAs (even entry level ones). Rather than using a mathematical computational model, the physical neuron was efficiently implemented with a conductance-based model, of which the parameters were randomised between neurons to emulate the variance in biological neurons. Besides these building blocks, the proposed time-multiplexed reconfigurable neural layer has an address buffer, which will generate a fixed random weight for each connection on the fly for incoming spikes. This structure effectively reduces the usage of memory. After presenting the architecture of the proposed neural layer, we present a network with 23 proposed neural layers, each containing 64k neurons, yielding 1.5 M neurons and 92 G synapses with a total spike throughput of 1.2T spikes/s, while running in real-time on a Virtex 6 FPGA.},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Wang, Runchun and Hamilton, Tara Julia and Tapson, Jonathan and van Schaik, André},
	month = jun,
	year = {2014},
	note = {ISSN: 2158-1525},
	keywords = {Arrays, Biological neural networks, Biological system modeling, Digital signal processing, Field programmable gate arrays, Generators, Neurons, ⛔ No INSPIRE recid found},
	pages = {457--460},
}

@incollection{cheng_fpaa_2009,
	address = {Berlin, Heidelberg},
	title = {{FPAA} {Based} on {Integration} of {CMOS} and {Nanojunction} {Devices} for {Neuromorphic} {Applications}},
	volume = {3},
	isbn = {978-3-642-02426-9 978-3-642-02427-6},
	url = {http://link.springer.com/10.1007/978-3-642-02427-6_9},
	abstract = {In this paper, a novel field programmable analog arrays (FPAA) architecture, namely, NueroFPAA, is introduced to utilize nanodevices to build a programmable neuromorphic system. By using nanodevices as programmable components, the proposed FPAA can achieve high-density and low-power operations for neuromorphic applications. The routing and function blocks of the FPAA are specifically designed so that this proposed architecture can support large-scale neuromorphic design as well as various analog circuitries.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {Nano-{Net}},
	publisher = {Springer Berlin Heidelberg},
	author = {Liu, Ming and Yu, Hua and Wang, Wei},
	editor = {Cheng, Maggie},
	year = {2009},
	keywords = {⛔ No INSPIRE recid found},
	pages = {44--48},
}

@article{diesmann_nest_2003,
	title = {{NEST}: {An} {Environment} for {Neural} {Systems} {Simulations}},
	abstract = {NEST is a framework for simulating large, structured neuronal systems. It is designed to investigate the functional behavior of neuronal systems in the context of their anatomical, morphological, and electrophysiological properties. NEST aims at large networks, while maintaining an appropriate degree of biological detail. This is achieved by combining a broad range of abstraction levels in a single network simulation. Great biological detail is then maintained only at the points of interest, while the rest of the system can be modeled by more abstract components. Here, we describe the conception of NEST and illustrate its key features. We demonstrate that software design and organizational aspects were of equal importance for the success of the project.},
	language = {en},
	journal = {GWDG-Bericht Nr. 58 Theo Plesser, Volker Macho (Hrsg.)},
	author = {Diesmann, Markus and Gewaltig, Marc-Oliver},
	year = {2003},
	keywords = {⛔ No INSPIRE recid found},
	pages = {29},
}

@article{camon_timing_2019,
	title = {The {Timing} of {Sensory}-{Guided} {Behavioral} {Response} is {Represented} in the {Mouse} {Primary} {Somatosensory} {Cortex}},
	volume = {29},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/bhy169},
	doi = {10.1093/cercor/bhy169},
	abstract = {Whisker-guided decision making in mice is thought to critically depend on
information processing occurring in the primary somatosensory cortex. However,
it is not clear if neuronal activity in this “early”
sensory region contains information about the timing and speed of motor
response. To address this question we designed a new task in which freely moving
mice learned to associate a whisker stimulus to reward delivery. The task was
tailored in such a way that a wide range of delays between whisker stimulation
and reward collection were observed due to differences of motivation and
perception. After training, mice were anesthetized and neuronal responses evoked
by stimulating trained and untrained whiskers were recorded across several
cortical columns of barrel cortex. We found a strong correlation between the
delay of the mouse behavioral response and the timing of multiunit activity
evoked by the trained whisker, outside its principal cortical column, in layers
4 and 5A but not in layer 2/3. Circuit mapping ex vivo revealed this effect was
associated with a weakening of layer 4 to layer 2/3 projection. We conclude that
the processes controlling the propagation of key sensory inputs to naive
cortical columns and the timing of sensory-guided action are linked.},
	number = {7},
	urldate = {2022-11-16},
	journal = {Cerebral Cortex},
	author = {Camon, Jérémy and Hugues, Sandrine and Erlandson, Melissa A and Robbe, David and Lagoun, Sabria and Marouane, Emna and Bureau, Ingrid},
	month = jul,
	year = {2019},
	keywords = {⛔ No INSPIRE recid found},
	pages = {3034--3047},
}

@article{boerlin_spike-based_2011,
	title = {Spike-{Based} {Population} {Coding} and {Working} {Memory}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	doi = {10.1371/journal.pcbi.1001080},
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	language = {en},
	number = {2},
	urldate = {2022-11-14},
	journal = {PLOS Computational Biology},
	author = {Boerlin, Martin and Denève, Sophie},
	month = feb,
	year = {2011},
	keywords = {Action potentials, Memory, Neural networks, Neuronal tuning, Neurons, Sensory cues, Sensory perception, Working memory, ⛔ No INSPIRE recid found},
	pages = {e1001080},
}

@article{young_functioning_1938,
	title = {The {Functioning} of the {Giant} {Nerve} {Fibres} of the {Squid}},
	volume = {15},
	issn = {0022-0949},
	url = {https://doi.org/10.1242/jeb.15.2.170},
	doi = {10.1242/jeb.15.2.170},
	abstract = {1. Stimulation of single giant nerve fibres in the stellar nerves of the squid (Loligo pealii) shows them to be motor axons which produce contraction of the circular fibres of the mantle muscles.2. When a stellar nerve is stimulated with condenser discharges a maximal response is obtained at threshold voltage. No increase of response is obtained by further increase in the strength of stimulation except for an occasional slight increase at about ten times threshold voltage probably due to repetitive firing. It therefore appears that the stimulus produces a single impulse in the giant fibre, and that this is capable of exciting contraction in all the muscle fibres which it reaches. This confirms the conclusion reached on histological grounds that in spite of their syncytial nature each of the giant nerve fibres is a single functional unit.3. Since there are about ten giant fibres on each side the mantle is divided into 20 neuromotor units, each nerve fibre innervating an enormous number of muscle fibres. The existence of these units can also very readily be demonstrated by the fact that threshold electrical stimulation at any point within the territory innervated by each single giant fibre sets up a contraction of the muscle fibres of all parts of the territory with which the stimulated area is in connexion through the nerve.4. Stimulation of the smaller fibres in a stellar nerve after destruction of the giant fibre also causes contraction of the circular muscles of the mantle. The amount of this contraction increases progressively with increased voltage, presumably on account of the stimulation of more and more nerve fibres. The maximum tension developed in this way is always very much less than that produced by stimulation of the giant fibres.5. The mantle is therefore provided with a double mechanism of expiratory contraction, maximal contractions being produced by single impulses in the giant fibres and graded contractions by those in the smaller fibres of the nerve. Presumably the former contractions are those involved in rapid movement, the latter in respiration.6. There are also radial muscles, running through the thickness of the mantle, whose contractions effect the inspiration by making the cavity larger.},
	number = {2},
	urldate = {2022-11-13},
	journal = {Journal of Experimental Biology},
	author = {Young, J. Z.},
	month = apr,
	year = {1938},
	keywords = {⛔ No INSPIRE recid found},
	pages = {170--185},
}

@article{linden_movement_2022,
	title = {Movement is governed by rotational neural dynamics in spinal motor networks},
	volume = {610},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05293-w},
	doi = {10.1038/s41586-022-05293-w},
	abstract = {Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. As flexor and extensor muscle activities alternate during rhythmic movements such as walking, it is often assumed that the responsible neural circuitry is similarly exhibiting alternating activity1. Here we present ensemble recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional ‘rotation’ in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behaviour. The radius of rotation correlates with the intended muscle force, and a perturbation of the low-dimensional trajectory can modify the motor behaviour. As existing models of spinal motor control do not offer an adequate explanation of rotation1,2, we propose a theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control and multifunctionalism, are readily explained.},
	language = {en},
	number = {7932},
	urldate = {2022-11-16},
	journal = {Nature},
	author = {Lindén, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	month = oct,
	year = {2022},
	keywords = {Central pattern generators, Network models, ⛔ No INSPIRE recid found},
	pages = {526--531},
}

@article{linden_movement_2021,
	title = {Movement is governed by rotational population dynamics in spinal motor networks},
	url = {https://doi.org/gqg6rb},
	abstract = {{\textless}jats:title{\textgreater}ABSTRACT{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}Although the generation of movements is a fundamental function of the nervous system, the underlying neural principles remain unclear. Since flexor- and extensor-muscles alternate during rhythmic movements like walking, it is often assumed that the responsible neural circuitry is similarly displaying alternating activity. Here, we present ensemble-recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional “rotation” in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic behavior. The radius of rotation correlates with the intended muscle force and a perturbation of the low-dimensional trajectory can modify the motor behavior. Since existing models of spinal motor control offer an inadequate explanation of rotation, we propose a new theory of neural generation of movements from which this and other unresolved issues, such as speed regulation, force control, and multi-functionalism, are readily explained.{\textless}/jats:p{\textgreater}},
	author = {Lindén, Henrik and Petersen, Peter C. and Vestergaard, Mikkel and Berg, Rune W.},
	month = sep,
	year = {2021},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {08936080},
	shorttitle = {Networks of spiking neurons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. © 1997 Elsevier Science Ltd. All rights reserved.},
	language = {en-US},
	number = {9},
	urldate = {2020-07-06},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	year = {1997},
	pages = {1659--1671},
}

@article{renner_sparse_2022,
	title = {Sparse {Vector} {Binding} on {Spiking} {Neuromorphic} {Hardware} {Using} {Synaptic} {Delays}},
	copyright = {info:eu-repo/semantics/closedAccess},
	url = {https://www.zora.uzh.ch/id/eprint/219676/},
	doi = {10.1145/3546790.3546820},
	abstract = {Vector Symbolic Architectures (VSA) were first proposed as connectionist models for symbolic reasoning, leveraging parallel and in-memory computing in brains and neuromorphic hardware that enable low-power, low-latency applications.
Symbols are defined in VSAs as points/vectors in a high-dimensional neural state-space.
For spiking neuromorphic hardware (and brains), particularly sparse representations are of interest, as they minimize the number of costly spikes. Furthermore, sparse representations can be efficiently stored in simple Hebbian auto-associative memories, which provide error correction in VSAs. 
However, the binding of spatially sparse representations is computationally expensive because it is not local to corresponding pairs of neurons as in VSAs with dense vectors.
Here, we present the first implementation of a sparse VSA on spiking neuromorphic hardware, specifically Intel's neuromorphic research chip Loihi.
To reduce the cost of binding, a delay line and coincidence detection are used, trading off space with time.
We show as proof of principle that our network on Loihi can perform the binding operation of a classical analogical reasoning task and discuss the cost of different sparse binding operations.
The proposed binding mechanism can be used as a building block for VSA-based architectures on neuromorphic hardware.},
	language = {eng},
	urldate = {2022-11-10},
	journal = {Proceedings of the International Conference on Neuromorphic Systems},
	author = {Renner, Alpha and Sandamirskaya, Yulia and Sommer, Friedrich T. and Frady, E. Paxon},
	month = jul,
	year = {2022},
	note = {Conference Name: ICONS 2022: International Conference on Neuromorphic Systems 2022
Meeting Name: ICONS 2022: International Conference on Neuromorphic Systems 2022
Place: Knoxville, TN, USA
Publisher: ACM Digital library},
	keywords = {Binding, Coincidence detection, Hyperdimensional Computing, Neuromorphic Hardware, Sparse distributed code, Spiking Neural Networks, Vector Symbolic Architecture (VSA)},
}

@article{schuman_survey_2017,
	title = {A {Survey} of {Neuromorphic} {Computing} and {Neural} {Networks} in {Hardware}},
	url = {http://arxiv.org/abs/1705.06963},
	abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
	urldate = {2021-03-25},
	journal = {arXiv:1705.06963 [cs]},
	author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06963},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{carandini_normalization_2012,
	title = {Normalization as a canonical neural computation},
	volume = {13},
	doi = {10.1038/nrn3136},
	number = {1},
	journal = {Nature Reviews Neuroscience},
	author = {Carandini, Matteo and Heeger, David J},
	year = {2012},
	keywords = {Adaptation, Afferent Pathways, Anim, Physiological, contrast\_response, divisive\_normalization, normalization},
	pages = {51--62},
}

@article{pasturel_humans_2020,
	title = {Humans adapt their anticipatory eye movements to the volatility of visual motion properties},
	copyright = {All rights reserved},
	url = {https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020},
	doi = {10.1371/journal.pcbi.1007438},
	abstract = {Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate "how much they are confident that the target will move to the right or left in the next trial" and to adjust the cursor's position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile).},
	journal = {PLoS Computational Biology},
	author = {Pasturel, Chloé and Montagnini, Anna and Perrinet, Laurent U},
	month = jan,
	year = {2020},
	keywords = {motion anticipation},
}

@article{hubel_receptive_1968,
	title = {Receptive fields and functional architecture of monkey striate cortex},
	volume = {195},
	issn = {1469-7793},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1968.sp008455},
	doi = {10.1113/jphysiol.1968.sp008455},
	language = {english},
	number = {1},
	journal = {The Journal of Physiology},
	author = {Hubel, David H and Wiesel, Torsten N},
	year = {1968},
	keywords = {area-v1, bicv-motion, bicv-sparse},
	pages = {215--243},
}

@article{dard_rapid_2022,
	title = {The rapid developmental rise of somatic inhibition disengages hippocampal dynamics from self-motion},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.78116},
	doi = {10.7554/eLife.78116},
	abstract = {Early electrophysiological brain oscillations recorded in preterm babies and newborn rodents are initially mostly driven by bottom-up sensorimotor activity and only later can detach from external inputs. This is a hallmark of most developing brain areas, including the hippocampus, which, in the adult brain, functions in integrating external inputs onto internal dynamics. Such developmental disengagement from external inputs is likely a fundamental step for the proper development of cognitive internal models. Despite its importance, the developmental timeline and circuit basis for this disengagement remain unknown. To address this issue, we have investigated the daily evolution of CA1 dynamics and underlying circuits during the first two postnatal weeks of mouse development using two-photon calcium imaging in non-anesthetized pups. We show that the first postnatal week ends with an abrupt shift in the representation of self-motion in CA1. Indeed, most CA1 pyramidal cells switch from activated to inhibited by self-generated movements at the end of the first postnatal week, whereas the majority of GABAergic neurons remain positively modulated throughout this period. This rapid switch occurs within 2 days and follows the rapid anatomical and functional surge of local somatic GABAergic innervation. The observed change in dynamics is consistent with a two-population model undergoing a strengthening of inhibition. We propose that this abrupt developmental transition inaugurates the emergence of internal hippocampal dynamics.},
	urldate = {2022-10-05},
	journal = {eLife},
	author = {Dard, Robin F and Leprince, Erwan and Denis, Julien and Rao Balappa, Shrisha and Suchkov, Dmitrii and Boyce, Richard and Lopez, Catherine and Giorgi-Kurz, Marie and Szwagier, Tom and Dumont, Théo and Rouault, Hervé and Minlebaev, Marat and Baude, Agnès and Cossart, Rosa and Picardo, Michel A},
	editor = {Peyrache, Adrien and Colgin, Laura L and Butt, Simon JB},
	month = jul,
	year = {2022},
	pages = {e78116},
}

@article{ikegaya_synfire_2004,
	title = {Synfire {Chains} and {Cortical} {Songs}: {Temporal} {Modules} of {Cortical} {Activity}},
	volume = {304},
	shorttitle = {Synfire {Chains} and {Cortical} {Songs}},
	url = {http://www.science.org/doi/10.1126/science.1093173},
	doi = {10/djckcn},
	number = {5670},
	urldate = {2021-11-29},
	journal = {Science},
	author = {Ikegaya, Yuji and Aaron, Gloster and Cossart, Rosa and Aronov, Dmitriy and Lampl, Ilan and Ferster, David and Yuste, Rafael},
	month = apr,
	year = {2004},
	keywords = {polychronization},
	pages = {559--564},
}

@article{simons_oligodendrocytes_2016,
	title = {Oligodendrocytes: {Myelination} and {Axonal} {Support}},
	volume = {8},
	issn = {1943-0264},
	shorttitle = {Oligodendrocytes},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691794/},
	doi = {10.1101/cshperspect.a020479},
	abstract = {Myelinated nerve fibers have evolved to enable fast and efficient transduction of electrical signals in the nervous system. To act as an electric insulator, the myelin sheath is formed as a multilamellar membrane structure by the spiral wrapping and subsequent compaction of the oligodendroglial plasma membrane around central nervous system (CNS) axons. Current evidence indicates that the myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes are metabolically active and functionally connected to the subjacent axon via cytoplasmic-rich myelinic channels for movement of macromolecules to and from the internodal periaxonal space under the myelin sheath. This review summarizes our current understanding of how myelin is generated and also the role of oligodendrocytes in supporting the long-term integrity of myelinated axons., The myelin sheath is more than an inert insulating membrane structure. Oligodendrocytes in the sheath are connected to the subjacent axon via cytoplasmic-rich myelinic channels, and they actively support the integrity of the neuron.},
	number = {1},
	urldate = {2022-11-10},
	journal = {Cold Spring Harbor Perspectives in Biology},
	author = {Simons, Mikael and Nave, Klaus-Armin},
	month = jan,
	year = {2016},
	pmid = {26101081},
	pmcid = {PMC4691794},
	pages = {a020479},
}

@inproceedings{pfeil_neuromorphic_2013,
	title = {Neuromorphic learning towards nano second precision},
	doi = {10.1109/IJCNN.2013.6706828},
	abstract = {Temporal coding is one approach to representing information in spiking neural networks. An example of its application is the location of sounds by barn owls that requires especially precise temporal coding. Dependent upon the azimuthal angle, the arrival times of sound signals are shifted between both ears. In order to determine these interaural time differences, the phase difference of the signals is measured. We implemented this biologically inspired network on a neuromorphic hardware system and demonstrate spike-timing dependent plasticity on an analog, highly accelerated hardware substrate. Our neuromorphic implementation enables the resolution of time differences of less than 50 ns. On-chip Hebbian learning mechanisms select inputs from a pool of neurons which code for the same sound frequency. Hence, noise caused by different synaptic delays across these inputs is reduced. Furthermore, learning compensates for variations on neuronal and synaptic parameters caused by device mismatch intrinsic to the neuromorphic substrate.},
	booktitle = {The 2013 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Pfeil, Thomas and Scherzer, Anne-Christine and Schemmel, Johannes and Meier, Karlheinz},
	month = aug,
	year = {2013},
	note = {ISSN: 2161-4407},
	keywords = {Delays, Emulation, Hardware, Neuromorphics, Neurons, System-on-chip, Vectors},
	pages = {1--5},
}

@article{bartolozzi_synaptic_2007,
	title = {Synaptic {Dynamics} in {Analog} {VLSI}},
	volume = {19},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/19/10/2581-2603/7219},
	doi = {10.1162/neco.2007.19.10.2581},
	abstract = {Synapses are crucial elements for computation and information transfer in both real and artificial neural systems. Recent experimental findings and theoretical models of pulse-based neural networks suggest that synaptic dynamics can play a crucial role for learning neural codes and encoding spatio-temporal spike patterns. Within the context of hardware implementations of pulse based neural networks, several analog VLSI circuits modeling synaptic functionality have been proposed. We present an overview of previously proposed circuits and describe a novel analog VLSI synaptic circuit suitable for integration in large VLSI spike-based neural systems. The circuit proposed is based on a computational model that fits the real post-synaptic currents with exponentials. We present experimental data showing how the circuit exhibits realistic dynamics and show how it can be connected to additional modules for implementing a wide range of synaptic properties.},
	language = {en},
	number = {10},
	urldate = {2022-11-10},
	journal = {Neural Computation},
	author = {Bartolozzi, Chiara and Indiveri, Giacomo},
	month = oct,
	year = {2007},
	pages = {2581--2603},
}

@article{chan_aer_2007,
	title = {{AER} {EAR}: {A} {Matched} {Silicon} {Cochlea} {Pair} {With} {Address} {Event} {Representation} {Interface}},
	volume = {54},
	issn = {1558-0806},
	shorttitle = {{AER} {EAR}},
	doi = {10.1109/TCSI.2006.887979},
	abstract = {In this paper, we present an analog integrated circuit containing a matched pair of silicon cochleae and an address event interface. Each section of the cochlea, modeled by a second-order low-pass filter, is followed by a simplified inner hair cell circuit and a spiking neuron circuit. When the neuron spikes, an address event is generated on the asynchronous data bus. We present the results of the chip characterization and the results of an interaural time difference based sound localization experiment using the address event representation (AER) EAR. The chip was fabricated in a 3-metal 2-poly 0.5-mum CMOS process},
	number = {1},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Chan, Vincent and Liu, Shih-Chii and van Schaik, Andr},
	month = jan,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {Analog integrated circuits, Biological system modeling, Circuits, Ear, Low pass filters, Neuromorphics, Neurons, Protocols, Silicon, Timing, Transmitters, neuromorphic engineering, silicon cochlea, sound localization},
	pages = {48--59},
}

@inproceedings{tschechne_bio-inspired_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Bio-{Inspired} {Optic} {Flow} from {Event}-{Based} {Neuromorphic} {Sensor} {Input}},
	isbn = {978-3-319-11656-3},
	doi = {10.1007/978-3-319-11656-3_16},
	abstract = {Computational models of visual processing often use frame-based image acquisition techniques to process a temporally changing stimulus. This approach is unlike biological mechanisms that are spike-based and independent of individual frames. The neuromorphic Dynamic Vision Sensor (DVS) [Lichtsteiner et al., 2008] provides a stream of independent visual events that indicate local illumination changes, resembling spiking neurons at a retinal level. We introduce a new approach for the modelling of cortical mechanisms of motion detection along the dorsal pathway using this type of representation. Our model combines filters with spatio-temporal tunings also found in visual cortex to yield spatio-temporal and direction specificity. We probe our model with recordings of test stimuli, articulated motion and ego-motion. We show how our approach robustly estimates optic flow and also demonstrate how this output can be used for classification purposes.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Tschechne, Stephan and Sailer, Roman and Neumann, Heiko},
	editor = {El Gayar, Neamat and Schwenker, Friedhelm and Suen, Cheng},
	year = {2014},
	keywords = {Classification, Event-Vision, Neural Model, Optic Flow},
	pages = {171--182},
}

@article{kaiser_emulating_2022,
	series = {Dendritic contributions to biological and artificial computations},
	title = {Emulating {Dendritic} {Computing} {Paradigms} on {Analog} {Neuromorphic} {Hardware}},
	volume = {489},
	issn = {0306-4522},
	url = {https://www.sciencedirect.com/science/article/pii/S0306452221004218},
	doi = {10.1016/j.neuroscience.2021.08.013},
	abstract = {BrainScaleS-2 is an accelerated and highly configurable neuromorphic system with physical models of neurons and synapses. Beyond networks of spiking point neurons, it allows for the implementation of user-defined neuron morphologies. Both passive propagation of electric signals between compartments as well as dendritic spikes and plateau potentials can be emulated. In this paper, three multi-compartment neuron morphologies are chosen to demonstrate passive propagation of postsynaptic potentials, spatio-temporal coincidence detection of synaptic inputs in a dendritic branch, and the replication of the BAC burst firing mechanism found in layer 5 pyramidal neurons of the neocortex.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Neuroscience},
	author = {Kaiser, Jakob and Billaudelle, Sebastian and Müller, Eric and Tetzlaff, Christian and Schemmel, Johannes and Schmitt, Sebastian},
	month = may,
	year = {2022},
	keywords = {AdEx neuron model, accelerated technology, mixed-signal neuromorphic, multi-compartmental models, physical model},
	pages = {290--300},
}

@article{rajendran_low-power_2019,
	title = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}: {A} {Review} of {Architectural} and {System}-{Level} {Design} {Approaches}},
	volume = {36},
	issn = {1558-0792},
	shorttitle = {Low-{Power} {Neuromorphic} {Hardware} for {Signal} {Processing} {Applications}},
	doi = {10.1109/MSP.2019.2933719},
	abstract = {Machine learning has emerged as the dominant tool for implementing complex cognitive tasks that require supervised, unsupervised, and reinforcement learning. While the resulting machines have demonstrated in some cases even superhuman performance, their energy consumption has often proved to be prohibitive in the absence of costly supercomputers. Most state-of-the-art machine-learning solutions are based on memoryless models of neurons. This is unlike the neurons in the human brain that encode and process information using temporal information in spike events. The different computing principles underlying biological neurons and how they combine together to efficiently process information is believed to be a key factor behind their superior efficiency compared to current machine-learning systems.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Rajendran, Bipin and Sebastian, Abu and Schmuker, Michael and Srinivasa, Narayan and Eleftheriou, Evangelos},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Encoding, Hardware, Mathematical model, Neuromorphics, Neurons, Signal processing algorithms, Synapses},
	pages = {97--110},
}

@inproceedings{hill_spike-timing_2017,
	title = {A {Spike}-{Timing} {Neuromorphic} {Architecture}},
	doi = {10.1109/ICRC.2017.8123631},
	abstract = {Unlike general purpose computer architectures that are comprised of complex processor cores and sequential computation, the brain is innately parallel and contains highly complex connections between computational units (neurons). Key to the architecture of the brain is a functionality enabled by the combined effect of spiking communication and sparse connectivity with unique variable efficacies and temporal latencies. Utilizing these neuroscience principles, we have developed the Spiking Temporal Processing Unit (STPU) architecture which is well-suited for areas such as pattern recognition and natural language processing. In this paper, we formally describe the STPU, implement the STPU on a field programmable gate array, and show measured performance data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Rebooting} {Computing} ({ICRC})},
	author = {Hill, Aaron J. and Donaldson, Jonathon W. and Rothganger, Fredrick H. and Vineyard, Craig M. and Follett, David R. and Follett, Pamela L. and Smith, Michael R. and Verzi, Stephen J. and Severa, William and Wang, Felix and Aimone, James B. and Naegle, John H. and James, Conrad D.},
	month = nov,
	year = {2017},
	keywords = {Biological system modeling, Computer architecture, Delays, Neuromorphics, Neurons},
	pages = {1--8},
}

@article{schuman_opportunities_2022,
	title = {Opportunities for neuromorphic computing algorithms and applications},
	volume = {2},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {http://www.nature.com/articles/s43588-021-00184-y},
	doi = {10.1038/s43588-021-00184-y},
	abstract = {Neuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic computing has focused on hardware development. Here, we review recent results in neuromorphic computing algorithms and applications. We highlight characteristics of neuromorphic computing technologies that make them attractive for the future of computing and we discuss opportunities for future development of algorithms and applications on these systems.},
	language = {en},
	number = {1},
	urldate = {2022-11-10},
	journal = {Nature Computational Science},
	author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science},
	pages = {10--19},
}

@article{nieters_neuromorphic_2017,
	title = {Neuromorphic computation in multi-delay coupled models},
	volume = {61},
	issn = {0018-8646},
	doi = {10.1147/JRD.2017.2664698},
	abstract = {Neuromorphic computing provides a promising platform for processing high-dimensional noisy signals on dedicated hardware. Using design elements inspired by neurobiological findings and advances in machine learning methodology, delay-coupled systems have recently been developed in the field of neuromorphic computing. Delayed feedback connections enable such systems to generate a complex representation of injected input in the internal state of single nodes, which in our context refer to hardware components with nonlinear behavior and without any memory. In contrast to classical combinatorial circuits or feed-forward networks, this state is not distributed in space but in time. Hardware implementations with low hardware component counts are therefore particularly easy to design for delay-coupled systems. In this paper, we present an argument for using delay-coupled reservoirs using multiple feedback terms with different delays. We present a theoretical analysis of the resulting system, discuss surprising effects pertaining to the precise choice of delays, and provide a guideline for the optimal design of such systems.},
	number = {2/3},
	journal = {IBM Journal of Research and Development},
	author = {Nieters, P. and Leugering, J. and Pipa, G.},
	month = mar,
	year = {2017},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Biological neural networks, Computational modeling, Computer architecture, Learning systems, Neuromorphics, Noise measurement},
	pages = {8:7--8:9},
}

@article{liang_neuromorphic_2022,
	title = {A {Neuromorphic} {Model} {With} {Delay}-{Based} {Reservoir} for {Continuous} {Ventricular} {Heartbeat} {Detection}},
	volume = {69},
	issn = {1558-2531},
	doi = {10.1109/TBME.2021.3129306},
	abstract = {There is a growing interest in neuromorphic hardware since it offers a more intuitive way to achieve bio-inspired algorithms. This paper presents a neuromorphic model for intelligently processing continuous electrocardiogram (ECG) signal. This model aims to develop a hardware-based signal processing model and avoid employing digitally intensive operations, such as signal segmentation and feature extraction, which are not desired in an analogue neuromorphic system. We apply delay-based reservoir computing as the information processing core, along with a novel training and labelling method. Different from the conventional ECG classification techniques, this computation model is a end-to-end dynamic system that mimics the real-time signal flow in neuromorphic hardware. The input is the raw ECG stream, while the amplitude of the output represents the risk factor of a ventricular ectopic heartbeat. The intrinsic memristive property of the reservoir empowers the system to retain the historical ECG information for high-dimensional mapping. This model was evaluated with the MIT-BIH database under the inter-patient paradigm and yields 81\% sensitivity and 98\% accuracy. Under this architecture, the minimum size of memory required in the inference process can be as low as 3.1 MegaByte(MB) because the majority of the computation takes place in the analogue domain. Such computational modelling boosts memory efficiency by simplifying the computing procedure and minimizing the required memory for future wearable devices.},
	number = {6},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Liang, Xiangpeng and Li, Haobo and Vuckovic, Aleksandra and Mercer, John and Heidari, Hadi},
	month = jun,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	keywords = {Computational modeling, Continuous ventricular heartbeat detection, Databases, Electrocardiography, Heart beat, Memory efficient analogue computing, Neuromorphics, Neurons, Physical neural network, Reservoirs, delay-based reservoir computing},
	pages = {1837--1849},
}

@inproceedings{hussain_deltron_2012,
	title = {{DELTRON}: {Neuromorphic} architectures for delay based learning},
	shorttitle = {{DELTRON}},
	doi = {10.1109/APCCAS.2012.6419032},
	abstract = {We present a neuromorphic spiking neural network, the DELTRON, that can remember and store patterns by changing the delays of every connection as opposed to modifying the weights. The advantage of this architecture over traditional weight based ones is simpler hardware implementation without multipliers or digital-analog converters (DACs). The name is derived due to similarity in the learning rule with an earlier architecture called Tempotron. We present simulations of memory capacity of the DELTRON for different random spatio-temporal spike patterns and also present SPICE simulation results of the core circuits involved in a reconfigurable mixed signal implementation of this architecture.},
	booktitle = {2012 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems}},
	author = {Hussain, Shaista and Basu, Arindam and Wang, Mark and Hamilton, Tara Julia},
	month = dec,
	year = {2012},
	keywords = {Computer architecture, Delay, Nerve fibers, Neuromorphics, Registers, Training},
	pages = {304--307},
}

@article{sandamirskaya_neuromorphic_2022,
	title = {Neuromorphic computing hardware and neural architectures for robotics},
	volume = {7},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.abl8419},
	doi = {10.1126/scirobotics.abl8419},
	abstract = {Neuromorphic hardware enables fast and power-efficient neural network–based artificial intelligence that is well suited to solving robotic tasks. Neuromorphic algorithms can be further developed following neural computing principles and neural network architectures inspired by biological neural systems. In this Viewpoint, we provide an overview of recent insights from neuroscience that could enhance signal processing in artificial neural networks on chip and unlock innovative applications in robotics and autonomous intelligent systems. These insights uncover computing principles, primitives, and algorithms on different levels of abstraction and call for more research into the basis of neural computation and neuronally inspired computing hardware.},
	number = {67},
	urldate = {2022-11-10},
	journal = {Science Robotics},
	author = {Sandamirskaya, Yulia and Kaboli, Mohsen and Conradt, Jorg and Celikel, Tansu},
	month = jun,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabl8419},
}

@article{markovic_physics_2020,
	title = {Physics for neuromorphic computing},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5820},
	url = {http://www.nature.com/articles/s42254-020-0208-2},
	doi = {10.1038/s42254-020-0208-2},
	abstract = {Neuromorphic computing takes inspiration from the brain to create energy-efficient hardware for information processing, capable of highly sophisticated tasks. Systems built with standard electronics achieve gains in speed and energy by mimicking the distributed topology of the brain. Scaling-up such systems and improving their energy usage, speed and performance by several orders of magnitude requires a revolution in hardware. We discuss how including more physics in the algorithms and nanoscale materials used for data processing could have a major impact in the field of neuromorphic computing. We review striking results that leverage physics to enhance the computing capabilities of artificial neural networks, using resistive switching materials, photonics, spintronics and other technologies. We discuss the paths that could lead these approaches to maturity, towards low-power, miniaturized chips that could infer and learn in real time.},
	language = {en},
	number = {9},
	urldate = {2022-11-10},
	journal = {Nature Reviews Physics},
	author = {Marković, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
	month = sep,
	year = {2020},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Electronics, Nanoscale devices, photonics and device physics},
	pages = {499--510},
}

@inproceedings{kim_real-time_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {3D} {Reconstruction} and 6-{DoF} {Tracking} with an {Event} {Camera}},
	isbn = {978-3-319-46466-4},
	doi = {10.1007/978-3-319-46466-4_21},
	abstract = {We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Kim, Hanme and Leutenegger, Stefan and Davison, Andrew J.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {3D reconstruction, 6-DoF tracking, Event-based camera, Intensity reconstruction, SLAM, Visual odometry},
	pages = {349--364},
}

@inproceedings{stoffregen_event-based_2019,
	title = {Event-{Based} {Motion} {Segmentation} by {Motion} {Compensation}},
	doi = {10.1109/ICCV.2019.00734},
	abstract = {In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called "events"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10\%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90\% accuracy at 4 pixels relative displacement.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Stoffregen, Timo and Gallego, Guillermo and Drummond, Tom and Kleeman, Lindsay and Scaramuzza, Davide},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Cameras, Computer vision, Image segmentation, Motion compensation, Motion segmentation, Robot vision systems, Tracking},
	pages = {7243--7252},
}

@article{hidalgo-carrio_learning_2020,
	title = {Learning {Monocular} {Dense} {Depth} from {Events}},
	url = {http://arxiv.org/abs/2010.08350},
	abstract = {Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous events instead of intensity frames. Compared to conventional image sensors, they offer significant advantages: high temporal resolution, high dynamic range, no motion blur, and much lower bandwidth. Recently, learning-based approaches have been applied to event-based data, thus unlocking their potential and making significant progress in a variety of tasks, such as monocular depth prediction. Most existing approaches use standard feed-forward architectures to generate network predictions, which do not leverage the temporal consistency presents in the event stream. We propose a recurrent architecture to solve this task and show significant improvement over standard feed-forward methods. In particular, our method generates dense depth predictions using a monocular setup, which has not been shown previously. We pretrain our model using a new dataset containing events and depth maps recorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo Event Camera Dataset (MVSEC). Quantitative experiments show up to 50\% improvement in average depth error with respect to previous event-based methods.},
	urldate = {2021-01-22},
	journal = {arXiv:2010.08350 [cs]},
	author = {Hidalgo-Carrió, Javier and Gehrig, Daniel and Scaramuzza, Davide},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{clady_asynchronous_2014,
	title = {Asynchronous visual event-based time-to-contact},
	volume = {8},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2014.00009/full},
	doi = {10.3389/fnins.2014.00009},
	abstract = {A reliable and fast sensing of the environment is a fundamental necessity of mobile platforms. Unfortunately conventional cameras due to the current frame-based acquisition paradigm output low temporal dynamics and redundant data flow leading to high computational costs. It is obviously incompatible with the necessities of mobile platforms where energy consumption and computational load are a major issue. The restrictions of the frame-based paradigm are contradictory with applications requiring high speed sensor-based reactive control. This paper introduces a fast obstacle avoidance using the output of an asynchronous event-based time encoded imaging sensor. The approach is event-based in the sense that every incoming event adds to the computation process thus allowing fast avoidance responses. It introduces an event-based time-to-contact approach relying on the computation of visual event-based motion flows. Experiments on a mobile robot are presented in an indoor environment. Time to contact results are compared with those provided by a laser range finder showing that event-based sensing offers new perspectives for mobile robotics sensing.},
	language = {English},
	urldate = {2020-11-04},
	journal = {Frontiers in Neuroscience},
	author = {Clady, Xavier and Clercq, Charles and Ieng, Sio-Hoi and Houseini, Fouzhan and Randazzo, Marco and Natale, Lorenzo and Bartolozzi, Chiara and Benosman, Ryad Benjamin},
	year = {2014},
	note = {00000 
Publisher: Frontiers},
	keywords = {Computer Vision, Event-based Computation, Neuromorphic vision, Robotics, time-to-contact},
}

@article{perez-cerda_pio_2015,
	title = {Pío del {Río} {Hortega} and the discovery of the oligodendrocytes},
	volume = {9},
	issn = {1662-5129},
	doi = {10.3389/fnana.2015.00092},
	abstract = {Pío del Río Hortega (1882-1945) discovered microglia and oligodendrocytes (OLGs), and after Ramón y Cajal, was the most prominent figure of the Spanish school of neurology. He began his scientific career with Nicolás Achúcarro from whom he learned the use of metallic impregnation techniques suitable to study non-neuronal cells. Later on, he joined Cajal's laboratory. and Subsequently, he created his own group, where he continued to develop other innovative modifications of silver staining methods that revolutionized the study of glial cells a century ago. He was also interested in neuropathology and became a leading authority on Central Nervous System (CNS) tumors. In parallel to this clinical activity, del Río Hortega rendered the first systematic description of a major polymorphism present in a subtype of macroglial cells that he named as oligodendroglia and later OLGs. He established their ectodermal origin and suggested that they built the myelin sheath of CNS axons, just as Schwann cells did in the periphery. Notably, he also suggested the trophic role of OLGs for neuronal functionality, an idea that has been substantiated in the last few years. Del Río Hortega became internationally recognized and established an important neurohistological school with outstanding pupils from Spain and abroad, which nearly disappeared after his exile due to the Spanish civil war. Yet, the difficulty of metal impregnation methods and their variability in results, delayed for some decades the confirmation of his great insights into oligodendrocyte biology until the development of electron microscopy and immunohistochemistry. This review aims at summarizing the pioneer and essential contributions of del Río Hortega to the current knowledge of oligodendrocyte structure and function, and to provide a hint of the scientific personality of this extraordinary and insufficiently recognized man.},
	language = {eng},
	journal = {Frontiers in Neuroanatomy},
	author = {Pérez-Cerdá, Fernando and Sánchez-Gómez, María Victoria and Matute, Carlos},
	year = {2015},
	pmid = {26217196},
	pmcid = {PMC4493393},
	keywords = {Del Río Hortega, Ramón y Cajal, myelin sheath, oligodendrocyte precursor cell (OPC), oligodendroglia},
	pages = {92},
}

@article{stetson_effects_1992,
	title = {Effects of age, sex, and anthropometric factors on nerve conduction measures},
	volume = {15},
	issn = {1097-4598},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mus.880151007},
	doi = {10.1002/mus.880151007},
	abstract = {Associations among measures of median, ulnar, and sural nerve conduction and age, skin temperature, sex, and anthropometric factors were evaluated in a population of 105 healthy, asymptomatic adults without occupational exposure to highly repetitive or forceful hand exertions. Height was negatively associated with sensory amplitude in all nerves tested (P {\textless} 0.001), and positively associated with median and ulnar sensory distal latencies (P {\textless} 0.01) and sural latency (P {\textless} 0.001). Index finger circumference was negatively associated with median and ulnar sensory amplitudes (P {\textless} 0.05). Sex, in isolation from highly correlated anthropometric factors such as height, was not found to be a significant predictor of median or ulnar nerve conduction measures. Equations using age, height, and finger circumference for prediction of normal values are presented. Failure to adjust normal nerve conduction values for these factors decreases the diagnostic specificity and sensitivity of the described measures, and may result in misclassification of individuals. © 1992 John Wiley \& Sons, Inc.},
	language = {en},
	number = {10},
	urldate = {2022-11-10},
	journal = {Muscle \& Nerve},
	author = {Stetson, Diana S. and Albers, James W. and Silverstein, Barbara A. and Wolfe, Robert A.},
	year = {1992},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mus.880151007},
	keywords = {age, anthropometry, height, nerve conduction studies, normal values},
	pages = {1095--1104},
}

@article{von_helmholz_messungen_1850,
	title = {Messungen über den zeitlichen {Verlauf} der {Zuckung} animalischer {Muskeln} und die {Fortpflanzungsgeschwindigkeit} der {Reizung} in den {Nerven}},
	volume = {17},
	url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2346292},
	language = {deu},
	urldate = {2022-11-09},
	journal = {Archiv für Anatomie, Physiologie und wissenschaftliche Medicin},
	author = {Von Helmholz, H.},
	year = {1850},
	pages = {176--364},
}

@article{peyrard_how_2020,
	title = {How is information transmitted in a nerve?},
	volume = {46},
	issn = {0092-0606},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7719126/},
	doi = {10.1007/s10867-020-09557-2},
	abstract = {In the last 15 years, a debate has emerged about the validity of the famous Hodgkin-Huxley model for nerve impulse. Mechanical models have been proposed. This note reviews the experimental properties of the nerve impulse and discusses the proposed alternatives. The experimental data, which rule out some of the alternative suggestions, show that while the Hodgkin-Huxley model may not be complete, it nevertheless includes essential features that should not be overlooked in the attempts made to improve, or supersede, it.},
	number = {4},
	urldate = {2022-11-08},
	journal = {Journal of Biological Physics},
	author = {Peyrard, Michel},
	month = dec,
	year = {2020},
	pmid = {33037976},
	pmcid = {PMC7719126},
	pages = {327--341},
}

@article{jazayeri_optimal_2006,
	title = {Optimal representation of sensory information by neural populations},
	volume = {9},
	doi = {10.1038/nn1691},
	number = {5},
	journal = {Nature neuroscience},
	author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
	year = {2006},
	note = {Publisher: Nature Publishing Group},
	pages = {690--696},
}

@article{lichtsteiner_128x128_2008,
	title = {A 128x128, 120 {dB} 15 ms {Latency} {Asynchronous} {Temporal} {Contrast} {Vision} {Sensor}},
	volume = {2},
	issn = {0018-9200, 1558-173X},
	url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000004444573},
	doi = {10.1109/JSSC.2007.914337},
	abstract = {This paper describes a 128 times 128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1\% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is \&amp;gt; 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at \&amp;gt; 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum\&lt;sup\&gt;2\&lt;/sup\&gt; pixels with 9.4\% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.},
	language = {English},
	number = {43},
	urldate = {2022-11-08},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Lichtsteiner, P. and Posch, C. and Delbruck, T.},
	year = {2008},
	pages = {566--576},
}

@article{schmitt_ultrastructure_1939,
	title = {The {Ultrastructure} of the {Nerve} {Axon} {Sheath}},
	volume = {14},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-185X.1939.tb00922.x},
	doi = {10.1111/j.1469-185X.1939.tb00922.x},
	abstract = {1. In avoiding certain inherent indeterminacies in classical morphological methods and in obtaining further details regarding the microscopic and ultra-microscopic structure of nerve axon sheaths, the methods of polarization optics and X-ray diffraction are of great value. In the case of the myelin sheaths of vertebrate nerve fibres, for example, the optical and diffraction studies indicate the structure of the living fibre's sheath to be of smectic mixed fluid-crystalline nature. The structure is, therefore, readily altered by chemical treatment to form the artifacts commonly observed in histological preparations. 2. A number of considerations suggest that the specific configuration of the lipoid and protein components of the myelin sheath is as follows. The proteins occur as thin sheets wrapped concentrically about the axon, with two bimolecular layers of lipoids interspersed between adjacent protein layers. While this means that in a radial direction within the cylindrical sheath there are alternate predominantly aqueous and predominantly hyirocarbon phases, the latter cannot be described as being entirely “non-aqueous” 3. Polarization optical studies show that, contrary to the general view, invertebrate nerve fibres quite widely possess, aside from connective tissue investments, thin sheaths which are essentially similar in ultrastructure to the well-defined myelin sheaths of vertebrate fibres. The demonstration of this fact involved a reinterpretation of the meaning of Gothlin's metatropic reaction, in which immersion of the fibre in media of high refractive index permits the (intrinsic) birefringence of lipoids present in the normal sheath in an oriented condition to become apparent by the reduction of the masking (form) double refraction of protein. Associated with the invertebrate metatropic axon sheaths are cells similar to the Schwann cells of vertebrate fibres. 4. Quantitative birefringence studies have disclosed that the axon sheaths of a wide variety of fibre types differ chiefly with respect to the relative amounts of oriented protein and lipoid present. This difference is observed not only between typical invertebrate and vertebrate fibres, but also when the fibres of a single vertebrate nerve are compared. For example, the curve obtained when sheath birefringence of frog sciatic fibres is plotted against fibre diameter shows wide variations in the magnitude of double refraction, changing continuously from birefringence due preponderantly to lipoids, in the case of the larger fibres, to that which, in the smallest fibres, results primarily from proteins. The transition from lipoid to protein predominance occurs at a fibre diameter of about 2μ., agreeing well with the division between “medullated” and “non-medullated” fibres arrived at by histologists. It has been suggested that the low concentration of lipoid in the sheaths of small fibres is related to physical factors opposing the introduction of the lipoids into cylindrical structures of high curvature. 5. Examination of available information with respect to the relation of the velocity of impulse propagation to certain fibre characteristics, such as diameter and sheath ultrastructure, indicates that in a wide variety of fibres conduction velocity is a function of both of these factors. Thus, if fibres from invertebrate and vertebrate sources are classified according to sheath composition and ultrastructure, it is found that, within a group having similar sheaths, fast conduction is favoured by large diameter, while between groups with different sheaths, heavy myelination results in faster propagation. Comparison of fibre velocities with diameter alone, without regard to degree of myelination, is apt to be confusing, a fact which should be borne in mind in attempting to relate conduction velocity to diameter in a nerve, such as the frog sciatic, which contains fibres with very different sheaths. 6. Several types of invertebrate and vertebrate unipolar ganglion cells have been observed to possess investments similar to the axon sheaths and continuous with the latter. The entire surface of these neurons, therefore, is provided with a characteristic lipoid-protein covering, except possibly at the nodes of the myelin sheaths of the vertebrate sensory axons. The limiting envelopes of certain other cells and nuclei have been shown to possess an ultrastructure similar in type to that of the axon sheath. Permeability studies on cells have indicated the importance of lipoids and proteins in determining the properties of the plasma membrane, but it cannot be concluded that the visible envelopes are identical with the membrane which determines the physiological properties, since electrical and chemical studies favour the view that this membrane is extremely thin. The parallelisms observed between nerve sheath ultrastructure and physiological function, however, suggest some relation of these to membrane phenomena, and it is particularly difficult to understand how a multilayered structure, such as the vertebrate axon's myelin sheath, could fail to influence the chemical and electrical accessibility of the axon.},
	language = {en},
	number = {1},
	urldate = {2022-11-08},
	journal = {Biological Reviews},
	author = {Schmitt, Francis O. and Bear, Richard S.},
	year = {1939},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-185X.1939.tb00922.x},
	pages = {27--50},
}

@article{reynolds_study_1928,
	title = {A {Study} of the {Structure} and {Function} of the {Interstitial} {Tissue} of the {Central} {Nervous} {System}},
	volume = {35},
	issn = {0367-1038},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5297025/},
	number = {2},
	urldate = {2022-11-08},
	journal = {Edinburgh Medical Journal},
	author = {Reynolds, F. E. and Slater, James K.},
	month = feb,
	year = {1928},
	pmid = {null},
	pmcid = {PMC5297025},
	pages = {49--57},
}

@article{brill_conduction_1977,
	title = {Conduction velocity and spike configuration in myelinated fibres: computed dependence on internode distance.},
	volume = {40},
	issn = {0022-3050},
	shorttitle = {Conduction velocity and spike configuration in myelinated fibres},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC492833/},
	abstract = {It has been argued theoretically and confirmed experimentally that conduction velocity (theta) should be proportional to nerve fibre diameter for myelinated fibre tracts, such as normal peripheral nerve, exhibiting 'structural' similarity'. In some axons, however, the nodes of Ranvier are more closely spaced than in normal peripheral nerve. Analytic arguments have suggested that when internodal distance (L) alone is changed, the plot of theta versus L should have a relatively flat maximum. This was confirmed by several previous computer simulations of myelinated axons, but internode lengths of less than half the normal case were not examined. In order to gain insight into impulse propagation in myelinated and remyelinated fibres with short internodal lengths, the present study examines the conduction velocity and spike configuration for a wide range of internodal lengths. As L becomes large, theta falls and finally propagation is blocked; as L becomes small, theta decreases more and more steeply. From this, it is predicted that for fibres with very short internodal lengths, small local changes in L should affect substantially the conduction velocity.},
	number = {8},
	urldate = {2022-11-07},
	journal = {Journal of Neurology, Neurosurgery, and Psychiatry},
	author = {Brill, M H and Waxman, S G and Moore, J W and Joyner, R W},
	month = aug,
	year = {1977},
	pmid = {925697},
	pmcid = {PMC492833},
	pages = {769--774},
}

@article{gasser_axon_1939,
	title = {{AXON} {DIAMETERS} {IN} {RELATION} {TO} {THE} {SPIKE} {DIMENSIONS} {AND} {THE} {CONDUCTION} {VELOCITY} {IN} {MAMMALIAN} {A} {FIBERS}},
	copyright = {Copyright © 1939 by American Physiological Society},
	url = {https://journals.physiology.org/doi/10.1152/ajplegacy.1939.127.2.393},
	doi = {10.1152/ajplegacy.1939.127.2.393},
	language = {en},
	urldate = {2022-11-07},
	journal = {American Journal of Physiology-Legacy Content},
	author = {Gasser, H. S. and Grundfest, H.},
	month = aug,
	year = {1939},
	note = {Publisher: American Physiological Society},
}

@article{benosman_event-based_2014,
	title = {Event-{Based} {Visual} {Flow}},
	volume = {25},
	issn = {2162-237X, 2162-2388},
	url = {https://www.neuromorphic-vision.com/public/publications/3/publication.pdf},
	doi = {10.1109/tnnls.2013.2273537},
	abstract = {This paper introduces a new methodology to compute dense visual ﬂow using the precise timings of spikes from an asynchronous event-based retina. Biological retinas, and their artiﬁcial counterparts, are totally asynchronous and data-driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework to estimate visual ﬂow from the local properties of events’ spatiotemporal space. We will show that precise visual ﬂow orientation and amplitude can be estimated using a local differential approach on the surface deﬁned by coactive events. Experimental results are presented; they show the method adequacy with high data sparseness and temporal resolution of event-based acquisition that allows the computation of motion ﬂow with microsecond accuracy and at very low computational cost.},
	language = {en},
	number = {2},
	urldate = {2022-02-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Benosman, Ryad and Clercq, Charles and Lagorce, Xavier and {Sio-Hoi Ieng} and Bartolozzi, Chiara},
	month = feb,
	year = {2014},
	pages = {407--417},
}

@article{bonilla_analyzing_2022,
	title = {Analyzing time-to-first-spike coding schemes},
	volume = {16},
	url = {https://hal.archives-ouvertes.fr/hal-03796195},
	doi = {10.3389/fnins.2022.971937},
	abstract = {Spiking neural networks (SNNs) using time-to-first-spike (TTFS) codes, in which neurons fire at most once, are appealing for rapid and low power processing. In this theoretical paper, we focus on information coding and decoding in those networks, and introduce a new unifying mathematical framework that allows the comparison of various coding schemes. In an early proposal, called rank-order coding (ROC), neurons are maximally activated when inputs arrive in the order of their synaptic weights, thanks to a shunting inhibition mechanism that progressively desensitizes the neurons as spikes arrive. In another proposal, called NoM coding, only the first N spikes of M input neurons are propagated, and these “first spike patterns” can be readout by downstream neurons with homogeneous weights and no desensitization: as a result, the exact order between the first spikes does not matter. This paper also introduces a third option—“Ranked-NoM” (R-NoM), which combines features from both ROC and NoM coding schemes: only the first N input spikes are propagated, but their order is readout by downstream neurons thanks to inhomogeneous weights and linear desensitization. The unifying mathematical framework allows the three codes to be compared in terms of discriminability, which measures to what extent a neuron responds more strongly to its preferred input spike pattern than to random patterns. This discriminability turns out to be much higher for R-NoM than for the other codes, especially in the early phase of the responses. We also argue that R-NoM is much more hardware-friendly than the original ROC proposal, although NoM remains the easiest to implement in hardware because it only requires binary synapses.},
	urldate = {2022-11-08},
	journal = {Frontiers in Neuroscience},
	author = {Bonilla, Lina and Gautrais, Jacques and Thorpe, Simon and Masquelier, Timothée},
	year = {2022},
	note = {Publisher: Frontiers},
}

@article{ermentrout_reliability_2008,
	title = {Reliability, synchrony and noise},
	volume = {31},
	issn = {0166-2236},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2574942/},
	doi = {10.1016/j.tins.2008.06.002},
	abstract = {The brain is noisy. Neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular. Much of this activity is spontaneous—uncoupled to overt stimuli or motor outputs—leading to questions about the functional impact of this noise. Although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli, recent theoretical and experimental work has shown that noise can play a constructive role—leading to increased reliability or regularity of neuronal firing in single neurons and across populations. These results raise fundamental questions about how noise can influence neural function and computation.},
	number = {8},
	urldate = {2022-11-08},
	journal = {Trends in neurosciences},
	author = {Ermentrout, G. Bard and Galán, Roberto F. and Urban, Nathaniel N.},
	month = aug,
	year = {2008},
	pmid = {18603311},
	pmcid = {PMC2574942},
	pages = {428--434},
}

@article{vanni_coinciding_2001,
	title = {Coinciding early activation of the human primary visual cortex  and anteromedial cuneus},
	volume = {98},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC30215/},
	doi = {10.1073/pnas.041600898},
	abstract = {Proper understanding of processes underlying visual perception
 requires information on the activation order of distinct brain areas.
 We measured dynamics of cortical signals with magnetoencephalography
 while human subjects viewed stimuli at four visual quadrants. The
 signals were analyzed with minimum current estimates at the individual
 and group level. Activation emerged 55–70 ms after stimulus onset both
 in the primary posterior visual areas and in the anteromedial part of
 the cuneus. Other cortical areas were active after this initial dual
 activation. Comparison of data between species suggests that the
 anteromedial cuneus either comprises a homologue of the monkey area V6
 or is an area unique to humans. Our results show that visual stimuli
 activate two cortical areas right from the beginning of the cortical
 response. The anteromedial cuneus has the temporal position needed to
 interact with the primary visual cortex V1 and thereby to modify
 information transferred via V1 to extrastriate cortices.},
	number = {5},
	urldate = {2022-11-08},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Vanni, Simo and Tanskanen, Topi and Seppä, Mika and Uutela, Kimmo and Hari, Riitta},
	month = feb,
	year = {2001},
	pmid = {11226316},
	pmcid = {PMC30215},
	pages = {2776--2780},
}

@article{schmolesky_signal_1998,
	title = {Signal timing across the macaque visual system},
	volume = {79},
	issn = {0022-3077},
	doi = {10.1152/jn.1998.79.6.3272},
	abstract = {The onset latencies of single-unit responses evoked by flashing visual stimuli were measured in the parvocellular (P) and magnocellular (M) layers of the dorsal lateral geniculate nucleus (LGNd) and in cortical visual areas V1, V2, V3, V4, middle temporal area (MT), medial superior temporal area (MST), and in the frontal eye field (FEF) in individual anesthetized monkeys. Identical procedures were carried out to assess latencies in each area, often in the same monkey, thereby permitting direct comparisons of timing across areas. This study presents the visual flash-evoked latencies for cells in areas where such data are common (V1 and V2), and are therefore a good standard, and also in areas where such data are sparse (LGNd M and P layers, MT, V4) or entirely lacking (V3, MST, and FEF in anesthetized preparation). Visual-evoked onset latencies were, on average, 17 ms shorter in the LGNd M layers than in the LGNd P layers. Visual responses occurred in V1 before any other cortical area. The next wave of activation occurred concurrently in areas V3, MT, MST, and FEF. Visual response latencies in areas V2 and V4 were progressively later and more broadly distributed. These differences in the time course of activation across the dorsal and ventral streams provide important temporal constraints on theories of visual processing.},
	language = {eng},
	number = {6},
	journal = {Journal of Neurophysiology},
	author = {Schmolesky, M. T. and Wang, Y. and Hanes, D. P. and Thompson, K. G. and Leutgeb, S. and Schall, J. D. and Leventhal, A. G.},
	month = jun,
	year = {1998},
	pmid = {9636126},
	keywords = {Animals, Evoked Potentials, Visual, Macaca, Neurons, Photic Stimulation, Signal Transduction, Time Factors, Vision, Ocular, Visual Cortex},
	pages = {3272--3278},
}

@inproceedings{grimaldi_learning_2022,
	title = {Learning hetero-synaptic delays for motion detection in a single layer of spiking neurons},
	doi = {10.1109/ICIP46576.2022.9897394},
	abstract = {The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. However, most artificial neuronal models do not take advantage of this minute temporal dimension and here, we develop a model for the efficient detection of temporal spiking motifs based on a layer of neurons with hetero-synaptic delays. Indeed, the variety of synaptic delays on the dendritic tree allows to synchronize synaptic inputs as they reach the basal dendritic tree. We show this can be formalized as a time-invariant logistic regression which can be trained using labelled data. We apply this model to solve the specific computer vision problem of motion detection, and demonstrate its application to synthetic naturalistic videos transformed into event streams similar to the output of event-based cameras. In particular, we quantify how its accuracy can vary with the total computational load. This end-to-end event-driven computational brick could help improve the performance of future Spiking Neural Network (SNN) algorithms and their prospective use in neuromorphic chips.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Grimaldi, Antoine and Perrinet, Laurent U},
	month = oct,
	year = {2022},
	note = {ISSN: 2381-8549},
	keywords = {Biological neural networks, Cameras, Delays, Motion detection, Neuromorphics, Neurons, Synchronization, efficient coding, event-based computations, logistic regression, motion detection, spiking neural networks, time code},
	pages = {3591--3595},
}

@inproceedings{sironi_hats_2018,
	address = {Salt Lake City, UT, USA},
	title = {{HATS}: {Histograms} of {Averaged} {Time} {Surfaces} for {Robust} {Event}-{Based} {Object} {Classification}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{HATS}},
	url = {https://ieeexplore.ieee.org/document/8578284/},
	doi = {10.1109/CVPR.2018.00186},
	abstract = {Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classiﬁcation algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their framebased counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classiﬁcation and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efﬁciently leverage past temporal information and build a robust eventbased representation. Second, we release the ﬁrst large real-world event-based dataset for object classiﬁcation. We compare our method to the state-of-the-art with extensive experiments, showing better classiﬁcation performance and real-time computation.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sironi, Amos and Brambilla, Manuele and Bourdis, Nicolas and Lagorce, Xavier and Benosman, Ryad},
	month = jun,
	year = {2018},
	pages = {1731--1740},
}

@article{maro_event-based_2020,
	title = {Event-{Based} {Gesture} {Recognition} {With} {Dynamic} {Background} {Suppression} {Using} {Smartphone} {Computational} {Capabilities}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00275/full?report=reader},
	doi = {10.3389/fnins.2020.00275},
	abstract = {In this paper, we introduce a framework for dynamic gesture recognition with background suppression operating on the output of a moving event-based camera. The system is developed to operate in real-time using only the computational capabilities of a mobile phone. It introduces a new development around the concept of time-surfaces. It also presents a novel event-based methodology to dynamically remove backgrounds that uses the high temporal resolution properties of event-based cameras. To our knowledge, this is the first Android event-based framework for vision-based recognition of {\textbackslash}textit\{dynamic\} gestures running on a smartphone without off-board processing. We assess the performances by considering several scenarios in both indoors and outdoors, for static and dynamic conditions, in uncontrolled lighting conditions. We also introduce a new event-based dataset for gesture recognition with static and dynamic backgrounds (made publicly available). The set of gestures has been selected following a clinical trial to allow human-machine interaction for the visually impaired and older adults. We finally report comparisons with prior work that addressed event-based gesture recognition reporting comparable results, without the use of advanced classification techniques nor power greedy hardware.},
	language = {English},
	urldate = {2021-01-15},
	journal = {Frontiers in Neuroscience},
	author = {Maro, Jean-Matthieu and Ieng, Sio-Hoi and Benosman, Ryad},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Background suppression, Dynamic gesture recognition, Neuromorphic, dynamic vision sensor (DVS), event-based, gesture recognition, mobile device, smartphone},
}

@techreport{chintaluri_metabolically_2022,
	type = {preprint},
	title = {Metabolically driven action potentials serve neuronal energy homeostasis and protect from reactive oxygen species},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.10.16.512428},
	abstract = {So-called spontaneous neuronal activity is a central hallmark of most nervous systems. Such non-causal firing is contrary to the tenet of spikes as a means of communication, and its origin and purpose remain unclear. Here, we propose that non-input driven firing can serve as a release valve to protect neurons from the toxic conditions arising in mitochondria from lower-than-baseline energy consumption. We built a framework of models that incorporate homeostatic control of metabolic products–ATP, ADP, and reactive oxygen species, among others–by way of changes in firing. Our theory can account for key features of neuronal activity observed in many experiments in studies ranging from ion channels function all the way to resting state dynamics. We propose an integrated, crucial role for metabolic spiking that bridges the gap between metabolic homeostasis and neuronal function. Finally, we make testable predictions to validate or falsify our theory.},
	language = {en},
	urldate = {2022-11-07},
	institution = {Neuroscience},
	author = {Chintaluri, Chaitanya and Vogels, Tim P.},
	month = oct,
	year = {2022},
	doi = {10.1101/2022.10.16.512428},
}

@article{seidl_mechanisms_2010,
	title = {Mechanisms for adjusting interaural time differences to achieve binaural coincidence detection},
	volume = {30},
	issn = {1529-2401},
	doi = {10.1523/JNEUROSCI.3464-09.2010},
	abstract = {Understanding binaural perception requires detailed analyses of the neural circuitry responsible for the computation of interaural time differences (ITDs). In the avian brainstem, this circuit consists of internal axonal delay lines innervating an array of coincidence detector neurons that encode external ITDs. Nucleus magnocellularis (NM) neurons project to the dorsal dendritic field of the ipsilateral nucleus laminaris (NL) and to the ventral field of the contralateral NL. Contralateral-projecting axons form a delay line system along a band of NL neurons. Binaural acoustic signals in the form of phase-locked action potentials from NM cells arrive at NL and establish a topographic map of sound source location along the azimuth. These pathways are assumed to represent a circuit similar to the Jeffress model of sound localization, establishing a place code along an isofrequency contour of NL. Three-dimensional measurements of axon lengths reveal major discrepancies with the current model; the temporal offset based on conduction length alone makes encoding of physiological ITDs impossible. However, axon diameter and distances between Nodes of Ranvier also influence signal propagation times along an axon. Our measurements of these parameters reveal that diameter and internode distance can compensate for the temporal offset inferred from axon lengths alone. Together with other recent studies, these unexpected results should inspire new thinking on the cellular biology, evolution, and plasticity of the circuitry underlying low-frequency sound localization in both birds and mammals.},
	language = {eng},
	number = {1},
	journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
	author = {Seidl, Armin H. and Rubel, Edwin W. and Harris, David M.},
	month = jan,
	year = {2010},
	pmid = {20053889},
	pmcid = {PMC2822993},
	keywords = {Acoustic Stimulation, Animals, Animals, Newborn, Auditory Pathways, Auditory Perception, Brain Stem, Chickens, Nerve Net, Sound Localization, Time Factors, itd},
	pages = {70--80},
}

@article{goltz_fast_2020,
	title = {Fast and deep: energy-efficient neuromorphic learning with first-spike times},
	shorttitle = {Fast and deep},
	url = {http://arxiv.org/abs/1912.11443},
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems also strive for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. In the time-to-first-spike-coding framework, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of learning such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how it can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the chip's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	urldate = {2021-03-02},
	journal = {arXiv:1912.11443 [cs, q-bio, stat]},
	author = {Göltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	month = nov,
	year = {2020},
	note = {arXiv: 1912.11443},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	url = {https://ieeexplore.ieee.org/document/9138762/},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a ﬁxed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of ms), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging ﬁeld of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic ﬂow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efﬁcient, bio-inspired way for machines to perceive and interact with the world.},
	language = {en},
	number = {1},
	urldate = {2022-07-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jorg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	pages = {154--180},
}

@article{zenke_visualizing_2021,
	title = {Visualizing a joint future of neuroscience and neuromorphic engineering},
	volume = {109},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S089662732100009X},
	doi = {10.1016/j.neuron.2021.01.009},
	abstract = {Recent research resolves the challenging problem of building biophysically plausible spiking neural models that are also capable of complex information processing. This advance creates new opportunities in neuroscience and neuromorphic engineering, which we discussed at an online focus meeting.},
	language = {en},
	number = {4},
	urldate = {2021-02-24},
	journal = {Neuron},
	author = {Zenke, Friedemann and Bohté, Sander M. and Clopath, Claudia and Comşa, Iulia M. and Göltz, Julian and Maass, Wolfgang and Masquelier, Timothée and Naud, Richard and Neftci, Emre O. and Petrovici, Mihai A. and Scherr, Franz and Goodman, Dan F. M.},
	month = feb,
	year = {2021},
	pages = {571--575},
}

@article{christensen_2022_2022,
	title = {2022 roadmap on neuromorphic computing and engineering},
	issn = {2634-4386},
	url = {http://iopscience.iop.org/article/10.1088/2634-4386/ac4a83},
	doi = {10.1088/2634-4386/ac4a83},
	abstract = {Modern computation based on the von Neumann architecture is today a mature cutting-edge science. In the Von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this Roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The Roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this Roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.},
	language = {en},
	urldate = {2022-01-13},
	journal = {Neuromorphic Computing and Engineering},
	author = {Christensen, Dennis Valbjørn and Dittmann, Regina and Linares-Barranco, Bernabe and Sebastian, Abu and Le Gallo, Manuel and Redaelli, Andrea and Slesazeck, Stefan and Mikolajick, Thomas and Spiga, Sabina and Menzel, Stephan and Valov, Ilia and Milano, Gianluca and Ricciardi, Carlo and Liang, Shi-Jun and Miao, Feng and Lanza, Mario and Quill, Tyler J. and Keene, Scott Tom and Salleo, Alberto and Grollier, Julie and Markovic, Danijela and Mizrahi, Alice and Yao, Peng and Yang, J. Joshua and Indiveri, Giacomo and Strachan, John Paul and Datta, Suman and Vianello, Elisa and Valentian, Alexandre and Feldmann, Johannes and Li, Xuan and Pernice, Wolfram HP and Bhaskaran, Harish and Furber, Steve and Neftci, Emre and Scherr, Franz and Maass, Wolfgang and Ramaswamy, Srikanth and Tapson, Jonathan and Panda, Priyadarshini and Kim, Youngeun and Tanaka, Gouhei and Thorpe, Simon and Bartolozzi, Chiara and Cleland, Thomas A and Posch, Christoph and Liu, Shih-Chii and Panuccio, Gabriella and Mahmud, Mufti and Mazumder, Arnab Neelim and Hosseini, Morteza and Mohsenin, Tinoosh and Donati, Elisa and Tolu, Silvia and Galeazzi, Roberto and Christensen, Martin Ejsing and Holm, Sune and Ielmini, Daniele and Pryds, Nini},
	year = {2022},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.},
	language = {en},
	number = {7784},
	urldate = {2021-03-23},
	journal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	month = nov,
	year = {2019},
	note = {Number: 7784
Publisher: Nature Publishing Group},
	pages = {607--617},
}

@article{olshausen_sparse_2017,
	title = {Sparse codes from memristor grids},
	volume = {12},
	issn = {1748-3395},
	url = {http://www.nature.com/articles/nnano.2017.112},
	doi = {10.1038/nnano.2017.112},
	abstract = {The adjustable resistive state of memristors makes it possible to implement sparse coding algorithms naturally and efficiently.},
	language = {en},
	number = {8},
	urldate = {2021-11-25},
	journal = {Nature Nanotechnology},
	author = {Olshausen, Bruno A. and Rozell, Christopher J.},
	month = aug,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: News \& Views
Publisher: Nature Publishing Group
Subject\_term: Computer science;Electrical and electronic engineering;Network models
Subject\_term\_id: computer-science;electrical-and-electronic-engineering;network-models},
	keywords = {Computer science, Electrical and electronic engineering, Network models},
	pages = {722--723},
}

@article{carr_processing_1993,
	title = {Processing of {Temporal} {Information} in the {Brain}},
	volume = {16},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.ne.16.030193.001255},
	doi = {10.1146/annurev.ne.16.030193.001255},
	language = {en},
	number = {1},
	urldate = {2022-11-07},
	journal = {Annual Review of Neuroscience},
	author = {Carr, C E},
	month = mar,
	year = {1993},
	pages = {223--243},
}

@misc{hazan_memory_2022,
	title = {Memory via {Temporal} {Delays} in weightless {Spiking} {Neural} {Network}},
	url = {http://arxiv.org/abs/2202.07132},
	abstract = {A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Hazan, Hananel and Caby, Simon and Earl, Christopher and Siegelmann, Hava and Levin, Michael},
	month = feb,
	year = {2022},
	note = {arXiv:2202.07132 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Computation},
}

@article{rueckauer_conversion_2017,
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	doi = {10.3389/fnins.2017.00682},
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	urldate = {2022-10-25},
	journal = {Frontiers in Neuroscience},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	year = {2017},
}

@article{gautrais_rate_1998,
	title = {Rate coding versus temporal order coding: a theoretical approach},
	volume = {48},
	shorttitle = {Rate coding versus temporal order coding},
	doi = {10.1016/S0303-2647(98)00050-1},
	number = {1-3},
	journal = {Biosystems},
	author = {Gautrais, Jacques and Thorpe, Simon},
	year = {1998},
	note = {Publisher: Elsevier},
	pages = {57--65},
}

@article{delorme_spikenet_1999,
	title = {{SpikeNET}: {A} simulator for modeling large networks of integrate and fire neurons},
	volume = {26},
	shorttitle = {{SpikeNET}},
	doi = {10.1016/S0925-2312(99)00095-8},
	journal = {Neurocomputing},
	author = {Delorme, Arnaud and Gautrais, Jacques and Van Rullen, Rufin and Thorpe, Simon},
	year = {1999},
	note = {Publisher: Elsevier},
	pages = {989--996},
}

@article{serre_feedforward_2007,
	title = {A feedforward architecture accounts for rapid categorization},
	volume = {104},
	doi = {10.1073/pnas.0700622104},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Serre, T. and Oliva, A. and Poggio, T.},
	year = {2007},
	note = {tex.bdsk-url-2: https://doi.org/10.1073/pnas.0700622104
tex.date-added: 2022-05-05 19:08:15 +0200
tex.date-modified: 2022-05-05 19:08:15 +0200},
	pages = {6424--6429},
}

@book{datadien_right_2011,
	title = {The {Right} {Delay} - {Detecting} {Specific} {Spike} {Patterns} with {STDP} and {Axonal} {Conduction} {Delays}.},
	abstract = {Axonal conduction delays should not be ignored in simulations of spiking neural networks. Here it is shown that by using axonal
conduction delays, neurons can display sensitivity to a specific spatio-temporal spike pattern. By using delays that complement
the firing times in a pattern, spikes can arrive simultaneously at an output neuron, giving it a high chance of firing in
response to that pattern. An unsupervised learning mechanism called spike-timing-dependent plasticity then increases the weights
for connections used in the pattern, and decreases the others. This allows for an attunement of output neurons to specific
activity patterns, based on temporal aspects of axonal conductivity.},
	author = {Datadien, Arvind and Haselager, Pim and Sprinkhuizen-Kuyper, Ida},
	month = jan,
	year = {2011},
	note = {Pages: 99},
}

@article{kreuz_measuring_2007,
	title = {Measuring spike train synchrony},
	volume = {165},
	issn = {0165-0270},
	doi = {10.1016/j.jneumeth.2007.05.031},
	abstract = {Estimating the degree of synchrony or reliability between two or more spike trains is a frequent task in both experimental and computational neuroscience. In recent years, many different methods have been proposed that typically compare the timing of spikes on a certain time scale to be optimized by the analyst. Here, we propose the ISI-distance, a simple complementary approach that extracts information from the interspike intervals by evaluating the ratio of the instantaneous firing rates. The method is parameter free, time scale independent and easy to visualize as illustrated by an application to real neuronal spike trains obtained in vitro from rat slices. In a comparison with existing approaches on spike trains extracted from a simulated Hindemarsh-Rose network, the ISI-distance performs as well as the best time-scale-optimized measure based on spike timing.},
	language = {eng},
	number = {1},
	journal = {Journal of Neuroscience Methods},
	author = {Kreuz, Thomas and Haas, Julie S. and Morelli, Alice and Abarbanel, Henry D. I. and Politi, Antonio},
	month = sep,
	year = {2007},
	pmid = {17628690},
	keywords = {Action Potentials, Animals, Electrophysiology, Neurons, Rats},
	pages = {151--161},
}

@article{meister_concerted_1995,
	title = {Concerted {Signaling} by {Retinal} {Ganglion} {Cells}},
	volume = {270},
	url = {https://doi.org/dszfrn},
	doi = {10.1126/science.270.5239.1207},
	language = {en},
	number = {5239},
	journal = {Science},
	author = {Meister, Markus and Lagnado, Leon and Baylor, Denis A.},
	month = nov,
	year = {1995},
	pages = {1207--1210},
}

@article{suthers_motor_2002,
	title = {Motor control of birdsong.},
	volume = {12},
	issn = {0959-4388},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/12490259},
	doi = {10.1016/s0959-4388(02)00386-0},
	abstract = {One of the challenges when considering the motor control of birdsong is to understand how such a wide variety of temporally and spectrally diverse vocalizations are learned and produced. A better understanding of central neural processing, together with direct endoscopic observations and physiological studies of peripheral motor function during singing, has resulted in the formation of new theoretical models of song production. Recent work suggests that it may be more profitable to focus on the temporal relationship between control parameters than to attempt to directly correlate neural processing with details of the acoustic output.},
	number = {6},
	journal = {Current opinion in neurobiology},
	author = {Suthers, Roderick A and Margoliash, Daniel},
	year = {2002},
	pages = {684--90},
}

@article{berry_spike_2022,
	title = {Spike {Trains} of {Retinal} {Ganglion} {Cells} {Viewing} a {Repeated} {Natural} {Movie}},
	url = {https://dataspace.princeton.edu/handle/88435/dsp015425kd84r},
	doi = {10.34770/V0V4-3H52},
	abstract = {This archive contains spike trains simultaneously recorded from ganglion cells in the tiger salamander retina with a multi-electrode array while viewing a repeated natural movie clip.  These data have been analyzed in previous papers, notably Puchalla et al. Neuron 2005 and Schneidman et al. Nature 2006.},
	language = {en\_US},
	urldate = {2022-10-04},
	author = {Berry, Michael J.},
	month = mar,
	year = {2022},
}

@article{caporale_spike_2008,
	title = {Spike {Timing}–{Dependent} {Plasticity}: {A} {Hebbian} {Learning} {Rule}},
	volume = {31},
	url = {https://doi.org/fqxrgj},
	doi = {10.1146/annurev.neuro.31.060407.125639},
	abstract = {{\textless}jats:p{\textgreater} Spike timing–dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Caporale, Natalia and Dan, Yang},
	year = {2008},
	pages = {25--46},
}

@article{guise_bayesian_2014,
	title = {A {Bayesian} {Model} of {Polychronicity}},
	volume = {26},
	url = {https://doi.org/f6chbq},
	doi = {10.1162/neco_a_00620},
	abstract = {{\textless}jats:p{\textgreater} A significant feature of spiking neural networks with varying connection delays, such as those in the brain, is the existence of strongly connected groups of neurons known as polychronous neural groups (PNGs). Polychronous groups are found in large numbers in these networks and are proposed by Izhikevich ( 2006a ) to provide a neural basis for representation and memory. When exposed to a familiar stimulus, spiking neural networks produce consistencies in the spiking output data that are the hallmarks of PNG activation. Previous methods for studying the PNG activation response to stimuli have been limited by the template-based methods used to identify PNG activation. In this letter, we outline a new method that overcomes these difficulties by establishing for the first time a probabilistic interpretation of PNG activation. We then demonstrate the use of this method by investigating the claim that PNGs might provide the foundation of a representational system. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {9},
	journal = {Neural Computation},
	author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
	month = sep,
	year = {2014},
	pages = {2052--2073},
}

@article{thorpe_seeking_2001,
	title = {Seeking {Categories} in the {Brain}},
	volume = {291},
	url = {https://doi.org/bzn42k},
	doi = {10.1126/science.1058249},
	language = {en},
	number = {5502},
	journal = {Science},
	author = {Thorpe, Simon J. and Fabre-Thorpe, Michèle},
	month = jan,
	year = {2001},
	pages = {260--263},
}

@article{russo_cell_2017,
	title = {Cell assemblies at multiple time scales with arbitrary lag constellations},
	volume = {6},
	url = {https://doi.org/f9kxd8},
	doi = {10.7554/elife.19428},
	abstract = {{\textless}jats:p{\textgreater}Hebb's idea of a cell assembly as the fundamental unit of neural information processing has dominated neuroscience like no other theoretical concept within the past 60 years. A range of different physiological phenomena, from precisely synchronized spiking to broadly simultaneous rate increases, has been subsumed under this term. Yet progress in this area is hampered by the lack of statistical tools that would enable to extract assemblies with arbitrary constellations of time lags, and at multiple temporal scales, partly due to the severe computational burden. Here we present such a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. Applying this methodology to multiple single unit recordings from various cortical areas, we find that there is no universal cortical coding scheme, but that assembly structure and precision significantly depends on the brain area recorded and ongoing task demands.{\textless}/jats:p{\textgreater}},
	language = {en},
	journal = {eLife},
	author = {Russo, Eleonora and Durstewitz, Daniel},
	month = jan,
	year = {2017},
}

@techreport{rasetto_challenges_2022,
	title = {The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event} {Processors}: {How} {Memristors} {Dynamic} {Properties} {Could} {Revolutionize} {Machine} {Learning}},
	url = {https://arxiv.org/abs/2201.12673},
	abstract = {Neuromorphic engineering has led to the necessary process of rethinking of how we process and integrate information, analyze data, and use the resulting insights to improve computation and avoid the current high power and latency of Artificial Intelligence (AI) hardware. Current neuromorphic processors are, however, limited by digital technologies, which cannot reproduce the abilities of biological neural computation in terms of power, latency and area cost. In this paper, we show that the combined use of the dynamic properties of memristors to implement a model of synaptic integration and the determination of the correct level of abstraction of biological neural networks has the potential to open a new range of capabilities for neuromorphic processors. We test this approach using a novel three-terminal LixWO3 electrochemical memristor, by deriving its conductance model and using it to emulate synaptic temporal kernel computation in the context of a pattern recognition task. We show that these devices allow for robust results with no loss in precision while opening the path for an energy efficient approach to build novel bio-inspired processing units in silicon.},
	number = {2201.12673},
	institution = {arXiv},
	author = {Rasetto, Marco and Wan, Qingzhou and Akolkar, Himanshu and Shi, Bertram and Xiong, Feng and Benosman, Ryad},
	year = {2022},
}

@article{moser_stability_2014,
	title = {On {Stability} of {Distance} {Measures} for {Event} {Sequences} {Induced} by {Level}-{Crossing} {Sampling}},
	volume = {62},
	url = {https://doi.org/gnpb7w},
	doi = {10.1109/tsp.2014.2305642},
	number = {8},
	journal = {IEEE Transactions on Signal Processing},
	author = {Moser, Bernhard A. and Natschlager, Thomas},
	year = {2014},
	pages = {1987--1999},
}

@article{brette_computing_2012,
	title = {Computing with {Neural} {Synchrony}},
	volume = {8},
	url = {https://doi.org/f32tvz},
	doi = {10.1371/journal.pcbi.1002561},
	language = {en},
	number = {6},
	journal = {PLoS Computational Biology},
	author = {Brette, Romain},
	editor = {Sporns, Olaf},
	year = {2012},
	pages = {e1002561},
}

@article{schrader_detecting_2008,
	title = {Detecting {Synfire} {Chain} {Activity} {Using} {Massively} {Parallel} {Spike} {Train} {Recording}},
	volume = {100},
	url = {https://doi.org/cvb22p},
	doi = {10.1152/jn.01245.2007},
	abstract = {{\textless}jats:p{\textgreater} The synfire chain model has been proposed as the substrate that underlies computational processes in the brain and has received extensive theoretical study. In this model cortical tissue is composed of a superposition of feedforward subnetworks (chains) each capable of transmitting packets of synchronized spikes with high reliability. Computations are then carried out by interactions of these chains. Experimental evidence for synfire chains has so far been limited to inference from detection of a few repeating spatiotemporal neuronal firing patterns in multiple single-unit recordings. Demonstration that such patterns actually come from synfire activity would require finding a meta organization among many detected patterns, as yet an untried approach. In contrast we present here a new method that directly visualizes the repetitive occurrence of synfire activity even in very large data sets of multiple single-unit recordings. We achieve reliability and sensitivity by appropriately averaging over neuron space (identities) and time. We test the method with data from a large-scale balanced recurrent network simulation containing 50 randomly activated synfire chains. The sensitivity is high enough to detect synfire chain activity in simultaneous single-unit recordings of 100 to 200 neurons from such data, enabling application to experimental data in the near future. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {4},
	journal = {Journal of Neurophysiology},
	author = {Schrader, Sven and Grün, Sonja and Diesmann, Markus and Gerstein, George L.},
	month = oct,
	year = {2008},
	pages = {2165--2176},
}

@article{pillow_spatio-temporal_2008,
	title = {Spatio-temporal correlations and visual signalling in a complete neuronal population},
	volume = {454},
	url = {https://doi.org/dzvdm3},
	doi = {10.1038/nature07140},
	language = {en},
	number = {7207},
	journal = {Nature},
	author = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
	year = {2008},
	pages = {995--999},
}

@techreport{yin_sata_2022,
	title = {{SATA}: {Sparsity}-{Aware} {Training} {Accelerator} for {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2204.05422},
	abstract = {Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. Recently, SNNs with backpropagation through time (BPTT) have achieved a higher accuracy result on image recognition tasks compared to other SNN training algorithms. Despite the success on the algorithm perspective, prior works neglect the evaluation of the hardware energy overheads of BPTT, due to the lack of a hardware evaluation platform for SNN training algorithm design. Moreover, although SNNs have been long seen as an energy-efficient counterpart of ANNs, a quantitative comparison between the training cost of SNNs and ANNs is missing. To address the above-mentioned issues, in this work, we introduce SATA (Sparsity-Aware Training Accelerator), a BPTT-based training accelerator for SNNs. The proposed SATA provides a simple and re-configurable accelerator architecture for the general-purpose hardware evaluation platform, which makes it easier to analyze the training energy for SNN training algorithms. Based on SATA, we show quantitative analyses on the energy efficiency of SNN training and make a comparison between the training cost of SNNs and ANNs. The results show that SNNs consume \$1.27{\textbackslash}times\$ more total energy with considering sparsity (spikes, gradient of firing function, and gradient of membrane potential) when compared to ANNs. We find that such high training energy cost is from time-repetitive convolution operations and data movements during backpropagation. Moreover, to guide the future SNN training algorithm design, we provide several observations on energy efficiency with respect to different SNN-specific training parameters.},
	number = {2204.05422},
	institution = {arXiv},
	author = {Yin, Ruokai and Moitra, Abhishek and Bhattacharjee, Abhiroop and Kim, Youngeun and Panda, Priyadarshini},
	year = {2022},
}

@techreport{bellec_fitting_2021,
	title = {Fitting summary statistics of neural data with a differentiable spiking network simulator},
	url = {https://arxiv.org/abs/2106.10064},
	abstract = {Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.},
	number = {2106.10064},
	institution = {arXiv},
	author = {Bellec, Guillaume and Wang, Shuqi and Modirshanechi, Alireza and Brea, Johanni and Gerstner, Wulfram},
	month = nov,
	year = {2021},
}

@article{wild_descending_1993,
	title = {Descending projections of the songbird nucleus robustus archistriatalis.},
	volume = {338},
	issn = {0021-9967},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/8308169},
	doi = {10.1002/cne.903380207},
	abstract = {The descending, efferent projections of nucleus robustus archistriatalis were investigated in male zebra finches and greenfinches with injections of either biotinylated dextran amine or cholera toxin B-chain conjugated to horseradish peroxidase. The results show that in addition to the well-known projections to the tracheosyringeal motor nucleus and the dorsomedial nucleus of the intercollicular complex, there are other projections of comparable density to the ipsilateral nucleus ambiguus and nucleus retroambigualis. Within nucleus ambiguus, robustus axons terminate in close proximity to laryngeal motoneurons which were retrogradely labelled in the same bird by injections of cholera B-chain into the laryngeal muscles; and within nucleus retroambigualis robustus axons terminate in relation to bulbospinal neurons previously shown to project to regions of spinal cord containing motoneurons innervating abdominal expiratory muscles (J.M. Wild, Brain Res. 606:119-124, 1993). These projections of nucleus robustus thus seem well placed to coordinate syringeal, laryngeal, and expiratory muscle activity during vocalization. Other relatively sparse, but distinct, projections of nucleus robustus were found to nucleus dorsolateralis anterior thalami, pars medialis, to a narrow region between the superior olivary nucleus and the spinal lemniscus, and to the rostral ventrolateral medulla. Neurons in these last two locations were retrogradely labelled bilaterally following injections of cholera B-chain into nucleus retroambigualis of one side. Together with sparse contralateral projections of nucleus robustus to all brainstem targets receiving ipsilateral projections, potential pathways are thus identified by which the respiratory-vocal activity controlled by one side of the lower medulla can be influenced by the nucleus robustus of either side, thereby possibly bringing about bilateral coordination of respiratory-vocal output.},
	number = {2},
	journal = {The Journal of comparative neurology},
	author = {Wild, J M},
	year = {1993},
	pages = {225--41},
}

@article{wehr_odour_1996,
	title = {Odour encoding by temporal sequences of firing in oscillating neural assemblies},
	volume = {384},
	url = {https://doi.org/b3t32c},
	doi = {10.1038/384162a0},
	language = {en},
	number = {6605},
	journal = {Nature},
	author = {Wehr, Michael and Laurent, Gilles},
	month = nov,
	year = {1996},
	pages = {162--166},
}

@article{branco_dendritic_2010,
	title = {Dendritic {Discrimination} of {Temporal} {Input} {Sequences} in {Cortical} {Neurons}},
	volume = {329},
	url = {https://doi.org/dqx4n4},
	doi = {10.1126/science.1189664},
	language = {en},
	number = {5999},
	journal = {Science},
	author = {Branco, Tiago and Clark, Beverley A. and Häusser, Michael},
	month = sep,
	year = {2010},
	pages = {1671--1675},
}

@article{yu_temporal_1996,
	title = {Temporal hierarchical control of singing in birds.},
	volume = {273},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/8791594},
	doi = {10.1126/science.273.5283.1871},
	abstract = {Songs of birds comprise hierarchical sets of vocal gestures. In zebra finches, songs include notes and syllables (groups of notes) delivered in fixed sequences. During singing, premotor neurons in the forebrain nucleus HVc exhibited reliable changes in activity rates whose patterns were uniquely associated with syllable identity. Neurons in the forebrain nucleus robustus archistriatalis, which receives input from the HVc, exhibited precisely timed and structured bursts of activity that were uniquely associated with note identity. Hence, units of vocal behavior are represented hierarchically in the avian forebrain. The representation of temporal sequences at each level of the hierarchy may be established by means of a decoding process involving interactions of higher level input with intrinsic local circuitry. Behavior is apparently represented by precise temporal patterning of spike trains at lower levels of the hierarchy.},
	number = {5283},
	journal = {Science (New York, N.Y.)},
	author = {Yu, A C and Margoliash, D},
	month = sep,
	year = {1996},
	pages = {1871--5},
}

@article{harris_organization_2003,
	title = {Organization of cell assemblies in the hippocampus},
	volume = {424},
	url = {https://doi.org/bm3vgb},
	doi = {10.1038/nature01834},
	language = {en},
	number = {6948},
	journal = {Nature},
	author = {Harris, Kenneth D. and Csicsvari, Jozsef and Hirase, Hajime and Dragoi, George and Buzsáki, György},
	year = {2003},
	pages = {552--556},
}

@article{stringer_high-precision_2021,
	title = {High-precision coding in visual cortex},
	volume = {184},
	url = {https://doi.org/gjqbjd},
	doi = {10.1016/j.cell.2021.03.042},
	language = {en},
	number = {10},
	journal = {Cell},
	author = {Stringer, Carsen and Michaelos, Michalis and Tsyboulski, Dmitri and Lindo, Sarah E. and Pachitariu, Marius},
	year = {2021},
	pages = {2767--2778.e15},
}

@techreport{williams_point_2020,
	title = {Point process models for sequence detection in high-dimensional neural spike trains},
	url = {https://arxiv.org/abs/2010.04875},
	abstract = {Sparse sequences of neural spikes are posited to underlie aspects of working memory, motor production, and learning. Discovering these sequences in an unsupervised manner is a longstanding problem in statistical neuroscience. Promising recent work utilized a convolutive nonnegative matrix factorization model to tackle this challenge. However, this model requires spike times to be discretized, utilizes a sub-optimal least-squares criterion, and does not provide uncertainty estimates for model predictions or estimated parameters. We address each of these shortcomings by developing a point process model that characterizes fine-scale sequences at the level of individual spikes and represents sequence occurrences as a small number of marked events in continuous time. This ultra-sparse representation of sequence events opens new possibilities for spike train modeling. For example, we introduce learnable time warping parameters to model sequences of varying duration, which have been experimentally observed in neural circuits. We demonstrate these advantages on experimental recordings from songbird higher vocal center and rodent hippocampus.},
	number = {2010.04875},
	institution = {arXiv},
	author = {Williams, Alex H. and Degleris, Anthony and Wang, Yixin and Linderman, Scott W.},
	month = oct,
	year = {2020},
}

@article{thorpe_speed_1996,
	title = {Speed of processing in the human visual system},
	volume = {381},
	url = {https://doi.org/c4v35x},
	doi = {10.1038/381520a0},
	language = {en},
	number = {6582},
	journal = {Nature},
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	year = {1996},
	pages = {520--522},
}

@article{grammont_spike_2003,
	title = {Spike synchronization and firing rate in a population of motor cortical neurons in relation to movement direction and reaction time},
	volume = {88},
	url = {https://doi.org/ctvhsb},
	doi = {10.1007/s00422-002-0385-3},
	number = {5},
	journal = {Biological Cybernetics},
	author = {Grammont, F. and Riehle, A.},
	year = {2003},
	pages = {360--373},
}

@techreport{panahi_generative_2021,
	title = {Generative {Models} of {Brain} {Dynamics} -- {A} review},
	url = {https://arxiv.org/abs/2112.12147},
	abstract = {The principled design and discovery of biologically- and physically-informed models of neuronal dynamics has been advancing since the mid-twentieth century. Recent developments in artificial intelligence (AI) have accelerated this progress. This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, AI, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.},
	number = {2112.12147},
	institution = {arXiv},
	author = {Panahi, Mahta Ramezanian and Abrevaya, Germán and Gagnon-Audet, Jean-Christophe and Voleti, Vikram and Rish, Irina and Dumas, Guillaume},
	year = {2021},
}

@article{perrinet_coherence_2002,
	title = {Coherence detection in a spiking neuron via {Hebbian} learning},
	volume = {44-46},
	url = {https://doi.org/fsk9mk},
	doi = {10.1016/s0925-2312(02)00374-0},
	language = {en},
	journal = {Neurocomputing},
	author = {Perrinet, L. and Samuelides, M.},
	year = {2002},
	pages = {133--139},
}

@article{di_mauro_alfio_sne_2022,
	title = {{SNE}: an {Energy}-{Proportional} {Digital} {Accelerator} for {Sparse} {Event}-{Based} {Convolutions}},
	url = {https://doi.org/gp3m5v},
	doi = {10.3929/ethz-b-000543342},
	abstract = {Event-based sensors are drawing increasing attention due to their high temporal resolution, low power consumption, and low bandwidth. To efficiently extract semantically meaningful information from sparse data streams produced by such sensors, we present a 4.5TOP/s/W digital accelerator capable of performing 4-bits-quantized event-based convolutional neural networks (eCNN). Compared to standard convolutional engines, our accelerator performs a number of operations proportional to the number of events contained into the input data stream, ultimately achieving a high energy-to-information processing proportionality. On the IBM-DVS-Gesture dataset, we report 80uJ/inf to 261uJ/inf, respectively, when the input activity is 1.2\% and 4.9\%. Our accelerator consumes 0.221pJ/SOP, to the best of our knowledge it is the lowest energy/OP reported on a digital neuromorphic engine.},
	language = {en},
	journal = {ETH Zurich},
	author = {{Di Mauro, Alfio} and {Prasad, Arpan Suravi} and {Huang, Zhikai} and {Spallanzani, Matteo} and {Conti, Francesco} and {Benini, Luca}},
	year = {2022},
}

@article{torre_synchronous_2016,
	title = {Synchronous {Spike} {Patterns} in {Macaque} {Motor} {Cortex} during an {Instructed}-{Delay} {Reach}-to-{Grasp} {Task}},
	volume = {36},
	url = {https://doi.org/f82b6j},
	doi = {10.1523/jneurosci.4375-15.2016},
	language = {en},
	number = {32},
	journal = {Journal of Neuroscience},
	author = {Torre, E. and Quaglio, P. and Denker, M. and Brochier, T. and Riehle, A. and Grun, S.},
	year = {2016},
	pages = {8329--8340},
}

@article{singer_visual_1995,
	title = {Visual {Feature} {Integration} and the {Temporal} {Correlation} {Hypothesis}},
	volume = {18},
	url = {https://doi.org/cgx8jp},
	doi = {10.1146/annurev.ne.18.030195.003011},
	language = {en},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Singer, W and Gray, C M},
	month = mar,
	year = {1995},
	pages = {555--586},
}

@article{miller_visual_2014,
	title = {Visual stimuli recruit intrinsically generated cortical ensembles},
	volume = {111},
	url = {https://doi.org/f6htkt},
	doi = {10.1073/pnas.1406077111},
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}This study demonstrates that neuronal groups or ensembles, rather than individual neurons, are emergent functional units of cortical activity. We show that in the presence and absence of visual stimulation, cortical activity is dominated by coactive groups of neurons forming ensembles. These ensembles are flexible and cannot be accounted for by the independent firing properties of neurons in isolation. Intrinsically generated ensembles and stimulus-evoked ensembles are similar, with one main difference: Whereas intrinsic ensembles recur at random time intervals, visually evoked ensembles are time-locked to stimuli. We propose that visual stimuli recruit endogenously generated ensembles to represent visual attributes.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {38},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Miller, Jae-eun Kang and Ayzenshtat, Inbal and Carrillo-Reid, Luis and Yuste, Rafael},
	month = sep,
	year = {2014},
}

@article{stringer_spontaneous_2019,
	title = {Spontaneous behaviors drive multidimensional, brainwide activity},
	volume = {364},
	url = {https://doi.org/gfz6mh},
	doi = {10.1126/science.aav7893},
	language = {en},
	number = {6437},
	journal = {Science},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
	year = {2019},
}

@article{van_kempen_top-down_2021,
	title = {Top-down coordination of local cortical state during selective attention},
	volume = {109},
	url = {https://doi.org/ghvj3k},
	doi = {10.1016/j.neuron.2020.12.013},
	language = {en},
	number = {5},
	journal = {Neuron},
	author = {van Kempen, Jochem and Gieselmann, Marc A. and Boyd, Michael and Steinmetz, Nicholas A. and Moore, Tirin and Engel, Tatiana A. and Thiele, Alexander},
	month = mar,
	year = {2021},
	pages = {894--904.e8},
}

@article{zhang_supervised_2020,
	title = {Supervised learning in spiking neural networks with synaptic delay-weight plasticity},
	volume = {409},
	url = {https://doi.org/ghsm45},
	doi = {10.1016/j.neucom.2020.03.079},
	language = {en},
	journal = {Neurocomputing},
	author = {Zhang, Malu and Wu, Jibin and Belatreche, Ammar and Pan, Zihan and Xie, Xiurui and Chua, Yansong and Li, Guoqi and Qu, Hong and Li, Haizhou},
	month = oct,
	year = {2020},
	pages = {103--118},
}

@article{aviel_embedding_2003,
	title = {On {Embedding} {Synfire} {Chains} in a {Balanced} {Network}},
	volume = {15},
	url = {https://doi.org/fgj3wf},
	doi = {10.1162/089976603321780290},
	abstract = {{\textless}jats:p{\textgreater} We investigate the formation of synfire waves in a balanced network of integrate-and-fire neurons. The synaptic connectivity of this network embodies synfire chains within a sparse random connectivity. This network can exhibit global oscillations but can also operate in an asynchronous activity mode. We analyze the correlations of two neurons in a pool as convenient indicators for the state of the network. We find, using different models, that these indicators depend on a scaling variable. {\textless}/jats:p{\textgreater}{\textless}jats:p{\textgreater} Beyond a critical point, strong correlations and large network oscillations are obtained. We looked for the conditions under which a synfire wave could be propagated on top of an otherwise asynchronous state of the network. This condition was found to be highly restrictive, requiring a large number of neurons for its implementation in our network. The results are based on analytic derivations and simulations. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {6},
	journal = {Neural Computation},
	author = {Aviel, Y. and Mehring, C. and Abeles, M. and Horn, D.},
	year = {2003},
	pages = {1321--1340},
}

@article{weyl_ueber_1916,
	title = {Ueber die {Gleichverteilung} von {Zahlen} mod. {Eins}},
	volume = {77},
	url = {https://doi.org/crprvc},
	doi = {10.1007/bf01475864},
	language = {de},
	number = {3},
	journal = {Mathematische Annalen},
	author = {Weyl, Hermann},
	month = sep,
	year = {1916},
	pages = {313--352},
}

@article{mainen_reliability_1995,
	title = {Reliability of {Spike} {Timing} in {Neocortical} {Neurons}},
	volume = {268},
	url = {https://doi.org/b2mms6},
	doi = {10.1126/science.7770778},
	language = {en},
	number = {5216},
	journal = {Science},
	author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
	year = {1995},
	pages = {1503--1506},
}

@article{tolle_fourth_2011,
	title = {The {Fourth} {Paradigm}: {Data}-{Intensive} {Scientific} {Discovery} [{Point} of {View}]},
	volume = {99},
	url = {https://doi.org/dfggjc},
	doi = {10.1109/jproc.2011.2155130},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Tolle, Kristin M. and Tansley, D. Stewart W. and Hey, Anthony J. G.},
	year = {2011},
	pages = {1334--1337},
}

@article{softky_highly_1993,
	title = {The highly irregular firing of cortical cells is inconsistent with temporal integration of random {EPSPs}},
	volume = {13},
	url = {https://doi.org/ggzph6},
	doi = {10.1523/jneurosci.13-01-00334.1993},
	language = {en},
	number = {1},
	journal = {The Journal of Neuroscience},
	author = {Softky, WR and Koch, C},
	month = jan,
	year = {1993},
	pages = {334--350},
}

@article{gerstner_time_1995,
	title = {Time structure of the activity in neural network models},
	volume = {51},
	url = {https://doi.org/cwcn9d},
	doi = {10.1103/physreve.51.738},
	language = {en},
	number = {1},
	journal = {Physical Review E},
	author = {Gerstner, Wulfram},
	month = jan,
	year = {1995},
	pages = {738--758},
}

@article{pipa_neuroxidence_2008,
	title = {{NeuroXidence}: reliable and efficient analysis of an excess or deficiency of joint-spike events},
	volume = {25},
	url = {https://doi.org/ddh5js},
	doi = {10.1007/s10827-007-0065-3},
	language = {en},
	number = {1},
	journal = {Journal of Computational Neuroscience},
	author = {Pipa, Gordon and Wheeler, Diek W. and Singer, Wolf and Nikolić, Danko},
	month = jan,
	year = {2008},
	pages = {64--88},
}

@article{tavanaei_representation_2018,
	title = {Representation learning using event-based {STDP}},
	volume = {105},
	url = {https://doi.org/gd6tgv},
	doi = {10.1016/j.neunet.2018.05.018},
	language = {en},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Masquelier, Timothée and Maida, Anthony},
	month = sep,
	year = {2018},
	pages = {294--303},
}

@article{stella_comparing_2022,
	title = {Comparing {Surrogates} to {Evaluate} {Precisely} {Timed} {Higher}-{Order} {Spike} {Correlations}},
	volume = {9},
	url = {https://doi.org/gqjvht},
	doi = {10.1523/eneuro.0505-21.2022},
	language = {en},
	number = {3},
	journal = {eneuro},
	author = {Stella, Alessandra and Bouss, Peter and Palm, Günther and Grün, Sonja},
	year = {2022},
	pages = {ENEURO.0505--21.2022},
}

@article{torre_asset_2016,
	title = {{ASSET}: {Analysis} of {Sequences} of {Synchronous} {Events} in {Massively} {Parallel} {Spike} {Trains}},
	volume = {12},
	url = {https://doi.org/gnpx4q},
	doi = {10.1371/journal.pcbi.1004939},
	language = {en},
	number = {7},
	journal = {PLOS Computational Biology},
	author = {Torre, Emiliano and Canova, Carlos and Denker, Michael and Gerstein, George and Helias, Moritz and Grün, Sonja},
	editor = {Sporns, Olaf},
	year = {2016},
	pages = {e1004939},
}

@article{torre_statistical_2013,
	title = {Statistical evaluation of synchronous spike patterns extracted by frequent item set mining},
	volume = {7},
	url = {https://doi.org/gpshgq},
	doi = {10.3389/fncom.2013.00132},
	journal = {Frontiers in Computational Neuroscience},
	author = {Torre, Emiliano and Picado-Muiño, David and Denker, Michael and Borgelt, Christian and Grün, Sonja},
	year = {2013},
}

@article{stringer_high-dimensional_2019,
	title = {High-dimensional geometry of population responses in visual cortex},
	volume = {571},
	url = {https://doi.org/gf4cfj},
	doi = {10.1038/s41586-019-1346-5},
	language = {en},
	number = {7765},
	journal = {Nature},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	year = {2019},
	pages = {361--365},
}

@article{levakova_review_2015,
	title = {A review of the methods for neuronal response latency estimation},
	volume = {136},
	url = {https://doi.org/gpshjz},
	doi = {10.1016/j.biosystems.2015.04.008},
	language = {en},
	journal = {Biosystems},
	author = {Levakova, Marie and Tamborrino, Massimiliano and Ditlevsen, Susanne and Lansky, Petr},
	month = oct,
	year = {2015},
	pages = {23--34},
}

@article{safaie_turning_2020,
	title = {Turning the body into a clock: {Accurate} timing is facilitated by simple stereotyped interactions with the environment},
	volume = {117},
	url = {https://doi.org/gnqsmh},
	doi = {10.1073/pnas.1921226117},
	abstract = {{\textless}jats:p{\textgreater}How animals adapt their behavior according to regular time intervals between events is not well understood, especially when intervals last several seconds. One possibility is that animals use disembodied internal neuronal representations of time to decide when to initiate a given action at the end of an interval. However, animals rarely remain immobile during time intervals but tend to perform stereotyped behaviors, raising the possibility that motor routines improve timing accuracy. To test this possibility, we used a task in which rats, freely moving on a motorized treadmill, could obtain a reward if they approached it after a fixed interval. Most animals took advantage of the treadmill length and its moving direction to develop, by trial-and-error, the same motor routine whose execution resulted in the precise timing of their reward approaches. Noticeably, when proficient animals did not follow this routine, their temporal accuracy decreased. Then, naïve animals were trained in modified versions of the task designed to prevent the development of this routine. Compared to rats trained in the first protocol, these animals didn’t reach a comparable level of timing accuracy. Altogether, our results indicate that timing accuracy in rats is improved when the environment affords cues that animals can incorporate into motor routines.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Safaie, Mostafa and Jurado-Parras, Maria-Teresa and Sarno, Stefania and Louis, Jordane and Karoutchi, Corane and Petit, Ludovic F. and Pasquet, Matthieu O. and Eloy, Christophe and Robbe, David},
	year = {2020},
	pages = {13084--13093},
}

@article{schneidman_weak_2006,
	title = {Weak pairwise correlations imply strongly correlated network states in a neural population},
	volume = {440},
	url = {https://doi.org/b9ph32},
	doi = {10.1038/nature04701},
	language = {en},
	number = {7087},
	journal = {Nature},
	author = {Schneidman, Elad and Berry, Michael J. and Segev, Ronen and Bialek, William},
	year = {2006},
	pages = {1007--1012},
}

@article{stella_3d-spade_2019,
	title = {3d-{SPADE}: {Significance} evaluation of spatio-temporal patterns of various temporal extents},
	volume = {185},
	url = {https://doi.org/gpshj2},
	doi = {10.1016/j.biosystems.2019.104022},
	language = {en},
	journal = {Biosystems},
	author = {Stella, Alessandra and Quaglio, Pietro and Torre, Emiliano and Grün, Sonja},
	month = nov,
	year = {2019},
	pages = {104022},
}

@article{roelfsema_visuomotor_1997,
	title = {Visuomotor integration is associated with zero time-lag synchronization among cortical areas},
	volume = {385},
	url = {https://doi.org/dp8q5v},
	doi = {10.1038/385157a0},
	language = {en},
	number = {6612},
	journal = {Nature},
	author = {Roelfsema, Pieter R. and Engel, Andreas K. and König, Peter and Singer, Wolf},
	month = jan,
	year = {1997},
	pages = {157--161},
}

@article{pfeil_six_2013,
	title = {Six {Networks} on a {Universal} {Neuromorphic} {Computing} {Substrate}},
	volume = {7},
	url = {https://doi.org/gh4jg3},
	doi = {10.3389/fnins.2013.00011},
	journal = {Frontiers in Neuroscience},
	author = {Pfeil, Thomas and Grübl, Andreas and Jeltsch, Sebastian and Müller, Eric and Müller, Paul and Petrovici, Mihai A. and Schmuker, Michael and Brüderle, Daniel and Schemmel, Johannes and Meier, Karlheinz},
	year = {2013},
}

@article{chase_first-spike_2007,
	title = {First-spike latency information in single neurons increases when referenced to population onset},
	volume = {104},
	url = {https://doi.org/cm3b98},
	doi = {10.1073/pnas.0610368104},
	abstract = {{\textless}jats:p{\textgreater}It is well known that many stimulus parameters, such as sound location in the auditory system or contrast in the visual system, can modulate the timing of the first spike in sensory neurons. Could first-spike latency be a candidate neural code? Most studies measuring first-spike latency information assume that the brain has an independent reference for stimulus onset from which to extract latency. This assumption creates an obvious confound that casts doubt on the feasibility of first-spike latency codes. If latency is measured relative to an internal reference of stimulus onset calculated from the responses of the neural population, the information conveyed by the latency of single neurons might decrease because of correlated changes in latency across the population. Here we assess the effects of a realistic model of stimulus onset detection on the first-spike latency information conveyed by single neurons in the auditory system. Contrary to expectation, we find that on average, the information contained in single neurons does not decrease; in fact, the majority of neurons show a slight increase in the information conveyed by latency referenced to a population onset. Our results show that first-spike latency codes are a feasible mechanism for information transfer even when biologically plausible estimates of stimulus onset are taken into account.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chase, Steven M. and Young, Eric D.},
	month = mar,
	year = {2007},
	pages = {5175--5180},
}

@incollection{nowak_timing_1997,
	title = {The {Timing} of {Information} {Transfer} in the {Visual} {System}},
	url = {https://doi.org/gpb33s},
	booktitle = {Extrastriate {Cortex} in {Primates}},
	publisher = {Springer US},
	author = {Nowak, Lionel G. and Bullier, Jean},
	year = {1997},
	pages = {205--241},
}

@article{kerr_delay_2013,
	title = {Delay {Selection} by {Spike}-{Timing}-{Dependent} {Plasticity} in {Recurrent} {Networks} of {Spiking} {Neurons} {Receiving} {Oscillatory} {Inputs}},
	volume = {9},
	url = {https://doi.org/f4rm5j},
	doi = {10.1371/journal.pcbi.1002897},
	language = {en},
	number = {2},
	journal = {PLoS Computational Biology},
	author = {Kerr, Robert R. and Burkitt, Anthony N. and Thomas, Doreen A. and Gilson, Matthieu and Grayden, David B.},
	editor = {Morrison, Abigail},
	year = {2013},
	pages = {e1002897},
}

@article{quaglio_methods_2018,
	title = {Methods for identification of spike patterns in massively parallel spike trains},
	volume = {112},
	url = {https://doi.org/gdgckg},
	doi = {10.1007/s00422-018-0755-0},
	language = {en},
	number = {1-2},
	journal = {Biological Cybernetics},
	author = {Quaglio, Pietro and Rostami, Vahid and Torre, Emiliano and Grün, Sonja},
	year = {2018},
	pages = {57--80},
}

@article{perrinet_networks_2001,
	title = {Networks of integrate-and-fire neuron using rank order coding {A}: {How} to implement spike time dependent {Hebbian} plasticity},
	volume = {38-40},
	url = {https://doi.org/d5p6b2},
	doi = {10.1016/s0925-2312(01)00460-x},
	language = {en},
	journal = {Neurocomputing},
	author = {Perrinet, L. and Delorme, A. and Samuelides, M. and Thorpe, S.J.},
	year = {2001},
	pages = {817--822},
}

@article{johansson_first_2004,
	title = {First spikes in ensembles of human tactile afferents code complex spatial fingertip events},
	volume = {7},
	url = {https://doi.org/dqstpm},
	doi = {10.1038/nn1177},
	language = {en},
	number = {2},
	journal = {Nature Neuroscience},
	author = {Johansson, Roland S and Birznieks, Ingvars},
	month = jan,
	year = {2004},
	pages = {170--177},
}

@article{rucci_temporal_2018,
	title = {Temporal {Coding} of {Visual} {Space}},
	volume = {22},
	url = {https://doi.org/gfcskd},
	doi = {10.1016/j.tics.2018.07.009},
	language = {en},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Rucci, Michele and Ahissar, Ehud and Burr, David},
	month = oct,
	year = {2018},
	pages = {883--895},
}

@article{grun_unitary_2002,
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {I}. {Detection} and {Significance}},
	volume = {14},
	url = {https://doi.org/c63kkn},
	doi = {10.1162/089976602753284455},
	abstract = {{\textless}jats:p{\textgreater} It has been proposed that cortical neurons organize dynamically into functional groups (cell assemblies) by the temporal structure of their joint spiking activity. Here, we describe a novel method to detect conspicuous patterns of coincident joint spike activity among simultaneously recorded single neurons. The statistical significance of these unitary events of coincident joint spike activity is evaluated by the joint-surprise. The method is tested and calibrated on the basis of simulated, stationary spike trains of independently firing neurons, into which coincident joint spike events were inserted under controlled conditions. The sensitivity and specificity of the method are investigated for their dependence on physiological parameters (firing rate, coincidence precision, coincidence pattern complexity) and temporal resolution of the analysis. In the companion article in this issue, we describe an extension of the method, designed to deal with nonstationary firing rates. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Neural Computation},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	month = jan,
	year = {2002},
	pages = {43--80},
}

@article{nowak_influence_1997,
	title = {Influence of low and high frequency inputs on spike timing in visual cortical neurons},
	volume = {7},
	url = {https://doi.org/fvjpx7},
	doi = {10.1093/cercor/7.6.487},
	number = {6},
	journal = {Cerebral Cortex},
	author = {Nowak, L.},
	month = sep,
	year = {1997},
	pages = {487--501},
}

@article{grun_unitary_2002-1,
	title = {Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking} {Activity}: {II}. {Nonstationary} {Data}},
	volume = {14},
	url = {https://doi.org/ffvbkp},
	doi = {10.1162/089976602753284464},
	abstract = {{\textless}jats:p{\textgreater} In order to detect members of a functional group (cell assembly) in simultaneously recorded neuronal spiking activity, we adopted the widely used operational definition that membership in a common assembly is expressed in near-simultaneous spike activity. Unitary event analysis, a statistical method to detect the significant occurrence of coincident spiking activity in stationary data, was recently developed (see the companion article in this issue). The technique for the detection of unitary events is based on the assumption that the underlying processes are stationary in time. This requirement, however, is usually not fulfilled in neuronal data. Here we describe a method that properly normalizes for changes of rate: the unitary events by moving window analysis (UEMWA). Analysis for unitary events is performed separately in overlapping time segments by sliding a window of constant width along the data. In each window, stationarity is assumed. Performance and sensitivity are demonstrated by use of simulated spike trains of independently firing neurons, into which coincident events are inserted. If cortical neurons organize dynamically into functional groups, the occurrence of near-simultaneous spike activity should be time varying and related to behavior and stimuli. UEMWA also accounts for these potentially interesting nonstationarities and allows locating them in time. The potential of the new method is illustrated by results from multiple single-unit recordings from frontal and motor cortical areas in awake, behaving monkey. {\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Neural Computation},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	month = jan,
	year = {2002},
	pages = {81--119},
}

@incollection{grun_unitary_2010,
	title = {Unitary {Event} {Analysis}},
	url = {https://doi.org/dxgxt9},
	booktitle = {Analysis of {Parallel} {Spike} {Trains}},
	publisher = {Springer US},
	author = {Grün, Sonja and Diesmann, Markus and Aertsen, Ad},
	year = {2010},
	pages = {191--220},
}

@article{burkitt_predictive_2021,
	title = {Predictive {Visual} {Motion} {Extrapolation} {Emerges} {Spontaneously} and without {Supervision} at {Each} {Layer} of a {Hierarchical} {Neural} {Network} with {Spike}-{Timing}-{Dependent} {Plasticity}},
	volume = {41},
	url = {https://doi.org/gjtwzk},
	doi = {10.1523/jneurosci.2017-20.2021},
	language = {en},
	number = {20},
	journal = {The Journal of Neuroscience},
	author = {Burkitt, Anthony N. and Hogendoorn, Hinze},
	year = {2021},
	pages = {4428--4438},
}

@article{kenyon_theory_2004,
	title = {A theory of the {Benham} {Top} based on center–surround interactions in the parvocellular pathway},
	volume = {17},
	url = {https://doi.org/bjwzzt},
	doi = {10.1016/j.neunet.2004.05.005},
	language = {en},
	number = {5-6},
	journal = {Neural Networks},
	author = {Kenyon, Garrett T and Hill, Dan and Theiler, James and George, John S and Marshak, David W},
	year = {2004},
	pages = {773--786},
}

@article{buzsaki_space_2018,
	title = {Space and {Time}: {The} {Hippocampus} as a {Sequence} {Generator}},
	volume = {22},
	url = {https://doi.org/gfcr76},
	doi = {10.1016/j.tics.2018.07.006},
	language = {en},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Buzsáki, György and Tingley, David},
	month = oct,
	year = {2018},
	pages = {853--869},
}

@article{kremkow_push-pull_2016,
	title = {Push-{Pull} {Receptive} {Field} {Organization} and {Synaptic} {Depression}: {Mechanisms} for {Reliably} {Encoding} {Naturalistic} {Stimuli} in {V1}},
	volume = {10},
	url = {https://doi.org/ggkdkh},
	doi = {10.3389/fncir.2016.00037},
	journal = {Frontiers in Neural Circuits},
	author = {Kremkow, Jens and Perrinet, Laurent U. and Monier, Cyril and Alonso, Jose-Manuel and Aertsen, Ad and Frégnac, Yves and Masson, Guillaume S.},
	year = {2016},
}

@article{denker_lfp_2018,
	title = {{LFP} beta amplitude is linked to mesoscopic spatio-temporal phase patterns},
	volume = {8},
	url = {https://doi.org/gc9xx6},
	doi = {10.1038/s41598-018-22990-7},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Denker, Michael and Zehl, Lyuba and Kilavik, Bjørg E. and Diesmann, Markus and Brochier, Thomas and Riehle, Alexa and Grün, Sonja},
	month = mar,
	year = {2018},
}

@article{keysers_speed_2001,
	title = {The {Speed} of {Sight}},
	volume = {13},
	url = {https://doi.org/cfdjtg},
	doi = {10.1162/089892901564199},
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}
               {\textless}jats:p{\textgreater}Macaque monkeys were presented with continuous rapid serial visual presentation (RSVP) sequences of unrelated naturalistic images at rates of 14-222 msec/image, while neurons that responded selectively to complex patterns (e.g., faces) were recorded in temporal cortex. Stimulus selectivity was preserved for 65\% of these neurons even at surprisingly fast presentation rates (14 msec/image or 72 images/sec). Five human subjects were asked to detect or remember images under equivalent conditions. Their performance in both tasks was above chance at all rates (14-111 msec/image). The performance of single neurons was comparable to that of humans and responded in a similar way to changes in presentation rate. The implications for the role of temporal cortex cells in perception are discussed.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {1},
	journal = {Journal of Cognitive Neuroscience},
	author = {Keysers, C. and Xiao, D.-K. and Földiák, P. and Perrett, D. I.},
	month = jan,
	year = {2001},
	pages = {90--101},
}

@article{pachitariu_robustness_2018,
	title = {Robustness of {Spike} {Deconvolution} for {Neuronal} {Calcium} {Imaging}},
	volume = {38},
	url = {https://doi.org/gd9mcx},
	doi = {10.1523/jneurosci.3339-17.2018},
	language = {en},
	number = {37},
	journal = {The Journal of Neuroscience},
	author = {Pachitariu, Marius and Stringer, Carsen and Harris, Kenneth D.},
	year = {2018},
	pages = {7976--7985},
}

@article{luo_supervised_2022,
	title = {Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks} {With} {Spike} {Temporal} {Error} {Backpropagation}},
	url = {https://doi.org/gpzp26},
	doi = {10.1109/tnnls.2022.3164930},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Luo, Xiaoling and Qu, Hong and Wang, Yuchen and Yi, Zhang and Zhang, Jilun and Zhang, Malu},
	year = {2022},
	pages = {1--13},
}

@article{dahlem_dynamics_2009,
	title = {Dynamics of delay-coupled excitable neural systems},
	volume = {19},
	url = {https://doi.org/d43v5b},
	doi = {10.1142/s0218127409023111},
	abstract = {We study the nonlinear dynamics of two delay-coupled neural systems each modeled by excitable dynamics of FitzHugh–Nagumo type and demonstrate that bistability between the stable fixed point and limit cycle oscillations occurs for sufficiently large delay times τ and coupling strength C. As the mechanism for these delay-induced oscillations, we identify a saddle-node bifurcation of limit cycles.},
	language = {en},
	number = {02},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Dahlem, M. A. and HILLER, G. and PANCHUK, A. and SCHÖLL, E.},
	year = {2009},
	pages = {745--753},
}

@article{lamme_distinct_2000,
	title = {The distinct modes of vision offered by feedforward and recurrent processing},
	volume = {23},
	url = {https://doi.org/ccv3w2},
	doi = {10.1016/s0166-2236(00)01657-x},
	language = {en},
	number = {11},
	journal = {Trends in Neurosciences},
	author = {Lamme, Victor A.F. and Roelfsema, Pieter R.},
	month = nov,
	year = {2000},
	pages = {571--579},
}

@article{luczak_packet-based_2015,
	title = {Packet-based communication in the cortex.},
	volume = {16},
	issn = {1471-0048},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/26507295},
	doi = {10.1038/nrn4026},
	abstract = {Cortical circuits work through the generation of coordinated, large-scale activity patterns. In sensory systems, the onset of a discrete stimulus usually evokes a temporally organized packet of population activity lasting ∼50-200 ms. The structure of these packets is partially stereotypical, and variation in the exact timing and number of spikes within a packet conveys information about the identity of the stimulus. Similar packets also occur during ongoing stimuli and spontaneously. We suggest that such packets constitute the basic building blocks of cortical coding.},
	number = {12},
	journal = {Nature reviews. Neuroscience},
	author = {Luczak, Artur and McNaughton, Bruce L and Harris, Kenneth D},
	month = oct,
	year = {2015},
	pages = {745--55},
}

@article{markram_regulation_1997,
	title = {Regulation of {Synaptic} {Efficacy} by {Coincidence} of {Postsynaptic} {APs} and {EPSPs}},
	volume = {275},
	url = {https://doi.org/ftvvd8},
	doi = {10.1126/science.275.5297.213},
	abstract = {{\textless}jats:p{\textgreater}Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5297},
	journal = {Science},
	author = {Markram, Henry and Lübke, Joachim and Frotscher, Michael and Sakmann, Bert},
	month = jan,
	year = {1997},
	pages = {213--215},
}

@article{bruno_cortex_2006,
	title = {Cortex {Is} {Driven} by {Weak} but {Synchronously} {Active} {Thalamocortical} {Synapses}},
	volume = {312},
	url = {https://doi.org/c5575v},
	doi = {10.1126/science.1124593},
	abstract = {{\textless}jats:p{\textgreater}Sensory stimuli reach the brain via the thalamocortical projection, a group of axons thought to be among the most powerful in the neocortex. Surprisingly, these axons account for only ∼15\% of synapses onto cortical neurons. The thalamocortical pathway might thus achieve its effectiveness via high-efficacy thalamocortical synapses or via amplification within cortical layer 4. In rat somatosensory cortex, we measured in vivo the excitatory postsynaptic potential evoked by a single synaptic connection and found that thalamocortical synapses have low efficacy. Convergent inputs, however, are both numerous and synchronous, and intracortical amplification is not required. Our results suggest a mechanism of cortical activation by which thalamic input alone can drive cortex.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5780},
	journal = {Science},
	author = {Bruno, Randy M. and Sakmann, Bert},
	year = {2006},
	pages = {1622--1627},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	url = {https://doi.org/gc6dqh},
	doi = {10.1016/j.neunet.2017.12.005},
	language = {en},
	journal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	month = mar,
	year = {2018},
	pages = {56--67},
}

@article{gollisch_rapid_2008,
	title = {Rapid {Neural} {Coding} in the {Retina} with {Relative} {Spike} {Latencies}},
	volume = {319},
	url = {https://doi.org/c6czvj},
	doi = {10.1126/science.1149639},
	abstract = {{\textless}jats:p{\textgreater}Natural vision is a highly dynamic process. Frequent body, head, and eye movements constantly bring new images onto the retina for brief periods, challenging our understanding of the neural code for vision. We report that certain retinal ganglion cells encode the spatial structure of a briefly presented image in the relative timing of their first spikes. This code is found to be largely invariant to stimulus contrast and robust to noisy fluctuations in response latencies. Mechanistically, the observed response characteristics result from different kinetics in two retinal pathways (“ON” and “OFF”) that converge onto ganglion cells. This mechanism allows the retina to rapidly and reliably transmit new spatial information with the very first spikes emitted by a neural population.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5866},
	journal = {Science},
	author = {Gollisch, Tim and Meister, Markus},
	year = {2008},
	pages = {1108--1111},
}

@article{kremkow_functional_2010,
	title = {Functional consequences of correlated excitatory and inhibitory conductances in cortical networks},
	volume = {28},
	url = {https://doi.org/c3wrbn},
	doi = {10.1007/s10827-010-0240-9},
	language = {en},
	number = {3},
	journal = {Journal of Computational Neuroscience},
	author = {Kremkow, Jens and Perrinet, Laurent U. and Masson, Guillaume S. and Aertsen, Ad},
	year = {2010},
	pages = {579--594},
}

@article{kilavik_long-term_2009,
	title = {Long-{Term} {Modifications} in {Motor} {Cortical} {Dynamics} {Induced} by {Intensive} {Practice}},
	volume = {29},
	url = {https://doi.org/bf84ps},
	doi = {10.1523/jneurosci.1554-09.2009},
	language = {en},
	number = {40},
	journal = {Journal of Neuroscience},
	author = {Kilavik, B. E. and Roux, S. and Ponce-Alvarez, A. and Confais, J. and Grun, S. and Riehle, A.},
	month = oct,
	year = {2009},
	pages = {12653--12663},
}

@article{grammont_precise_1999,
	title = {Precise spike synchronization in monkey motor cortex involved in preparation for movement},
	volume = {128},
	url = {https://doi.org/b67khx},
	doi = {10.1007/s002210050826},
	number = {1-2},
	journal = {Experimental Brain Research},
	author = {Grammont, Franck and Riehle, Alexa},
	month = sep,
	year = {1999},
	pages = {118--122},
}

@article{gilson_stdp_2010,
	title = {{STDP} in recurrent neuronal networks},
	volume = {4},
	url = {https://doi.org/c8ck59},
	doi = {10.3389/fncom.2010.00023},
	journal = {Frontiers in Computational Neuroscience},
	author = {Gilson, Matthieu},
	year = {2010},
}

@article{celebrini_dynamics_1993,
	title = {Dynamics of orientation coding in area {V1} of the awake primate},
	volume = {10},
	url = {https://doi.org/dqt5cm},
	doi = {10.1017/s0952523800006052},
	abstract = {{\textless}jats:title{\textgreater}Abstract{\textless}/jats:title{\textgreater}{\textless}jats:p{\textgreater}To investigate the importance of feedback loops in visual information processing, we have analyzed the dynamic aspects of neuronal responses to oriented gratings in cortical area V1 of the awake primate. If recurrent feedback is important in generating orientation selectivity, the initial part of the neuronal response should be relatively poorly selective, and full orientation selectivity should only appear after a delay. Thus, by examining the dynamics of the neuronal responses it should be possible to assess the importance of feedback processes in the development of orientation selectivity. The results were base on a sample of 259 cells recorded in two monkeys, of which 89\% were visually responsive. Of these, approximately two-thirds were orientation selective. Response latency varied considerably between neurons, ranging from a minimum of 41 ms to over 150 ms, although most had latencies of 50–70 ms. Orientation tuning (defined as the bandwidth at half-height) ranged from 16 deg to over 90 deg, with a mean value of around 55 deg. By examining the selectivity of these different neurons by 10-ms time slices, starting at the onset of the neuronal response, we found that the orientation selectivity of virtually every neuron was fully developed at the very start of the neuronal response. Indeed, many neurons showed a marked tendency to respond at somewhat longer latencies to stimuli that were nonoptimally oriented, with the result that orientation selectivity was highest at the very start of the neuronal response. Furthermore, there was no evidence that the neurons with the shortest onset latencies were less selective. Such evidence is inconsistent with the hypothesis that recurrent intracortical feedback plays an important role in the generation of orientation selectivity. Instead, we suggest that orientation selectivity is primarily generated using feedforward mechanisms, including feedforward inhibition. Such a strategy has the advantage of allowing orientation to be computed rapidly, and avoids the initially poorly selective neuronal responses that characterize processing involving recurrent loops.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {5},
	journal = {Visual Neuroscience},
	author = {Celebrini, Simona and Thorpe, Simon and Trotter, Yves and Imbert, Michel},
	month = sep,
	year = {1993},
	pages = {811--825},
}

@inproceedings{grimaldi_homeostatic_2021,
	title = {A homeostatic gain control mechanism to improve event-driven object recognition},
	url = {https://doi.org/gkzcrv},
	doi = {10.1109/cbmi50038.2021.9461901},
	booktitle = {2021 {International} {Conference} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	publisher = {IEEE},
	author = {Grimaldi, Antoine and Boutin, Victor and Perrinet, Laurent and Ieng, Sio-Hoi and Benosman, Ryad},
	year = {2021},
}

@article{davison_pynn_2008,
	title = {{PyNN}: a common interface for neuronal network simulators},
	volume = {2},
	url = {https://doi.org/fh8h6j},
	doi = {10.3389/neuro.11.011.2008},
	journal = {Frontiers in Neuroinformatics},
	author = {Davison, Andrew P},
	year = {2008},
}

@article{duffy_variation_2019,
	title = {Variation in sequence dynamics improves maintenance of stereotyped behavior in an example from bird song},
	volume = {116},
	url = {https://doi.org/gpfjm6},
	doi = {10.1073/pnas.1815910116},
	abstract = {{\textless}jats:title{\textgreater}Significance{\textless}/jats:title{\textgreater}
          {\textless}jats:p{\textgreater}In this work, we show a way by which the nervous system maintains precise, stereotyped behavior in the face of environmental and neural changes. Through a model of bird song learning, we show how instability in neural representation of stable behavior can allow a system to more readily adapt and maintain performance with minimal cost. In this perspective, behaviors are made more robust to environmental change by continually seeking subtly new ways of performing the same task. Thus, one should expect to find variability in neural systems executing stereotyped behaviors, and this variability can serve a constructive role in maintaining skilled behavior.{\textless}/jats:p{\textgreater}},
	language = {en},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Duffy, Alison and Abe, Elliott and Perkel, David J. and Fairhall, Adrienne L.},
	year = {2019},
	pages = {9592--9597},
}

@article{gutig_tempotron_2006,
	title = {The tempotron: a neuron that learns spike timing–based decisions},
	volume = {9},
	url = {https://doi.org/ch29r4},
	doi = {10.1038/nn1643},
	language = {en},
	number = {3},
	journal = {Nature Neuroscience},
	author = {Gütig, Robert and Sompolinsky, Haim},
	year = {2006},
	pages = {420--428},
}

@techreport{goltz_fast_2021,
	title = {Fast and energy-efficient neuromorphic deep learning with first-spike times},
	url = {https://arxiv.org/abs/1912.11443},
	abstract = {For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems are optimized for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. With time-to-first-spike coding both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of a learning rule for such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how this mechanism can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the system's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.},
	number = {1912.11443},
	institution = {arXiv},
	author = {Göltz, Julian and Kriener, Laura and Baumbach, Andreas and Billaudelle, Sebastian and Breitwieser, Oliver and Cramer, Benjamin and Dold, Dominik and Kungl, Akos Ferenc and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai Alexandru},
	year = {2021},
}

@book{deweese_binary_2003,
	title = {Binary coding in auditory cortex},
	url = {http://papers.nips.cc/paper/2342-binary-coding-in-auditory-cortex},
	abstract = {Cortical neurons have been reported to use both rate and temporal codes. Here we describe a novel mode in which each neuron generates exactly 0 or 1 action potentials, but not more, in response to a stimulus. We used cell-attached recording, which ensured single-unit isolation, to record responses in rat auditory cortex to brief tone pips. Surprisingly, the majority of neurons exhibited binary behavior with few multi-spike responses; several dramatic examples consisted of exactly one spike on 100\% of trials, with no trial-to-trial variability in spike count. Many neurons were tuned to stimulus frequency. Since individual trials yielded at most one spike for most neurons, the information about stimulus frequency was encoded in the population, and would not have been accessible to later stages of processing that only had access to the activity of a single unit. These binary units allow a more efficient population code than is possible with conventional rate coding units, and are consistent with a model of cortical processing in which synchronous packets of spikes propagate stably from one neuronal population to the next.},
	urldate = {2022-10-04},
	publisher = {Neural Information Processing Systems Foundation},
	author = {DeWeese, M. R. and Zador, A. M.},
	year = {2003},
}

@article{berens_fast_2012,
	title = {A {Fast} and {Simple} {Population} {Code} for {Orientation} in {Primate} {V1}},
	volume = {32},
	url = {https://doi.org/f365rn},
	doi = {10.1523/jneurosci.1335-12.2012},
	language = {en},
	number = {31},
	journal = {Journal of Neuroscience},
	author = {Berens, P. and Ecker, A. S. and Cotton, R. J. and Ma, W. J. and Bethge, M. and Tolias, A. S.},
	year = {2012},
	pages = {10618--10626},
}

@article{azouz_stimulus-selective_2008,
	title = {Stimulus-selective spiking is driven by the relative timing of synchronous excitation and disinhibition in cat striate neurons\textit{in vivo}},
	volume = {28},
	url = {https://doi.org/cbcr8h},
	doi = {10.1111/j.1460-9568.2008.06434.x},
	language = {en},
	number = {7},
	journal = {European Journal of Neuroscience},
	author = {Azouz, Rony and Gray, Charles M.},
	month = oct,
	year = {2008},
	pages = {1286--1300},
}

@article{agus_rapid_2010,
	title = {Rapid {Formation} of {Robust} {Auditory} {Memories}: {Insights} from {Noise}},
	volume = {66},
	url = {https://doi.org/dc3r2d},
	doi = {10.1016/j.neuron.2010.04.014},
	language = {en},
	number = {4},
	journal = {Neuron},
	author = {Agus, Trevor R. and Thorpe, Simon J. and Pressnitzer, Daniel},
	year = {2010},
	pages = {610--618},
}

@book{abeles_corticonics_1991,
	address = {Cambridge ; New York},
	title = {Corticonics: neural circuits of the cerebral cortex},
	isbn = {978-0-521-37476-7},
	shorttitle = {Corticonics},
	publisher = {Cambridge University Press},
	author = {Abeles, Moshe},
	year = {1991},
}

@article{sotomayor-gomez_spikeship_2021,
	title = {{SpikeShip}: {A} method for fast, unsupervised discovery of high-dimensional neural spiking patterns},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.03.131573},
	doi = {10.1101/2020.06.03.131573},
	journal = {bioRxiv : the preprint server for biology},
	author = {Sotomayor-Gómez, Boris and Battaglia, Francesco P and Vinck, Martin},
	year = {2021},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2020--06},
}

@misc{yu_stsc-snn_2022,
	title = {{STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with {Temporal} {Convolution} and {Attention} for {Spiking} {Neural} {Networks}},
	shorttitle = {{STSC}-{SNN}},
	url = {http://arxiv.org/abs/2210.05241},
	abstract = {Spiking Neural Networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing the event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a spatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Concretely, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance vias varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05241 [cs, q-bio, stat]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{vanrullen_continuous_2006,
	title = {The {Continuous} {Wagon} {Wheel} {Illusion} {Is} {Associated} with {Changes} in {Electroencephalogram} {Power} at 13 {Hz}},
	volume = {26},
	copyright = {Copyright © 2006 Society for Neuroscience 0270-6474/06/26502-06.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/26/2/502},
	doi = {10.1523/jneurosci.4654-05.2006},
	language = {en},
	number = {2},
	urldate = {2019-11-05},
	journal = {Journal of Neuroscience},
	author = {VanRullen, Rufin and Reddy, Leila and Koch, Christof},
	month = jan,
	year = {2006},
	pmid = {16407547},
	note = {00000 },
	pages = {502--507},
}

@article{ghosh_synchronized_2022,
	title = {The synchronized dynamics of time-varying networks},
	volume = {949},
	issn = {03701573},
	url = {http://arxiv.org/abs/2109.07618},
	doi = {10.1016/j.physrep.2021.10.006},
	abstract = {Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in term of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in details. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.},
	urldate = {2022-10-17},
	journal = {Physics Reports},
	author = {Ghosh, Dibakar and Frasca, Mattia and Rizzo, Alessandro and Majhi, Soumen and Rakshit, Sarbendu and Alfaro-Bittner, Karin and Boccaletti, Stefano},
	month = feb,
	year = {2022},
	note = {arXiv:2109.07618 [physics]},
	keywords = {Physics - Physics and Society},
	pages = {1--63},
}

@article{lee_combinatorial_2004,
	title = {A {Combinatorial} {Method} for {Analyzing} {Sequential} {Firing} {Patterns} {Involving} an {Arbitrary} {Number} of {Neurons} {Based} on {Relative} {Time} {Order}},
	volume = {92},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.01030.2003},
	doi = {10.1152/jn.01030.2003},
	abstract = {Information processing in the brain is believed to require coordinated activity across many neurons. With the recent development of techniques for simultaneously recording the spiking activity of large numbers of individual neurons, the search for complex multicell firing patterns that could help reveal this neural code has become possible. Here we develop a new approach for analyzing sequential firing patterns involving an arbitrary number of neurons based on relative firing order. Specifically, we develop a combinatorial method for quantifying the degree of matching between a “reference sequence” of N distinct “letters” (representing a particular target order of firing by N cells) and an arbitrarily long “word” composed of any subset of those letters including repeats (representing the relative time order of spikes in an arbitrary firing pattern). The method involves computing the probability that a random permutation of the word's letters would by chance alone match the reference sequence as well as or better than the actual word does, assuming all permutations were equally likely. Lower probabilities thus indicate better matching. The overall degree and statistical significance of sequence matching across a heterogeneous set of words (such as those produced during the course of an experiment) can be computed from the corresponding set of probabilities. This approach can reduce the sample size problem associated with analyzing complex firing patterns. The approach is general and thus applicable to other types of neural data beyond multiple spike trains, such as EEG events or imaging signals from multiple locations. We have recently applied this method to quantify memory traces of sequential experience in the rodent hippocampus during slow wave sleep.},
	language = {en},
	number = {4},
	urldate = {2022-10-17},
	journal = {Journal of Neurophysiology},
	author = {Lee, Albert K. and Wilson, Matthew A.},
	month = oct,
	year = {2004},
	pages = {2555--2573},
}

@article{nadasdy_replay_1999,
	title = {Replay and {Time} {Compression} of {Recurring} {Spike} {Sequences} in the {Hippocampus}},
	volume = {19},
	copyright = {Copyright © 1999 Society for Neuroscience},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/19/21/9497},
	doi = {10.1523/JNEUROSCI.19-21-09497.1999},
	abstract = {Information in neuronal networks may be represented by the spatiotemporal patterns of spikes. Here we examined the temporal coordination of pyramidal cell spikes in the rat hippocampus during slow-wave sleep. In addition, rats were trained to run in a defined position in space (running wheel) to activate a selected group of pyramidal cells. A template-matching method and a joint probability map method were used for sequence search. Repeating spike sequences in excess of chance occurrence were examined by comparing the number of repeating sequences in the original spike trains and in surrogate trains after Monte Carlo shuffling of the spikes. Four different shuffling procedures were used to control for the population dynamics of hippocampal neurons. Repeating spike sequences in the recorded cell assemblies were present in both the awake and sleeping animal in excess of what might be predicted by random variations. Spike sequences observed during wheel running were “replayed” at a faster timescale during single sharp-wave bursts of slow-wave sleep. We hypothesize that the endogenously expressed spike sequences during sleep reflect reactivation of the circuitry modified by previous experience. Reactivation of acquired sequences may serve to consolidate information.},
	language = {en},
	number = {21},
	urldate = {2022-10-17},
	journal = {Journal of Neuroscience},
	author = {Nádasdy, Zoltán and Hirase, Hajime and Czurkó, András and Csicsvari, Jozsef and Buzsáki, György},
	month = nov,
	year = {1999},
	pmid = {10531452},
	note = {Publisher: Society for Neuroscience
Section: ARTICLE},
	keywords = {coding, decoding, memory, network, retrieval, sharp waves, sleep, θ},
	pages = {9497--9507},
}

@article{aronov_non-euclidean_2004,
	title = {Non-{Euclidean} properties of spike train metric spaces},
	volume = {69},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.69.061905},
	doi = {10.1103/PhysRevE.69.061905},
	abstract = {Quantifying the dissimilarity (or distance) between two sequences is essential to the study of action potential (spike) trains in neuroscience and genetic sequences in molecular biology. In neuroscience, traditional methods for sequence comparisons rely on techniques appropriate for multivariate data, which typically assume that the space of sequences is intrinsically Euclidean. More recently, metrics that do not make this assumption have been introduced for comparison of neural activity patterns. These metrics have a formal resemblance to those used in the comparison of genetic sequences. Yet the relationship between such metrics and the traditional Euclidean distances has remained unclear. We show, both analytically and computationally, that the geometries associated with metric spaces of event sequences are intrinsically non-Euclidean. Our results demonstrate that metric spaces enrich the study of neural activity patterns, since accounting for perceptual spaces requires a non-Euclidean geometry.},
	number = {6},
	urldate = {2022-10-17},
	journal = {Physical Review E},
	author = {Aronov, Dmitriy and Victor, Jonathan D.},
	month = jun,
	year = {2004},
	note = {Publisher: American Physical Society},
	pages = {061905},
}

@article{victor_nature_1996,
	title = {Nature and precision of temporal coding in visual cortex: a metric-space analysis},
	volume = {76},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Nature and precision of temporal coding in visual cortex},
	url = {https://www.physiology.org/doi/10.1152/jn.1996.76.2.1310},
	doi = {10.1152/jn.1996.76.2.1310},
	abstract = {1. We recorded single-unit and multi-unit activity in response to transient presentation of texture and grating patterns at 25 sites within the parafoveal representation of V1, V2, and V3 of two awake monkeys trained to perform a fixation task. In grating experiments, stimuli varied in orientation, spatial frequency, or both. In texture experiments, stimuli varied in contrast, check size, texture type, or pairs of these attributes. 2. To examine the nature and precision of temporal coding, we compared individual responses elicited by each set of stimuli in terms of two families of metrics. One family of metrics, D(spike), was sensitive to the absolute spike time (following stimulus onset). The second family of metrics, D(interval), was sensitive to the pattern of interspike intervals. In each family, the metrics depend on a parameter q, which expresses the precision of temporal coding. For q = 0, both metrics collapse into the "spike count" metric D(Count), which is sensitive to the number of impulses but insensitive to their position in time. 3. Each of these metrics, with values of q ranging from 0 to 512/s, was used to calculate the distance between all pairs of spike trains within each dataset. The extent of stimulus-specific clustering manifest in these pairwise distances was quantified by an information measure. Chance clustering was estimated by applying the same procedure to synthetic data sets in which responses were assigned randomly to the input stimuli. 4. Of the 352 data sets, 170 showed evidence of tuning via the spike count (q = 0) metric, 294 showed evidence of tuning via the spike time metric, 272 showed evidence of tuning via the spike interval metric to the stimulus attribute (contrast, check size, orientation, spatial frequency, or texture type) under study. Across the entire dataset, the information not attributable to chance clustering averaged 0.042 bits for the spike count metric, 0.171 bits for the optimal spike time metric, and 0.107 bits for the optimal spike interval metric. 5. The reciprocal of the optimal cost q serves as a measure of the temporal precision of temporal coding. In V1 and V2, with both metrics, temporal precision was highest for contrast (ca. 10-30 ms) and lowest for texture type (ca. 100 ms). This systematic dependence of q on stimulus attribute provides a possible mechanism for the simultaneous representation of multiple stimulus attributes in one spike train. 6. Our findings are inconsistent with Poisson models of spike trains. Synthetic data sets in which firing rate was governed by a time-dependent Poisson process matched to the observed poststimulus time histogram (PSTH) overestimated clustering induced by D(count) and, for low values of q, D(spike)[q] and D(intervals)[q]. Synthetic data sets constructed from a modified Poisson process, which preserved not only the PSTH but also spike count statistics accounted for the clustering induced by D(count) but underestimated the clustering induced by D(spike)[q] and D(interval)[q].},
	language = {en},
	number = {2},
	urldate = {2022-10-17},
	journal = {Journal of Neurophysiology},
	author = {Victor, J. D. and Purpura, K. P.},
	month = aug,
	year = {1996},
	pages = {1310--1326},
}

@article{van_rossum_novel_2001,
	title = {A novel spike distance},
	volume = {13},
	issn = {0899-7667},
	doi = {10.1162/089976601300014321},
	abstract = {The discrimination between two spike trains is a fundamental problem for both experimentalists and the nervous system itself. We introduce a measure for the distance between two spike trains. The distance has a time constant as a parameter. Depending on this parameter, the distance interpolates between a coincidence detector and a rate difference counter. The dependence of the distance on noise is studied with an integrate-and-fire model. For an intermediate range of the time constants, the distance depends linearly on the noise. This property can be used to determine the intrinsic noise of a neuron.},
	language = {eng},
	number = {4},
	journal = {Neural Computation},
	author = {van Rossum, M. C.},
	month = apr,
	year = {2001},
	pmid = {11255567},
	keywords = {Algorithms, Evoked Potentials, Models, Neurological, Neurons, Poisson Distribution},
	pages = {751--763},
}

@article{khoei_motion-based_2013,
	title = {Motion-based prediction explains the role of tracking in motion extrapolation},
	volume = {107},
	issn = {09284257},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092842571300051X},
	doi = {10.1016/j.jphysparis.2013.08.001},
	abstract = {During normal viewing, the continuous stream of visual input is regularly interrupted, for instance by blinks of the eye. Despite these frequents blanks (that is the transient absence of a raw sensory source), the visual system is most often able to maintain a continuous representation of motion. For instance, it maintains the movement of the eye such as to stabilize the image of an object. This ability suggests the existence of a generic neural mechanism of motion extrapolation to deal with fragmented inputs. In this paper, we have modeled how the visual system may extrapolate the trajectory of an object during a blank using motion-based prediction. This implies that using a prior on the coherency of motion, the system may integrate previous motion information even in the absence of a stimulus. In order to compare with experimental results, we simulated tracking velocity responses. We found that the response of the motion integration process to a blanked trajectory pauses at the onset of the blank, but that it quickly recovers the information on the trajectory after reappearance. This is compatible with behavioral and neural observations on motion extrapolation. To understand these mechanisms, we have recorded the response of the model to a noisy stimulus. Crucially, we found that motion-based prediction acted at the global level as a gain control mechanism and that we could switch from a smooth regime to a binary tracking behavior where the dot is tracked or lost. Our results imply that a local prior implementing motion-based prediction is sufﬁcient to explain a large range of neural and behavioral results at a more global level. We show that the tracking behavior deteriorates for sensory noise levels higher than a certain value, where motion coherency and predictability fail to hold longer. In particular, we found that motion-based prediction leads to the emergence of a tracking behavior only when enough information from the trajectory has been accumulated. Then, during tracking, trajectory estimation is robust to blanks even in the presence of relatively high levels of noise. Moreover, we found that tracking is necessary for motion extrapolation, this calls for further experimental work exploring the role of noise in motion extrapolation.},
	language = {en},
	number = {5},
	urldate = {2022-10-17},
	journal = {Journal of Physiology-Paris},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	month = nov,
	year = {2013},
	pages = {409--420},
}

@article{susi_nmnsd-spiking_2021,
	title = {{nMNSD}-{A} {Spiking} {Neuron}-{Based} {Classifier} {That} {Combines} {Weight}-{Adjustment} and {Delay}-{Shift}},
	volume = {15},
	issn = {1662-4548},
	doi = {10.3389/fnins.2021.582608},
	abstract = {The recent "multi-neuronal spike sequence detector" (MNSD) architecture integrates the weight- and delay-adjustment methods by combining heterosynaptic plasticity with the neurocomputational feature spike latency, representing a new opportunity to understand the mechanisms underlying biological learning. Unfortunately, the range of problems to which this topology can be applied is limited because of the low cardinality of the parallel spike trains that it can process, and the lack of a visualization mechanism to understand its internal operation. We present here the nMNSD structure, which is a generalization of the MNSD to any number of inputs. The mathematical framework of the structure is introduced, together with the "trapezoid method," that is a reduced method to analyze the recognition mechanism operated by the nMNSD in response to a specific input parallel spike train. We apply the nMNSD to a classification problem previously faced with the classical MNSD from the same authors, showing the new possibilities the nMNSD opens, with associated improvement in classification performances. Finally, we benchmark the nMNSD on the classification of static inputs (MNIST database) obtaining state-of-the-art accuracies together with advantageous aspects in terms of time- and energy-efficiency if compared to similar classification methods.},
	language = {eng},
	journal = {Frontiers in Neuroscience},
	author = {Susi, Gianluca and Antón-Toro, Luis F. and Maestú, Fernando and Pereda, Ernesto and Mirasso, Claudio},
	year = {2021},
	pmid = {33679293},
	pmcid = {PMC7933525},
	keywords = {MNIST database, MNSD, classification, delay learning, heterosynaptic plasticity, online learning, spike latency},
	pages = {582608},
}

@article{dugue_phase_2011,
	title = {The {Phase} of {Ongoing} {Oscillations} {Mediates} the {Causal} {Relation} between {Brain} {Excitation} and {Visual} {Perception}},
	volume = {31},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1161-11.2011},
	doi = {10.1523/JNEUROSCI.1161-11.2011},
	language = {en},
	number = {33},
	urldate = {2022-10-17},
	journal = {Journal of Neuroscience},
	author = {Dugue, L. and Marque, P. and VanRullen, R.},
	month = aug,
	year = {2011},
	pages = {11889--11893},
}

@article{fries_mechanism_2005,
	title = {A mechanism for cognitive dynamics: neuronal communication through neuronal coherence},
	volume = {9},
	issn = {1364-6613},
	shorttitle = {A mechanism for cognitive dynamics},
	doi = {10.1016/j.tics.2005.08.011},
	abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility.},
	language = {eng},
	number = {10},
	journal = {Trends in Cognitive Sciences},
	author = {Fries, Pascal},
	month = oct,
	year = {2005},
	pmid = {16150631},
	keywords = {Action Potentials, Animals, Biological Clocks, Brain, Cognition, Humans, Nerve Net, Neurons, Nonlinear Dynamics, Periodicity, Pyramidal Tracts, Synaptic Transmission},
	pages = {474--480},
}

@book{hebb_organization_1949,
	address = {New York},
	title = {The organization of behavior: {A} neuropsychological theory},
	publisher = {Wiley},
	author = {Hebb, Donald O.},
	year = {1949},
	keywords = {bicv-sparse},
}

@article{benvenuti_anticipatory_2020,
	title = {Anticipatory responses along motion trajectories in awake monkey area {V1}},
	copyright = {All rights reserved},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.26.010017v1},
	doi = {10.1101/2020.03.26.010017},
	abstract = {What are the neural mechanisms underlying motion integration of translating objects? Visual motion integration is generally conceived of as a feedforward, hierarchical, information processing. However, feedforward models fail to account for many contextual effects revealed using natural moving stimuli. In particular, a translating object evokes a sequence of transient feedforward responses in the primary visual cortex but also propagations of activity through horizontal and feedback pathways. We investigated how these pathways shape the representation of a translating bar in monkey V1. We show that, for long trajectories, spiking activity builds-up hundreds of milliseconds before the bar enters the neurons receptive fields. Using VSDI and LFP recordings guided by a phenomenological model of propagation dynamics, we demonstrate that this anticipatory response arises from the interplay between horizontal and feedback networks driving V1 neurons well ahead of their feedforward inputs. This mechanism could subtend several perceptual contextual effects observed with translating objects.},
	language = {english},
	urldate = {2020-03-31},
	journal = {bioRxiv : the preprint server for biology},
	author = {Benvenuti, Giacomo and Chemla, Sandrine and Boonman, Arjan and Perrinet, Laurent U and Masson, Guillaume S and Chavane, Frederic},
	month = mar,
	year = {2020},
	pages = {2020.03.26.010017},
}

@article{le_bec_horizontal_2022,
	title = {Horizontal connectivity in {V1}: {Prediction} of coherence in contour and motion integration},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Horizontal connectivity in {V1}},
	url = {https://dx.plos.org/10.1371/journal.pone.0268351},
	doi = {10.1371/journal.pone.0268351},
	abstract = {This study demonstrates the functional importance of the Surround context relayed laterally in V1 by the horizontal connectivity, in controlling the latency and the gain of the cortical response to the feedforward visual drive. We report here four main findings: 1) a centripetal apparent motion sequence results in a shortening of the spiking latency of V1 cells, when the orientation of the local inducer and the global motion axis are both co-aligned with the RF orientation preference; 2) this contextual effects grows with visual flow speed, peaking at 150–250°/s when it matches the propagation speed of horizontal connectivity (0.15–0.25 mm/ms); 3) For this speed range, the axial sensitivity of V1 cells is tilted by 90° to become co-aligned with the orientation preference axis; 4) the strength of modulation by the surround context correlates with the spatiotemporal coherence of the apparent motion flow. Our results suggest an internally-generated binding process, linking local (orientation /position) and global (motion/direction) features as early as V1. This long-range diffusion process constitutes a plausible substrate in V1 of the human psychophysical bias in speed estimation for collinear motion. Since it is demonstrated in the anesthetized cat, this novel form of contextual control of the cortical gain and phase is a built-in property in V1, whose expression does not require behavioral attention and top-down control from higher cortical areas. We propose that horizontal connectivity participates in the propagation of an internal “prediction” wave, shaped by visual experience, which links contour co-alignment and global axial motion at an apparent speed in the range of saccade-like eye movements.},
	language = {en},
	number = {7},
	urldate = {2022-09-26},
	journal = {PLOS ONE},
	author = {Le Bec, Benoit and Troncoso, Xoana G. and Desbois, Christophe and Passarelli, Yannick and Baudot, Pierre and Monier, Cyril and Pananceau, Marc and Frégnac, Yves},
	editor = {Charpier, Stéphane},
	month = jul,
	year = {2022},
	pages = {e0268351},
}

@article{pearce_marie-jean-pierre_2009,
	title = {Marie-{Jean}-{Pierre} {Flourens} (1794–1867) and {Cortical} {Localization}},
	volume = {61},
	issn = {0014-3022, 1421-9913},
	url = {https://www.karger.com/Article/FullText/206858},
	doi = {10.1159/000206858},
	abstract = {The child prodigy Marie-Jean-Pierre Flourens received his medical degree at Montpellier when aged 19. As a young promising physician Flourens was asked to investigate Gall’s controversial views on cerebral localization. To test Gall’s assertions, Flourens developed ablation as a procedure to explore the workings of the brain. By removing anatomically defined areas of the brain of an animal and watching its behaviour, he thought he might localize certain functions. Flourens did not favour the idea of cerebral localization and concluded that the brain functioned as a whole and thus arose the concept of ‘cerebral equipotentiality’. This culminated in his 1824 Recherches expérimentales sur les propriétés et les fonctions du système nerveux. His techniques were, however, crude and imperfect, and his experiments were mainly on birds. Much criticism and debate ensued. A gifted man, Flourens also advanced the physiology of the vestibular apparatus and described the anaesthetic properties of ether.},
	language = {en},
	number = {5},
	urldate = {2022-10-10},
	journal = {European Neurology},
	author = {Pearce, J.M.S.},
	year = {2009},
	pages = {311--314},
}

@article{adrian_impulses_1926,
	title = {The impulses produced by sensory nerve endings},
	volume = {61},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1514868/},
	abstract = {Images
null},
	number = {4},
	urldate = {2022-10-10},
	journal = {The Journal of Physiology},
	author = {Adrian, E. D. and Zotterman, Yngve},
	month = aug,
	year = {1926},
	pmid = {16993807},
	pmcid = {PMC1514868},
	pages = {465--483},
}

@article{piccolino_luigi_1997,
	title = {Luigi {Galvani} and animal electricity: two centuries after the foundation of electrophysiology},
	volume = {20},
	issn = {0166-2236},
	shorttitle = {Luigi {Galvani} and animal electricity},
	url = {https://www.sciencedirect.com/science/article/pii/S0166223697011016},
	doi = {10.1016/S0166-2236(97)01101-6},
	abstract = {Luigi Galvani and his famous experiments on frogs carried out in the second half of the 18th century belong more to legend than to the history of science. Galvani not only laid the foundations of a new science, electrophysiology, but also opened the way for the invention of the electric battery, and thus for the development of the physical investigations of electricity. However, in spite of the widespread celebration of his work, Galvani's scientific endeavours have been largely misrepresented in the history of science. The scholar of Bologna has a stereotyped image as an `occasional' scientist, who started his studies by chance, largely ignored the scientific theories of his time and wandered aimlessly in mental elaborations until the physicist of Pavia, Alessandro Volta, entered the field, correctly interpreted Galvani's results and eventually developed the electric battery. With the present understanding of electrical phenomena in excitable membranes, it is now time to reconsider the real matter raised by Galvani's discoveries and by his hypothesis of an intrinsic `animal electricity', and to make a clearer evaluation of a revolutionary phase of scientific progress.},
	language = {en},
	number = {10},
	urldate = {2022-10-10},
	journal = {Trends in Neurosciences},
	author = {Piccolino, Marco},
	month = oct,
	year = {1997},
	keywords = {Galvani, Volta, animal electricity, electrophysiology, history of science, nervous signalling},
	pages = {443--448},
}

@article{wang_neuromorphic_2015,
	title = {A neuromorphic implementation of multiple spike-timing synaptic plasticity rules for large-scale neural networks},
	volume = {9},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00180},
	doi = {10.3389/fnins.2015.00180},
	abstract = {We present a neuromorphic implementation of multiple synaptic plasticity learning rules, which include both Spike Timing Dependent Plasticity (STDP) and Spike Timing Dependent Delay Plasticity (STDDP). We present a fully digital implementation as well as a mixed-signal implementation, both of which use a novel dynamic-assignment time-multiplexing approach and support up to 226 (64M) synaptic plasticity elements. Rather than implementing dedicated synapses for particular types of synaptic plasticity, we implemented a more generic synaptic plasticity adaptor array that is separate from the neurons in the neural network. Each adaptor performs synaptic plasticity according to the arrival times of the pre- and post-synaptic spikes assigned to it, and sends out a weighted or delayed pre-synaptic spike to the post-synaptic neuron in the neural network. This strategy provides great flexibility for building complex large-scale neural networks, as a neural network can be configured for multiple synaptic plasticity rules without changing its structure. We validate the proposed neuromorphic implementations with measurement results and illustrate that the circuits are capable of performing both STDP and STDDP. We argue that it is practical to scale the work presented here up to 236 (64G) synaptic adaptors on a current high-end FPGA platform.},
	urldate = {2022-10-06},
	journal = {Frontiers in Neuroscience},
	author = {Wang, Runchun M. and Hamilton, Tara J. and Tapson, Jonathan C. and van Schaik, André},
	year = {2015},
}

@article{isbister_clustering_2021,
	title = {Clustering and control for adaptation uncovers time-warped spike time patterns in cortical networks in vivo},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-94002-0},
	doi = {10.1038/s41598-021-94002-0},
	abstract = {How information in the nervous system is encoded by patterns of action potentials (i.e. spikes) remains an open question. Multi-neuron patterns of single spikes are a prime candidate for spike time encoding but their temporal variability requires further characterisation. Here we show how known sources of spike count variability affect stimulus-evoked spike time patterns between neurons separated over multiple layers and columns of adult rat somatosensory cortex in vivo. On subsets of trials (clusters) and after controlling for stimulus-response adaptation, spike time differences between pairs of neurons are “time-warped” (compressed/stretched) by trial-to-trial changes in shared excitability, explaining why fixed spike time patterns and noise correlations are seldom reported. We show that predicted cortical state is correlated between groups of 4 neurons, introducing the possibility of spike time pattern modulation by population-wide trial-to-trial changes in excitability (i.e. cortical state). Under the assumption of state-dependent coding, we propose an improved potential encoding capacity.},
	language = {en},
	number = {1},
	urldate = {2022-10-06},
	journal = {Scientific Reports},
	author = {Isbister, James B. and Reyes-Puerta, Vicente and Sun, Jyh-Jang and Horenko, Illia and Luhmann, Heiko J.},
	month = jul,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {15066},
}

@article{wang_delay_2019,
	title = {A {Delay} {Learning} {Algorithm} {Based} on {Spike} {Train} {Kernels} for {Spiking} {Neurons}},
	volume = {13},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00252},
	abstract = {Neuroscience research confirms that the synaptic delays are not constant, but can be modulated. This paper proposes a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. The proposed algorithm firstly defines spike train kernels to transform discrete spike trains during the learning phase into continuous analog signals so that common mathematical operations can be performed on them, and then deduces the supervised learning rules of synaptic weights and delays by gradient descent method. The proposed algorithm is successfully applied to various spike train learning tasks, and the effects of parameters of synaptic delays are analyzed in detail. Experimental results show that the network with dynamic delays achieves higher learning accuracy and less learning epochs than the network with static delays. The delay learning algorithm is further validated on a practical example of an image classification problem. The results again show that it can achieve a good classification performance with a proper receptive field. Therefore, the synaptic delay learning is significant for practical applications and theoretical researches of spiking neural networks.},
	urldate = {2022-10-04},
	journal = {Frontiers in Neuroscience},
	author = {Wang, Xiangwen and Lin, Xianghong and Dang, Xiaochao},
	year = {2019},
}

@article{perkel_neuronal_1967,
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {I}. {The} {Single} {Spike} {Train}},
	volume = {7},
	issn = {0006-3495},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865962},
	doi = {10.1016/S0006-3495(67)86596-2},
	abstract = {In a growing class of neurophysiological experiments, the train of impulses (“spikes”) produced by a nerve cell is subjected to statistical treatment involving the time intervals between spikes. The statistical techniques available for the analysis of single spike trains are described and related to the underlying mathematical theory, that of stochastic point processes, i.e., of stochastic processes whose realizations may be described as series of point events occurring in time, separated by random intervals. For single stationary spike trains, several orders of complexity of statistical treatment are described; the major distinction is that between statistical measures that depend in an essential way on the serial order of interspike intervals and those that are order-independent. The interrelations among the several types of calculations are shown, and an attempt is made to ameliorate the current nomenclatural confusion in this field. Applications, interpretations, and potential difficulties of the statistical techniques are discussed, with special reference to types of spike trains encountered experimentally. Next, the related types of analysis are described for experiments which involve repeated presentations of a brief, isolated stimulus. Finally, the effects of nonstationarity, e.g. long-term changes in firing rate, on the various statistical measures are discussed. Several commonly observed patterns of spike activity are shown to be differentially sensitive to such changes. A companion paper covers the analysis of simultaneously observed spike trains.},
	language = {en},
	number = {4},
	urldate = {2022-10-04},
	journal = {Biophysical Journal},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	month = jul,
	year = {1967},
	pages = {391--418},
}

@article{perkel_neuronal_1967-1,
	title = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {II}. {Simultaneous} {Spike} {Trains}},
	volume = {7},
	issn = {0006-3495},
	shorttitle = {Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349567865974},
	doi = {10.1016/S0006-3495(67)86597-4},
	abstract = {The statistical analysis of two simultaneously observed trains of neuronal spikes is described, using as a conceptual framework the theory of stochastic point processes. The first statistical question that arises is whether the observed trains are independent; statistical techniques for testing independence are developed around the notion that, under the null hypothesis, the times of spike occurrence in one train represent random instants in time with respect to the other. If the null hypothesis is rejected—if dependence is attributed to the trains—the problem then becomes that of characterizing the nature and source of the observed dependencies. Statistical signs of various classes of dependencies, including direct interaction and shared input, are discussed and illustrated through computer simulations of interacting neurons. The effects of nonstationarities on the statistical measures for simultaneous spike trains are also discussed. For two-train comparisons of irregularly discharging nerve cells, moderate nonstationarities are shown to have little effect on the detection of interactions. Combining repetitive stimulation and simultaneous recording of spike trains from two (or more) neurons yields additional clues as to possible modes of interaction among the monitored neurons; the theory presented is illustrated by an application to experimentally obtained data from auditory neurons. A companion paper covers the analysis of single spike trains.},
	language = {en},
	number = {4},
	urldate = {2022-10-04},
	journal = {Biophysical Journal},
	author = {Perkel, Donald H. and Gerstein, George L. and Moore, George P.},
	month = jul,
	year = {1967},
	pages = {419--440},
}

@misc{ghosh_spatiotemporal_2019,
	title = {Spatiotemporal filtering for event-based action recognition},
	url = {http://arxiv.org/abs/1903.07067},
	author = {Ghosh, Rohan and Gupta, Anupam and Silva, Andrei Nakagawa and Soares, Alcimar and Thakor, Nitish V.},
	year = {2019},
}

@article{pauli_reproducing_2018,
	title = {Reproducing {Polychronization}: {A} {Guide} to {Maximizing} the {Reproducibility} of {Spiking} {Network} {Models}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {Reproducing {Polychronization}},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00046},
	doi = {10/gd8zj5},
	abstract = {Any modeler who has attempted to reproduce a spiking neural network model from its description in a paper has discovered what a painful endeavor this is. Even when all parameters appear to have been specified, which is rare, typically the initial attempt to reproduce the network does not yield results that are recognizably akin to those in the original publication. Causes include inaccurately reported or hidden parameters (e.g., wrong unit or the existence of an initialization distribution), differences in implementation of model dynamics, and ambiguities in the text description of the network experiment. The very fact that adequate reproduction often cannot be achieved until a series of such causes have been tracked down and resolved is in itself disconcerting, as it reveals unreported model dependencies on specific implementation choices that either were not clear to the original authors, or that they chose not to disclose. In either case, such dependencies diminish the credibility of the model's claims about the behavior of the target system. To demonstrate these issues, we provide a worked example of reproducing a seminal study for which, unusually, source code was provided at time of publication. Despite this seemingly optimal starting position, reproducing the results was time consuming and frustrating. Further examination of the correctly reproduced model reveals that it is highly sensitive to implementation choices such as the realization of background noise, the integration timestep, and the thresholding parameter of the analysis algorithm. From this process, we derive a guideline of best practices that would substantially reduce the investment in reproducing neural network studies, whilst simultaneously increasing their scientific quality. We propose that this guideline can be used by authors and reviewers to assess and improve the reproducibility of future network models.},
	urldate = {2021-10-21},
	journal = {Frontiers in Neuroinformatics},
	author = {Pauli, Robin and Weidel, Philipp and Kunkel, Susanne and Morrison, Abigail},
	year = {2018},
	note = {00000},
	pages = {46},
}

@article{sun_learning_2016,
	title = {Learning polychronous neuronal groups using joint weight-delay spike-timing-dependent plasticity},
	volume = {28},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00879},
	doi = {10.1162/NECO_a_00879},
	abstract = {Polychronous neuronal group (PNG), a type of cell assembly, is one of the putative mechanisms for neural information representation. According to the reader-centric definition, some readout neurons can become selective to the information represented by polychronous neuronal groups under ongoing activity. Here, in computational models, we show that the frequently activated polychronous neuronal groups can be learned by readout neurons with joint weight-delay spike-timing-dependent plasticity. The identity of neurons in the group and their expected spike timing at millisecond scale can be recovered from the incoming weights and delays of the readout neurons. The detection performance can be further improved by two layers of readout neurons. In this way, the detection of polychronous neuronal groups becomes an intrinsic part of the network, and the readout neurons become differentiated members in the group to indicate whether subsets of the group have been activated according to their spike timing. The readout spikes representing this information can be used to analyze how PNGs interact with each other or propagate to downstream networks for higher-level processing.},
	number = {10},
	journal = {Neural Computation},
	author = {Sun, Haoqi and Sourina, Olga and Huang, Guang-Bin},
	month = oct,
	year = {2016},
	note = {tex.eprint: https://direct.mit.edu/neco/article-pdf/28/10/2181/972099/neco{\textbackslash}\_a{\textbackslash}\_00879.pdf},
	pages = {2181--2212},
}

@article{kirchner_ultra-rapid_2006,
	title = {Ultra-rapid object detection with saccadic eye movements: {Visual} processing speed revisited},
	volume = {46},
	issn = {0042-6989},
	shorttitle = {Ultra-rapid object detection with saccadic eye movements},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698905005110},
	doi = {10.1016/j.visres.2005.10.002},
	abstract = {Previous ultra-rapid go/no-go categorization studies with manual responses have demonstrated the remarkable speed and efficiency with which humans process natural scenes. Using a forced-choice saccade task we show here that when two scenes are simultaneously flashed in the left and right hemifields, human participants can reliably make saccades to the side containing an animal in as little as 120 ms. Low level differences between target and distractor images were unable to account for these exceptionally fast responses. The results suggest a very fast and unexpected route linking visual processing in the ventral stream with the programming of saccadic eye movements.},
	number = {11},
	journal = {Vision Research},
	author = {Kirchner, H and Thorpe, Sj},
	year = {2006},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Kirchner06},
	pages = {1762--76},
}

@article{muller_cortical_2018,
	title = {Cortical travelling waves: {Mechanisms} and computational principles},
	issn = {1471-003X},
	shorttitle = {Cortical travelling waves},
	url = {http://www.nature.com/doifinder/10.1038/nrn.2018.20},
	doi = {10.1038/nrn.2018.20},
	abstract = {Advanced recording techniques have enabled the identification of travelling waves of neuronal activity in different areas of the cortex. Sejnowski and colleagues review these findings, consider the mechanisms by which travelling waves are generated and evaluate their possible roles in cortical function.},
	journal = {Nature Reviews Neuroscience},
	author = {Muller, Lyle and Chavane, Frédéric and Reynolds, John and Sejnowski, Terrence J.},
	month = mar,
	year = {2018},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Muller18},
}

@article{muller_stimulus-evoked_2014,
	title = {The stimulus-evoked population response in visual cortex of awake monkey is a propagating wave},
	volume = {5},
	issn = {2041-1723},
	doi = {10.1038/ncomms4675},
	abstract = {Propagating waves occur in many excitable media and were recently found in neural systems from retina to neocortex. While propagating waves are clearly present under anaesthesia, whether they also appear during awake and conscious states remains unclear. One possibility is that these waves are systematically missed in trial-averaged data, due to variability. Here we present a method for detecting propagating waves in noisy multichannel recordings. Applying this method to single-trial voltage-sensitive dye imaging data, we show that the stimulus-evoked population response in primary visual cortex of the awake monkey propagates as a travelling wave, with consistent dynamics across trials. A network model suggests that this reliability is the hallmark of the horizontal fibre network of superficial cortical layers. Propagating waves with similar properties occur independently in secondary visual cortex, but maintain precise phase relations with the waves in primary visual cortex. These results show that, in response to a visual stimulus, propagating waves are systematically evoked in several visual areas, generating a consistent spatiotemporal frame for further neuronal interactions.},
	journal = {Nature Communications},
	author = {Muller, Lyle and Reynaud, Alexandre and Chavane, Frédéric and Destexhe, Alain},
	year = {2014},
	note = {00068
Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Muller14},
	pages = {3675},
}

@article{kaplan_anisotropic_2013,
	title = {Anisotropic connectivity implements motion-based prediction in a spiking neural network},
	volume = {7},
	url = {https://laurentperrinet.github.io/publication/kaplan-13},
	doi = {10.3389/fncom.2013.00112},
	abstract = {Predictive coding hypothesizes that the brain explicitly infers upcoming sensory input to establish a coherent representation of the world. Although it is becoming generally accepted, it is not clear on which level spiking neural networks may implement predictive coding and what function their connectivity may have. We present a network model of conductance-based integrate-and-fire neurons inspired by the architecture of retinotopic cortical areas that assumes predictive coding is implemented through network connectivity, namely in the connection delays and in selectiveness for the tuning properties of source and target cells. We show that the applied connection pattern leads to motion-based prediction in an experiment tracking a moving dot. In contrast to our proposed model, a network with random or isotropic connectivity fails to predict the path when the moving dot disappears. Furthermore, we show that a simple linear decoding approach is sufficient to transform neuronal spiking activity into a probabilistic estimate for reading out the target trajectory.},
	number = {112},
	journal = {Frontiers in Computational Neuroscience},
	author = {Kaplan, Bernhard A and Lansner, Anders and Masson, Guillaume S and Perrinet, Laurent U},
	month = sep,
	year = {2013},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Kaplan13},
}

@article{chemla_suppressive_2019,
	title = {Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey {V1}},
	volume = {2792},
	url = {http://www.jneurosci.org/content/early/2019/03/18/JNEUROSCI.2792-18.2019},
	doi = {10.1523/JNEUROSCI.2792-18.2019},
	abstract = {The “apparent motion” illusion is evoked when stationary stimuli are successively flashed in spatially separated positions. It depends on the precise spatial and temporal separations of the stimuli. For large spatiotemporal separation, the long-range apparent motion (lrAM), it remains unclear how the visual system computes unambiguous motion signals. Here we investigated whether intracortical interactions within retinotopic maps could shape a global motion representation at the level of V1 population in response to a lrAM. In fixating monkeys, voltage-sensitive dye imaging revealed the emergence of a spatio-temporal representation of the motion trajectory at the scale of V1 population activity, shaped by systematic backward suppressive waves. We show that these waves are the expected emergent property of a recurrent gain control fed by the horizontal intra-cortical network. Such non-linearities explain away ambiguous correspondence problems of the stimulus along the motion path, preformating V1 population response for an optimal read-out by downstream areas.},
	urldate = {2018-07-27},
	journal = {Journal of Neuroscience},
	author = {Chemla, Sandrine and Reynaud, Alexandre and diVolo, Matteo and Zerlaut, Yann and Perrinet, Laurent U and Destexhe, Alain and Chavane, Frédéric Y},
	month = mar,
	year = {2019},
	note = {Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Chemla19},
	pages = {18},
}

@article{bringuier_horizontal_1999,
	title = {Horizontal {Propagation} of {Visual} {Activity} in the {Synaptic} {Integration} {Field} of {Area} 17 {Neurons}},
	volume = {283},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/283/5402/695},
	doi = {10.1126/science.283.5402.695},
	abstract = {The receptive field of a visual neuron is classically defined as the region of space (or retina) where a visual stimulus evokes a change in its firing activity. At the cortical level, a challenging issue concerns the roles of feedforward, local recurrent, intracortical, and cortico-cortical feedback connectivity in receptive field properties. Intracellular recordings in cat area 17 showed that the visually evoked synaptic integration field extends over a much larger area than that established on the basis of spike activity. Synaptic depolarizing responses to stimuli flashed at increasing distances from the center of the receptive field decreased in strength, whereas their onset latency increased. These findings suggest that subthreshold responses in the unresponsive region surrounding the classical discharge field result from the integration of visual activation waves spread by slowly conducting horizontal axons within primary visual cortex.},
	number = {5402},
	urldate = {2019-02-07},
	journal = {Science},
	author = {Bringuier, Vincent and Chavane, Frédéric and Glaeser, Larry and Frégnac, Yves},
	month = jan,
	year = {1999},
	note = {00535
Loaded from an external bibliography file by Manubot.
source\_bibliography: manual-references.bib
standard\_id: Bringuier99},
	pages = {695--699},
}

@article{pastalkova_internally_2008,
	title = {Internally {Generated} {Cell} {Assembly} {Sequences} in the {Rat} {Hippocampus}},
	volume = {321},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570043/},
	doi = {10.1126/science.1159775},
	abstract = {A longstanding conjecture in neuroscience is that aspects of cognition depend on the brain's ability to self-generate sequential neuronal activity. We found that reliably and continually-changing cell assemblies in the rat hippocampus appeared not only during spatial navigation but also in the absence of changing environmental or body-derived inputs. During the delay period of a memory task each moment in time was characterized by the activity of a unique assembly of neurons. Identical initial conditions triggered a similar assembly sequence, whereas different conditions gave rise, uniquely, to different sequences, thereby predicting behavioral choices, including errors. Such sequences were not formed in control, non-memory, tasks. We hypothesize that neuronal representations, evolved for encoding distance in spatial navigation, also support episodic recall and the planning of action sequences.},
	number = {5894},
	urldate = {2022-02-23},
	journal = {Science (New York, N.Y.)},
	author = {Pastalkova, Eva and Itskov, Vladimir and Amarasingham, Asohan and Buzsáki, György},
	month = sep,
	year = {2008},
	pages = {1322--1327},
}

@article{riehle_spike_1997,
	title = {Spike synchronization and rate modulation differentially involved in motor cortical function},
	volume = {278},
	doi = {10.1126/science.278.5345.1950},
	number = {5345},
	journal = {Science (New York, N.Y.)},
	author = {Riehle, Alexa and Grun, Sonja and Diesmann, Markus and Aertsen, Ad},
	year = {1997},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1950--1953},
}

@article{gutig_spike_2014,
	series = {Theoretical and computational neuroscience},
	title = {To spike, or when to spike?},
	volume = {25},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438814000129},
	doi = {10.1016/j.conb.2014.01.004},
	abstract = {Recent experimental reports have suggested that cortical networks can operate in regimes were sensory information is encoded by relatively small populations of spikes and their precise relative timing. Combined with the discovery of spike timing dependent plasticity, these findings have sparked growing interest in the capabilities of neurons to encode and decode spike timing based neural representations. To address these questions, a novel family of methodologically diverse supervised learning algorithms for spiking neuron models has been developed. These models have demonstrated the high capacity of simple neural architectures to operate also beyond the regime of the well established independent rate codes and to utilize theoretical advantages of spike timing as an additional coding dimension.},
	language = {en},
	urldate = {2022-05-13},
	journal = {Current Opinion in Neurobiology},
	author = {Gütig, Robert},
	month = apr,
	year = {2014},
	pages = {134--139},
}

@article{luczak_sequential_2007,
	title = {Sequential structure of neocortical spontaneous activity in vivo},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/1/347},
	doi = {10.1073/pnas.0605643104},
	abstract = {Even in the absence of sensory stimulation, the neocortex shows complex spontaneous activity patterns, often consisting of alternating “DOWN” states of generalized neural silence and “UP” states of massive, persistent network activity. To investigate how this spontaneous activity propagates through neuronal assemblies in vivo, we simultaneously recorded populations of 50–200 cortical neurons in layer V of anesthetized and awake rats. Each neuron displayed a virtually unique spike pattern during UP states, with diversity seen amongst both putative pyramidal cells and interneurons, reflecting a complex but stereotypically organized sequential spread of activation through local cortical networks. Spike timing was most precise during the first ≈100 ms after UP state onset, and decayed as UP states progressed. A subset of UP states propagated as traveling waves, but waves passing a given point in either direction initiated similar local sequences, suggesting local networks as the substrate of sequential firing patterns. A search for repeating motifs indicated that their occurrence and structure was predictable from neurons' individual latencies to UP state onset. We suggest that these stereotyped patterns arise from the interplay of intrinsic cellular conductances and local circuit properties.},
	language = {en},
	number = {1},
	urldate = {2022-02-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Luczak, Artur and Barthó, Peter and Marguet, Stephan L. and Buzsáki, György and Harris, Kenneth D.},
	month = jan,
	year = {2007},
	keywords = {microcircuits, neuronal assembly, repeating sequences, slow oscillations, syntire chains},
	pages = {347--352},
}

@article{bohte_evidence_2004,
	title = {The evidence for neural information processing with precise spike-times: {A} survey},
	volume = {3},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	number = {2},
	journal = {Natural Computing},
	author = {Bohte, Sander M},
	year = {2004},
	pages = {195--206},
}

@article{davis_spontaneous_2021,
	title = {Spontaneous traveling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states},
	volume = {12},
	doi = {10.1038/s41467-021-26175-1},
	number = {1},
	journal = {Nature Communications},
	author = {Davis, Zachary W and Benigno, Gabriel B and Fletterman, Charlee and Desbordes, Theo and Steward, Christopher and Sejnowski, Terrence J and H Reynolds, John and Muller, Lyle},
	year = {2021},
	pages = {1--16},
}

@article{perrinet_active_2014,
	title = {Active inference, eye movements and oculomotor delays},
	volume = {108},
	copyright = {All rights reserved},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-014-0620-8},
	doi = {10.1007/s00422-014-0620-8},
	abstract = {This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.},
	number = {6},
	journal = {Biological Cybernetics},
	author = {Perrinet, Laurent U and Adams, Rick A and Friston, Karl J},
	month = dec,
	year = {2014},
	keywords = {Active inference, Bayesian model, Biologically Inspired Computer vision, Generalised coordinates, Oculomotor delays, Smooth pursuit eye movements, Tracking eye movements, Variational free energy, active inference, active-inference, bayesian, bicv-motion, bicv-sparse, delays, eye, eye movements, eye-movements, free energy, free-energy, generalized-coordinates, generalized-filtering, motion detection, oculomotor, perception, perrinetadamsfriston14, smooth-pursuit, tracking-eye-movements, variational-filtering},
	pages = {777--801},
}

@article{villette_internally_2015,
	title = {Internally {Recurring} {Hippocampal} {Sequences} as a {Population} {Template} of {Spatiotemporal} {Information}},
	volume = {88},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627315008417},
	doi = {10/f7whnn},
	abstract = {The hippocampus is essential for spatiotemporal cognition. Sequences of neuronal activation provide a substrate for this fundamental function. At the behavioral timescale, these sequences have been shown to occur either in the presence of successive external landmarks or through internal mechanisms within an episodic memory task. In both cases, activity is externally constrained by the organization of the task and by the size of the environment explored. Therefore, it remains unknown whether hippocampal activity can self-organize into a default mode in the absence of any external memory demand or spatiotemporal boundary. Here we show that, in the presence of self-motion cues, a population code integrating distance naturally emerges in the hippocampus in the form of recurring sequences. These internal dynamics clamp spontaneous travel since run distance distributes into integer multiples of the span of these sequences. These sequences may thus guide navigation when external landmarks are reduced.},
	language = {en},
	number = {2},
	urldate = {2022-01-17},
	journal = {Neuron},
	author = {Villette, Vincent and Malvache, Arnaud and Tressard, Thomas and Dupuy, Nathalie and Cossart, Rosa},
	month = oct,
	year = {2015},
	note = {00085},
	pages = {357--366},
}

@incollection{paugam-moisy_computing_2012,
	title = {Computing with spiking neuron networks},
	booktitle = {Handbook of natural computing},
	publisher = {Springer-Verlag},
	author = {Paugam-Moisy, Hélène and Bohte, Sander M.},
	month = sep,
	year = {2012},
}

@article{haimerl_internal_2019,
	title = {Internal representation of hippocampal neuronal population spans a time-distance continuum},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/15/7477},
	doi = {10.1073/pnas.1718518116},
	abstract = {The hippocampus plays a critical role in episodic memory: the sequential representation of visited places and experienced events. This function is mirrored by hippocampal activity that self organizes into sequences of neuronal activation that integrate spatiotemporal information. What are the underlying mechanisms of such integration is still unknown. Single cell activity was recently shown to combine time and distance information; however, it remains unknown whether a degree of tuning between space and time can be defined at the network level. Here, combining daily calcium imaging of CA1 sequence dynamics in running head-fixed mice and network modeling, we show that CA1 network activity tends to represent a specific combination of space and time at any given moment, and that the degree of tuning can shift within a continuum from 1 day to the next. Our computational model shows that this shift in tuning can happen under the control of the external drive power. We propose that extrinsic global inputs shape the nature of spatiotemporal integration in the hippocampus at the population level depending on the task at hand, a hypothesis which may guide future experimental studies.},
	language = {en},
	number = {15},
	urldate = {2022-01-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Haimerl, Caroline and Angulo-Garcia, David and Villette, Vincent and Reichinnek, Susanne and Torcini, Alessandro and Cossart, Rosa and Malvache, Arnaud},
	month = apr,
	year = {2019},
	keywords = {attractor network, hippocampus, neural model, space representation, time representation},
	pages = {7477--7482},
}

@article{malvache_awake_2016,
	title = {Awake hippocampal reactivations project onto orthogonal neuronal assemblies},
	volume = {353},
	issn = {1095-9203},
	doi = {10/bqpq},
	abstract = {The chained activation of neuronal assemblies is thought to support major cognitive processes, including memory. In the hippocampus, this is observed during population bursts often associated with sharp-wave ripples, in the form of an ordered reactivation of neurons. However, the organization and lifetime of these assemblies remain unknown. We used calcium imaging to map patterns of synchronous neuronal activation in the CA1 region of awake mice during runs on a treadmill. The patterns were composed of the recurring activation of anatomically intermingled, but functionally orthogonal, assemblies. These assemblies reactivated discrete temporal segments of neuronal sequences observed during runs and could be stable across consecutive days. A binding of these assemblies into longer chains revealed temporally ordered replay. These modules may represent the default building blocks for encoding or retrieving experience.},
	language = {eng},
	number = {6305},
	journal = {Science (New York, N.Y.)},
	author = {Malvache, Arnaud and Reichinnek, Susanne and Villette, Vincent and Haimerl, Caroline and Cossart, Rosa},
	month = sep,
	year = {2016},
	pmid = {27634534},
	note = {00105 },
	keywords = {Animals, Brain Mapping, CA1 Region, Hippocampal, Calcium Signaling, Exercise Test, Male, Mice, Nerve Net, Neurons, Running, Wakefulness},
	pages = {1280--1283},
}

@article{khoei_flash-lag_2017,
	title = {The {Flash}-{Lag} {Effect} as a {Motion}-{Based} {Predictive} {Shift}},
	volume = {13},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068},
	doi = {10.1371/journal.pcbi.1005068},
	abstract = {Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object’s motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects’ position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation at the present time of spatial information in the visual pathways.},
	language = {en},
	number = {1},
	urldate = {2022-08-31},
	journal = {PLOS Computational Biology},
	author = {Khoei, Mina A. and Masson, Guillaume S. and Perrinet, Laurent U.},
	month = jan,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Coding mechanisms, Extrapolation, Motion, Psychophysics, Sensory perception, Velocity, Vision, Visual system},
	pages = {e1005068},
}

@article{abeles_role_1982,
	title = {Role of the cortical neuron: integrator or coincidence detector?},
	volume = {18},
	number = {1},
	journal = {Israel journal of medical sciences},
	author = {Abeles, Moshe},
	year = {1982},
	pages = {83--92},
}

@article{benosman_asynchronous_2012,
	title = {Asynchronous frameless event-based optical flow},
	volume = {27},
	url = {https://doi.org/10/b55t75},
	doi = {10.1016/j.neunet.2011.11.001},
	abstract = {This paper introduces a process to compute optical flow using an asynchronous event-based retina at high speed and low computational load. A new generation of artificial vision sensors has now started to rely on biologically inspired designs for light acquisition. Biological retinas, and their artificial counterparts, are totally asynchronous and data driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework for processing visual data using asynchronous event-based acquisition, providing a method for the evaluation of optical flow. The paper shows that current limitations of optical flow computation can be overcome by using event-based visual acquisition, where high data sparseness and high temporal resolution permit the computation of optical flow with micro-second accuracy and at very low computational cost.},
	language = {english},
	journal = {Neural Networks},
	author = {Benosman, Ryad},
	year = {2012},
	keywords = {Asynchronous acquisition, Event-based vision, Frameless vision, Optical flow, Spikes, Temporal dynamics},
	pages = {6},
}

@article{lagorce_hots_2017,
	title = {{HOTS}: {A} {Hierarchy} of {Event}-{Based} {Time}-{Surfaces} for {Pattern} {Recognition}},
	volume = {39},
	issn = {0162-8828},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/27411216%20http://ieeexplore.ieee.org/document/7508476/},
	doi = {10.1109/TPAMI.2016.2574707},
	abstract = {This paper describes novel event-based spatiotemporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving near 100\% accuracy on each. We introduce a new 7 class moving face recognition task, achieving 79\% accuracy.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lagorce, Xavier and Orchard, Garrick and Galluppi, Francesco and Shi, Bertram E. and Benosman, Ryad B.},
	year = {2017},
	keywords = {Neuromorphic sensing, event-based vision, feature extraction},
	pages = {1346--1359},
}

@article{grimaldi_robust_2022,
	title = {A robust event-driven approach to always-on object recognition},
	doi = {10.36227/techrxiv.18003077.v1},
	abstract = {We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization.},
	urldate = {2022-01-13},
	journal = {TechRxiv preprint},
	author = {Grimaldi, Antoine and Boutin, Victor and Ieng, Sio-Hoi and Benosman, Ryad and Perrinet, Laurent U},
	month = jan,
	year = {2022},
	keywords = {efficient coding, event-based vision, homeostasis, neuromorphic hardware, online classification},
}

@article{hogendoorn_predictive_2019,
	title = {Predictive {Coding} with {Neural} {Transmission} {Delays}: {A} {Real}-{Time} {Temporal} {Alignment} {Hypothesis}},
	volume = {6},
	issn = {2373-2822},
	shorttitle = {Predictive {Coding} with {Neural} {Transmission} {Delays}},
	url = {http://eneuro.org/lookup/doi/10.1523/ENEURO.0412-18.2019},
	doi = {10.1523/eneuro.0412-18.2019},
	language = {en},
	number = {2},
	urldate = {2019-11-12},
	journal = {eneuro},
	author = {Hogendoorn, Hinze and Burkitt, Anthony N.},
	month = mar,
	year = {2019},
	pages = {ENEURO.0412--18.2019},
}

@article{perrinet_motion-based_2012,
	title = {Motion-{Based} {Prediction} {Is} {Sufficient} to {Solve} the {Aperture} {Problem}},
	volume = {24},
	copyright = {All rights reserved},
	issn = {0899-7667},
	doi = {10.1162/neco_a_00332},
	abstract = {In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to physio-logy and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independent of their texture. Second, we observe that incoherent features are explained away, while coherent information diffuses progressively to the global scale. Most previous models included ad hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features as necessary conditions to solve the aperture problem. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This solution may give insights into the role of prediction underlying a large class of sensory computations.},
	number = {10},
	journal = {Neural Computation},
	author = {Perrinet, Laurent U. and Masson, G.S. Guillaume S.},
	month = aug,
	year = {2012},
	keywords = {Bayesian model, aperture, aperture problem, aperture-problem, association field, coding, emergence, khoei12jpp, khoei13jpp, motion detection, motion prediction, perrinet12pred, predictive, predictive coding, predictive-coding, probabilistic, probabilistic representation, problem, representation, thesis, toupate-inpress},
	pages = {2726--2750},
}

@article{bohte_error-backpropagation_2002,
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	volume = {48},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	doi = {10.1016/S0925-2312(01)00658-0},
	abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
	language = {en},
	number = {1},
	urldate = {2022-09-28},
	journal = {Neurocomputing},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutré, Han},
	month = oct,
	year = {2002},
	keywords = {Error-backpropagation, Spiking neurons, Temporal coding},
	pages = {17--37},
}

@article{dardelet_event-by-event_2021,
	title = {An {Event}-by-{Event} {Feature} {Detection} and {Tracking} {Invariant} to {Motion} {Direction} and {Velocity}},
	doi = {10.36227/techrxiv.17013824.v1},
	abstract = {Contour velocity estimation and tracking from a fully event-based perspective.},
	language = {en},
	urldate = {2022-09-28},
	author = {Dardelet, Laurent and Benosman, Ryad and Ieng, Sio-Hoi},
	month = nov,
	year = {2021},
}

@article{zenke_remarkable_2021,
	title = {The {Remarkable} {Robustness} of {Surrogate} {Gradient} {Learning} for {Instilling} {Complex} {Function} in {Spiking} {Neural} {Networks}},
	volume = {33},
	issn = {0899-7667},
	doi = {10.1162/neco_a_01367},
	abstract = {Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.},
	number = {4},
	urldate = {2021-12-02},
	journal = {Neural Computation},
	author = {Zenke, Friedemann and Vogels, Tim P.},
	month = mar,
	year = {2021},
	pages = {899--925},
}

@article{delorme_ultra-rapid_2000,
	title = {Ultra-rapid categorisation of natural scenes does not rely on colour cues: a study in monkeys and humans},
	volume = {40},
	issn = {0042-6989},
	shorttitle = {Ultra-rapid categorisation of natural scenes does not rely on colour cues},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698900000833},
	doi = {10.1016/S0042-6989(00)00083-3},
	abstract = {In a rapid categorisation task, monkeys and humans had to detect a target (animal or food) in briefly flashed (32 ms) and previously unseen natural images. Removing colour cues had very little effect on average performance. Impairments were restricted to a mild accuracy drop (in some human subjects) and a small reaction time mean increase (10–15 ms) observed both in monkeys and humans but only in the detection of food targets. In both tasks, accuracy and latency of the fastest behavioural responses were unaffected, suggesting that such ultra-rapid categorisations could depend on feed-forward processing of early coarse achromatic magnocellular information.},
	language = {en},
	number = {16},
	urldate = {2022-09-21},
	journal = {Vision Research},
	author = {Delorme, A and Richard, G and Fabre-Thorpe, M},
	month = jul,
	year = {2000},
	keywords = {Categorisation, Colour, Natural scenes, Primate, Visual processing},
	pages = {2187--2200},
}
